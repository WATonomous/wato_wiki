{"/about":{"title":"About WATonomous","data":{"":"WATonomous is the autonomous vehicle design team at the University of Waterloo. We are an agile group of developers, engineers, businessmen, and marketers looking to lead the next generation of robotic systems and their applications to society.","our-big-audacious-goal#Our Big Audacious Goal":"Members of WATonomous center around a singular goal:\nTo show the world a bunch of students can build a self-driving car!\n... and in doing so, drive ourselves to become better people :)."}},"/finance":{"title":"Finance System","data":{"":"Instructions on how to use the WATonomous Finance System, which is used to track funding, our account balances, and manage and reimburse purhcases.Creating and Submitting a Personal Purchase Request Contains details for members on how to purchase items using their own money and get reimbursed for it. (go-to)Creating and Submitting a Purchase Request Contains details for members on how to purchase items using WATonomous cash or directly from one of our funds. (go-to)"}},"/finance/creating_personal_purchases":{"title":"Creating Personal Purchases","data":{"":"Go to https://finance-frontend.watonomous.ca/\nCreate New Ticket > Ticket Type: Personal Purchase\nEnter all purchase details. For funding item link, you would either be using FI-1 (WATO Cash) or one of the Funding Items that match the category. For example, if you're purchasing an RTX 4090 it would probably fit under GPU funding.\nAwait team approval. Please reach out to the finance team and the faculty advisor.\nOnce this has been completed, the item will be sent to the finance coordinator. They will purchase the item, in which case the status will be ORDERED\nOnce the item has arrived, the item will be transitioned to READY_FOR_PICKUP, and the pick up instructions will give you next steps\nOnce completed, the item should be in the PICKED_UP state, completing the flow."}},"/finance/creating_purchase_requests":{"title":"Creating Purchase Requests","data":{"":"Go to https://finance-frontend.watonomous.ca/\nCreate New Ticket > Ticket Type: UW Finance Purchase\nEnter all purchase details. For funding item link, you would either be using FI-1 (WATO Cash) or one of the Funding Items that match the category. For example, if you're purchasing an RTX 4090 it would probably fit under GPU funding.\nAwait team approval. Please reach out to the finance team and the faculty advisor.\nOnce this has been completed, the status will transition to READY_FOR_BUY. You should purchase the item at this step.\nOnce the item has been purchased, please upload the receipt, as well as a University of Waterloo Finance reimbursement form. Instructions found below.\nClick Confirm Item(s) Purchased and Submit Reimbursement","creating-a-university-of-waterloo-finance-reimbursement-form#Creating a University of Waterloo Finance reimbursement form.":"Navigate here\nClick the link in step 1 titled \"reimbursement form (excel)\"\nEnter the following fields\nPayee is Student\nClaimant Name, Student Number, Department, Phone Number, E-mail Address\nMailing Address, City, Province/State, Postal Code. For these, use the address your items were shipped to, or your personal address.\nDestination/Reason for Request: Please put the corresponding reference number. For WEEF, this can be found here. Reach out to @smileycow on Discord if you do not know what this is\nIgnore the Travel Advance Request section, and fill in your item details on the next section. For the description, please put the name of your item, and if available, the reason it was purchased.\nHas an advance been issued: No\nEnter your signature of Claimant:\nDone!"}},"/":{"title":"Welcome to the Wiki","data":{"":"Welcome to the WATonomous Wiki! This is an open-source wiki for all things WATonomous.","new-here#New Here?":"","how-to-edit#How to Edit":"Editing the WATonomous wiki is easy. Simply click Edit this page on the far right of each page. Note, you must be part of the WATonomous Organization to be able to edit.You can use regular react components, or choose from a plethora of pre-built components and tools here."}},"/onboarding":{"title":"Onboarding","data":{"":"WATonomous onboarding guides. The following are quick jists of what each onboarding guide is for:ASD - Developing with WATcloud Contains setup steps for getting access to WATcloud as well as how to use WATcloud in the Autonomous Software Division. (go-to)ASD - General Onboarding Contains a walk-through that teaches you the basics of ROS2, Docker, Docker Compose, and the rest of our robotics software stack. (go-to)"}},"/onboarding/asd_watcloud_dev":{"title":"Developing with WATcloud","data":{"":"You must complete your Cluster Access Form before proceeding with this guide.\nHere, we discuss setting up WATcloud to be used for software development in the Autonomous Software Division.","why-watcloud#Why WATcloud":"Due to the high computational requirements of many aspects of the ASD stack, WATO has a large server infrastructure for remote development WATcloud. In this section, you will learn to connect to WATcloud on VS code. Connecting to a server to do remote development is not only a crucial aspect of software development at WATonomous, but is also a very common practice in the industry.\nFun Fact: WATcloud closely mimics server infrastructures used by OpenAI, NASA, Nvidia, and more!","a-look-from-afar#A Look from Afar":"","how-does-watcloud-share-compute-resources-fairly#How does WATcloud share compute resources fairly?":"WATcloud relies heavily on a resource management tool known as SLURM. SLURM ensures that all resources in WATcloud are shared in a fair and well-managed manner.For the everyday developer, you can imagine SLURM as a \"build your own computer\" tool. You specify to SLURM what compute resources you want (CPU, RAM, GPU, memory, time limit, etc.) and SLURM will build a compute node with the resources it has on hand.","so-how-does-remote-development-actually-work#So how does remote development actually work?":"Remote development for a WATonomous member typically consists of a local machine, host machine, SLURM node, and a docker container. They are defined below:\nLocal Machine Your personal computer.\nHost Machine The computer you connect to. In the case of WATcloud, this is the SLURM login node.\nSLURM Node An \"imaginary computer\" that is created by WATcloud. You specify to WATcloud what compute you need by running commands in the SLURM login node.\nDocker Container An isolated coding environment.\nTo do remote development in the Autonomous Software Division, the process can be summed up by the image below:\nAs shown in the image, there are two ways to use a SLURM node.","job-scheduling-vs-interactive-development#Job Scheduling vs Interactive Development":"Use job scheduling when you want to run a command for a very long time (>1 day long). Use interactive development when you are actively making changes to your code and testing it.For most WATonomous members, you would use job scheduling for tasks like training neural networks, large data processing, numerical optimization, etc. On the other hand, you would use interactive development when you are coding/testing ROS2 nodes, interacting with / visualizing live data, making code changes in general, etc.","setting-up-watcloud-for-asd#Setting up WATcloud for ASD":"This section is experimental. Please let us know of any issues on our Discord\nDealing with SSH can be quite foreign to alot of new developers. Thankfully, we provide a series of helper scripts that will make setup for WATcloud easier on you.","general-setup#General Setup":"","local-machine-clone-the-wato_asd_tooling-repository#[Local Machine] Clone the wato_asd_tooling repository":"git clone git@github.com:WATonomous/wato_asd_tooling.git","local-machine-generate-an-ssh-config#[Local Machine] Generate an SSH config":"If you have never created an ~/.ssh/config file before, do that now. Note, we assume that all your SSH files are stored under ~/.ssh\ntouch ~/.ssh/config\nGenerate a WATcloud SSH config. Follow the prompts whenever you get them.\ncd wato_asd_tooling\nbash ssh_helpers/generate_ssh_config.sh\nYou should now be able to connect our cluster using these commands:\nssh tr-ubuntu3\nssh derek3-ubuntu2\nssh delta-ubuntu2","local-machine-setup-vscode-for-ssh#[Local Machine] Setup VScode for SSH":"To do this, download the Remote - SSH VScode Extension. After that, you should be able to attach VScode to any of the machines.","local-machine-setup-agent-forwarding#[Local Machine] Setup Agent Forwarding":"Agent forwarding lets us carry our identity onto other machines that we connect to. What this means is, you can use git commands on other machines without having to create an SSH key on each machine you connect to.Setup agent forwarding with our helper script. Follow the prompts whenever you get them.\ncd wato_asd_tooling\nbash ssh_helpers/configure_agent_forwarding.sh","host-machine-confirm-agent-forwarding-works#[Host Machine] Confirm Agent Forwarding Works":"You should now be able to use git on all the WATcloud machines you connect to. Confirm by running the following inside a WATcloud machine you connected to.\nssh -T git@github.com\nDeliverable Get SSH and SSH Agent Forwarding working.","setup-for-job-scheduling#Setup for Job Scheduling":"There is no setup. Creating an SLURM job is really easy. It was what SLURM was designed for. You can view docs on SLURM in the WATcloud documentation.\nDeliverable Run a SLURM batch job with 2 CPUs that counts to 60.\nIf you want to create a slurm job that runs inside a docker container, you can use the following helper script.\ncd wato_asd_tooling\nbash slurm_templates/custom_job_node.sh\nYou need to have access to our docker registry to make this work. You can come back to this when you've learned about docker in the General Onboarding","setup-for-interactive-development#Setup for Interactive Development":"Unlike job scheduling, SLURM was not built to handle interactive development. Luckily we have a team of very talented individuals, and we managed to make interactive development work nonetheless :).Creating an interactive development environment entails starting an SSH server inside the SLURM node, some wacky SSH key sharing, a netcast proxycommand, as well as pointing docker to a persistent filesystem. You don't have to do that though. You just need to do the following.","ssh-into-a-slurm-login-node#SSH into a SLURM Login Node":"Both tr-ubuntu3 and derek3-ubuntu2 are SLURM login nodes. You can connect to them by running either\nssh tr-ubuntu3\nssh derek3-ubuntu2","start-a-slurm-dev-node#Start a SLURM Dev Node":"Run the helper script to startup a SLURM Dev Node. Follow all the prompts carefully.\ncd wato_asd_tooling\nbash slurm_templates/small_dev_node.sh\nWe also have other dev node configurations (including a custom configurator) inside the tooling repo.\nbash slurm_templates/medium_dev_node.sh\nbash slurm_templates/large_dev_node.sh\nbash slurm_templates/custom_dev_node.sh\nDO NOT START MORE THAN ONE DEV NODE. You have a chance of corrupting your docker filesystem. Starting more than one dev node is like building multiple computers. It is NOT the same as creating multiple terminals.","local-machine-setup-ssh-for-slurm#[Local Machine] Setup SSH for SLURM":"Run this last helper script LOCALLY. Follow the prompts carefully.\ncd wato_asd_tooling\nbash ssh_helpers/setup_slurm_ssh.sh\nAnd you're good to go! Whenever you want to startup a SLURM Dev Node, start one up by running any of the SLURM Dev templates, and then SSH into the SLURM node through VScode."}},"/onboarding/asd_general_onboarding":{"title":"General Autonomous Software Onboarding - ASD Assignment","data":{"":"You must complete ASD - Developing with WATcloud before proceeding with this guide.","introduction#Introduction":"Congratulations! WATonomous has recognized your talent, and we would like to get you ready to contribute to the team.The Autonomous Software Division (ASD) assignment is designed to train a newcomer robotics programing at WATonomous from the ground up.The onboarding assignment consists of the following:\nAccessing the WATonomous Server Cluster (WATcloud)\nGetting used to ROS2 - Docker robotics infrastructure\nWriting your own control program to direct a robot in simulation\nShould you finish this assignment, you will have all the skills necessary to contribute towards building a real autonomous vehicle. Note, this assignment will not cover ML concepts. However, ML is used throughout robot autonomy, so we highly recommend you learn ML on your own time. Here’s some ML resources for you (DO NOT SHARE).After completing this assignment, you will become an official member of WATonomous with a dedicated watonomous email. Please contact the Director of ASD (Eddy Zhou) on discord, showing proof of completion and 2 subteams you are interested in joining. Link to subteams here.The ASD leads are here to help so feel free to ask any questions about the onboarding assignment in the asd-questions channel on discord. This document is pretty long, but rest assured that a large majority of the assignment is a gigantic walk through.Good Luck!","server-access#Server Access":"Server access steps have been moved to here. Please let the WATcloud leads know if you have any issues.Deliverable 1.0 Create a small SLURM job. Specify to the SLURM node that keep the job running for 2 hours or more.Deliverable 2.0 Connect VS Code to a WATcloud SLURM job. Clone the ASD Training Repository into your WATcloud user’s home directory (~/). Open your cloned repository in VS Code. If you aren’t familiar with git, here’s a cheatsheet.Deliverable 2.0.1 STAR THE WATO_MONOREPO. It’s voluntary, but it will help us legitimize the repository overtime :).Deliverable 2.1 In the opened ASD Training Repo, create a new branch titled \"your_name\"_training. If you aren’t familiar with git, here’s a cheatsheet.","about-the-training-repo#About the Training Repo":"The repository you just cloned contains a barebones setup of WATonomous’ current ASD infrastructure. It closely mirrors the infrastructure of the wato_monorepo, which is the central repository for all of the code that goes into the car. You can read up on the reasoning behind having a monorepo here.","quality-of-life#Quality of Life":"To make connecting to the server in your terminal easier and quicker, you can set up aliases. Aliases are short-form commands that represent a bigger command you don’t feel like remembering.","for-linux-and-mac#For Linux and Mac":"Edit your ~/.bashrc to include an alias. Its general structure is…\nalias \"alias_name\"=”the_long_command_you_dont_wanna_remember”\nYou may need to fiddle around with your bashrc to get it to work. Ideally, you should have your alias ready whenever you open up the terminal.A sample alias to connect to the server could look like:\nSSH_PRIV_KEY_PATH=~/.ssh/id_rsa\nSSH_USERNAME=eddyzhou\nalias sshdelta=\"ssh -v -o ProxyCommand=\"ssh -W %h:%p -i '$SSH_PRIV_KEY_PATH' $SSH_USERNAME@bastion.watonomous.ca\" -i '$SSH_PRIV_KEY_PATH' $SSH_USERNAME@delta-ubuntu2.cluster.watonomous.ca”\nYou should now be able to just type \"sshdelta\" in a new tab of your terminal to connect to our WATcloud server. Aliases are a very powerful command line tool to make development quicker. If you don’t want to remember a command that you use all the time, make an alias! You can also create aliases on our server for remote development.","congratulations#Congratulations!":"You now know how to connect to the WATcloud! Whenever you want to do WATonomous work, you can simply SSH into your WATcloud user and continue where you left off. You do not have to go through this entire process to connect to our server again (well, until the server runs into issues). VS Code will always have the same SSH targets to connect to, and you can always run one of your aliases to enter our server anywhere around the globe.","a-note-on-docker#A note on Docker":"Docker is used heavily throughout our server cluster. It allows for multiple users to set up their own environments and code dependencies without interfering with others. Whether you are doing ASD work, or working on your own project, Docker is an important tool to learn not only for our server, but for any companies you work at that have a large server infrastructure.","asd-training-repository#ASD Training Repository":"In the previous section, you would have cloned the ASD Training Repository into your user’s home directory on the server. Please keep this folder open as you read. There is alot less hand-holding from this point onward.The repository you just cloned contains a barebones setup of WATonomous’ current ASD infrastructure. It closely mirrors the infrastructure of the wato_monorepo, which is the central repository for all of the code that goes into the car. You can read up on the reasoning behind having a monorepo here.","our-tech-stack#Our Tech stack":"Our autonomy stack contains a multitude of coding tools and libraries (PyTorch, TensorFlow, OpenCV, Numpy, Scikit, Foxglove, Gymnasium, etc.), but the three MOST-PROMINENT, REOCCURING TOOLS you must know are ROS2, Docker, and Docker Compose.These software tools enable cool AI tools and algorithms to communicate properly in a robotic system. Permutations of these open source tools are used throughout the cutting-edge robotics industry. It is especially prevalent in R&D, where new tools need to be integrated and tested at an alarming rate.","ros2#ROS2":"The Robotics Operating System 2 (ROS2) is the second iteration of open source tools and libraries used for quickly building robot applications. It helps us intuitively do interprocess communication without the need to dig extremely deep into low-level programming. ROS2 is the monorepo’s main communication interface.Deliverable 3.0 Identify locations in the code where ROS2 is being used.","docker#Docker":"For our ASD stack, we use Docker to containerize all of our code. Docker can be thought of as a lightweight virtual machine, allowing us to create separate environments (containers) for running code. This makes our codebase portable and modular.Docker uses docker files (.Dockerfile extension) to configure and set up each container. Dockerfiles generally start with a machine base, and commands are specified to setup the machine, install dependencies, and setup the code workspace. Here is an example dockerfile from the wato_monorepo.Generally the Dockerfiles for your modules will be set up already by your team leads. However, Docker is still a very useful tool to learn, and you can reference the getting starteddocumentation for more details if interested. Those who can understand our monorepo down to the docker-level are much valued :).Deliverable 3.1.0 Identify locations in the code where Docker is being used.Deliverable 3.1.1 What is the difference between a Dockerfile, a Docker Image, and a Docker Container? Refer to online references.","docker-compose-and-wato#Docker Compose and wato":"Docker Compose is a utility for managing a codebase with multiple docker components. It provides an intuitive yaml interface to configure, build, and run docker containers. With a single Docker Compose command, you can startup multiple containers at once.In our ASD training repo, and our monorepo, you will make great use of a software tool known as ‘watod’. Under the hood, watod is a wrapper around Docker Compose. You can use watod just like how you would use Docker Compose.Deliverable 3.2.0 Identify locations where Docker Compose is being used. What does WATO call these Docker Compose files? Hint: directory nameDeliverable 3.2.1 Identify environment variables in a docker-compose file (Hint: they start with a $). How are these environment variables set during runtime? The next question can clear up this question.Deliverable 3.2.2 Identify any files prefixed with ‘watod’. Take a closer look at ‘watod-setup-env.sh’, what is it doing? What script do you run to run ‘watod-setup-env.sh’? How does this relate to the environment variables found in the docker-compose file?Deliverable 3.2.3 In your own words, explain what watod does. How does it interface with our docker-compose files?","whats-up-with-the-long-image-names#What's up with the long image names?":"You may have noticed docker image names along the lines of “git.uwaterloo.ca:5050/watonomous/wato_monorepo/…”. This long image name refers to a docker image stored in a Docker Registry.Building docker images from scratch can take an extremely long time. So our team takes advantage of Docker Registries to quickly pull pre-built images. This saves us a lot of time.In the ASD Training Assignment, docker registries are disabled. This is why you see the manifest error when building. No fret, we disabled registries on purpose. In the actual wato_monorepo, you’ll have to learn how to use registries.","a-visual-understanding#A Visual Understanding":"Below is a possible way to visualize how our ASD infrastructure works:\nThe entire system communicates within itself using ROS2 messages. All the cool algorithms for perception, planning, control, etc. are wrapped in their own ROS2 nodes to function concurrently. Various nodes share docker containers docker containers if they require the same libraries and tools to function (eg. two nodes may require the same version of PyTorch, so they share the same docker container).Deliverable 3.2.3 In your own words, describe an analogy to real life that closely resembles the ASD infrastructure.","on-startup#On startup":"The visualization of the tech stack above represents our code in a running state. On startup, watod (docker-compose under the hood) is used to orchestrate the startup of each and every docker container, ROS2 node, and core algorithms. It carefully and automatically builds out the entire software architecture piece by piece until you end up with the visualization above.","training-overview#Training overview":"Now that you have a decent understanding of our tech stack. We can now move on to the final part of the ASD training. This part will be the most time consuming, and it will also contain an extremely limited amount of hand holding. Our team is here to help, so if you have any questions, feel free to ask on Discord!The provided training repository contains the following:\nA Gazebo server, which is a robotics simulator allowing us to interact with and write code for a virtual robot\nA Foxglove websocket, which streams ROS data over a websocket to be viewed using the Foxglove ROS visualization dashboard.\nROS2, Docker, and WATO backend infrastructure.\nSample ROS2 nodes in C++ and Python that demonstrate custom message passing.\nA barebones ROS2 node which you will write your code in.\nThe end result of the training will be some custom-written ROS2 nodes which will interact with various sensors on the robot, perform some computation/processing, and output commands to control the robot. All of this will be done using ROS2 as a framework.","set-up#Set up":"In previous sections, we made you analyze the ASD Training code. Now is the time to learn how to use it.","configuration-and-startup#Configuration and startup":"","watod-orchestration#Watod Orchestration":"As you may or may not know, watod is used to orchestrate our entire tech stack together. It sets any necessary environment variables that our docker compose profiles would need on startup, and then calls the docker compose command to begin building all the docker containers.Oftentimes, you don’t need to run the entire software pipeline to begin development, so you may only need to run a minimum slice of the software pipeline to begin. The powerful thing about watod is that you can configure what profiles (docker-compose files) to start up. This equates to only starting only a portion of the entire stack as opposed to starting up the whole thing.","watod-configsh#watod-config.sh":"To configure what profiles to startup, we use watod-config.sh Specifically the ACTIVE_PROFILES field.\n## ----------------------- Watod2 Configuration File Override ----------------------------\n## ACTIVE PROFILES CONFIGURATION\n## List of active profiles to run, defined in docker-compose.yaml.\n##\n## Possible values:\n## - vis_tools : starts tools for data visualization (foxglove)\n## - gazebo : starts robot simulator (gazebo)\n## - samples : starts up sample nodes for reference (optional)\n## - robot : starts up robot nodes\nACTIVE_PROFILES=\"vis_tools gazebo samples\"\n## Name to append to docker containers. DEFAULT = <your_watcloud_username>\n# COMPOSE_PROJECT_NAME=\"\"\n## Tag to use. Images are formatted as <IMAGE_NAME>:<TAG> with forward slashes replaced with dashes.\n## DEFAULT = <your_current_github_branch>\n# TAG=\"\"\nYour watod-config.sh would have ACTIVE_PROFILES commented out, so please populate this variable just like above. vis_tools, gazebo, and samples are all profiles.\nNOTE: Once you are a full ASD member, please leave the watod-config.sh unchanged when you make a Pull Request.","watod-up#./watod up":"After you’ve changed your watod-config.sh, simply startup the training stack by entering\n./watod up\nin your command line while inside the ./wato_asd_training/ directory. You should then see docker compose building all the Docker images present in each of the profiles you specified. You could startup each of the three profiles alone if you wanted to. Use  Ctrl + C  to stop watod.Deliverable 4.0 Startup all three profiles in the training repo (vis_tools, gazebo, and samples). After you’ve done that, stop watodwatod and startup each profile alone. Do this again with two of the three profiles.","dev-containers#Dev containers":"Dev containers are a very important aspect of WATonomous’ development cycle. When you run ./watod up, you spin up multiple containers that you can develop in. Inside these containers, you can change code, debug, and develop to your heart’s content, and all changes made inside the dev container will propagate out to your host machine.The typical WATonomous development cycle looks like this:","step-1#Step 1":"Up a container using ./watod up","step-2#Step 2":"Enter the container using VScode Docker extension","step-3#Step 3":"Make changes inside the container and debug with ros2 tools","step-4#Step 4":"Changes in the container automatically propagate out (and vice versa)","step-5#Step 5":"Outside the container, Git add, commit, and push\nThe main benefit to dev containers is that you can use linters and tools installed inside the container. For example, you cannot install numpy in the WATcloud host, but you can install numpy in a container and play around with it inside that container.Deliverable 4.0.1 Enter the transformer container and change its BUFFER_CAPACITY.Deliverable 4.0.2 Rebuild the transformer inside the container using colcon build under the ~/ament_ws directory. Source your changes with source install/setup.bash.Deliverable 4.0.3 Launch the transformer node with ros2 launch transformer transformer.launch.py. You should notice your transformer buffer fill up faster or slower based on the new BUFFER_CAPACITY you specified.Deliverable 4.0.4 Outside the container, check that your changes to the transformer have propagated out using git status. You should see something like the following…\nYou can also to ./watod up outside your container, and the changes you made inside the container should reflect in the robot logs.Deliverable 4.0.5 BONUS  when doing ./watod up, the transformer node spins up automatically. Can you think of a way to not make the node spin up? Hint: you need to stop the node from launching, but you have to keep the container running for an indefinite period.","foxglove#Foxglove":"Foxglove is an open source data visualization tool for robotics. Founded by a couple of folks at Cruise, Foxglove’s main advantage is its ability to visualize server-side data.You may have heard of RViz, which is ROS’s native visualization software. As powerful as this tool can be, RViz is not great for server-side development. In order to use RViz on our server, you’d have to spin up a VNC docker container. This is what our team has done in the past, but it was terrible. RViz would run at no faster than 3fps.","port-forwarding#Port Forwarding":"In a nutshell, Port Forwarding establishes a connection between a communication endpoint on one device in a private network to another device on another network. We use Port Forwarding to view Graphic User Interfaces (GUIs) and stream data to your local machine. The world of networking is scary, so we will try to make it as straightforward as possible.A recurring pattern for Port Forwarding is: “software A is streaming data out of some port ####, I want to read this data myself, so I will forward port #### to my machine to read it”. If software A is in a Docker container on a server host machine, then you have to forward the port from the Docker container to the host machine, and then from the host machine to your local machine.You don’t have to do much port forwarding yourself, but it’s good to know the gist of it as we move forward.","using-foxglove#Using Foxglove":"Make sure you have the gazebo and vis_tools profiles running\nHave some sort of web browser on your local machine\nHave the asd_training_config downloaded on your local machine (this is in the training repo)\nOnce you have the two profiles running, you should notice an auto-forwarded port in your VS Code.\nThis port is where foxglove is streaming its data from. You don’t have to do any forwarding yourself, as we’ve done it for you ;P.To view this data, go to the Foxglove website, create an account (with your personal email, we need to hash out sponsorship from Foxglove before you can use your watonomous email), and either visualize the data through your web browser or a local download of Foxglove.","viewing-ros-data-through-foxglove#Viewing ROS data through foxglove":"To view the data coming from the forwarded port, in the Foxglove dashboard click Visualize Data -> Open Connection -> Foxglove Websocket and set it as the port that was forwarded. In the example above, that would be ws://localhost:33401. It will be different for everyone.\nYou can now proceed to open your Foxglove Studio to begin viewing the robot simulation.Deliverable 4.1 Up the ASD Training Repo and view your robot simulation in Foxglove on your local machine.Your Foxglove Studio should look something like this. Don’t worry if it looks slightly different. We will fix that now. In the ASD Training Repo, we’ve provided a configuration file that sets up your Foxglove Studio to look exactly like how we want it to look. If you haven’t yet, download the configuration file from the repo.You then need to load this file into your local Foxglove Studio.Reload your Foxglove, and you should see something like the following.Deliverable 4.2 Upload the ASD Foxglove config to your local foxglove.Deliverable 4.3 Interact with Foxglove Studio. You can move around the robot with the Teleop Panel. Here are a couple of subtasks to get you acquainted.\nDeliverable 4.3.1\nDeliverable 4.3.2 BONUS: View the list of Topics. What is a Topic? This can be a great opportunity to get an initial idea of ROS Topics.Deliverable 4.3.2 BONUS: The visualized data is, in-reality, a bunch of numbers. Foxglove (and RViz) do some post processing to make it look intuitive. View the /lidar topic’s raw data through a new Raw Message Panel.","hands-on-training#Hands-on Training":"Now that your development environment is set up, we can begin hands-on training. You can always refer back to the previous steps for reference. Kind reminder that our team is here to help, so if you ever have any questions, please feel free to ask in the ASD-Questions channel :).Originally, the intent of this training was to code up a semi-complete navigation system to make the robot rather-intelligently navigate to a destination of your choice. However, for the sake of not blowing up your mind, we have trimmed the assignment down to at least touch all the fundamental concepts we would like you to learn. If you would like to do the whole assignment, feel free to reach out, and we can give you more info on how the robot should navigate the map.","the-basis-of-robot-navigation#The Basis of Robot navigation":"Robot navigation is a very large field of study, so let’s be more specific. The navigation problem we are most concerned with here at WATonomous is: given an end position, and no prior knowledge of the environment, navigate to that end position.In Foxglove Studio, you should see an environment that closely relates to the problem above.\nHere, the end position can be specified by the user to be anywhere on the map.In an ideal environment, where obstacles don’t move, robot navigation can be quite simple. A naive approach to this problem could be: Occupancy, Immediate Trajectory, and Execution. We provide a brief description of them below:Occupancy: A map of squares indicating where the robot can and cannot go.Immediate Trajectory: The planned trajectory to take to get to the end position.Execution: Following the planned trajectory as best as we can.In this system, the occupancy around the robot is found continuously. Given this geometric representation, we can produce a trajectory using the Immediate Trajectory planner. This trajectory is then Executed for a period of time. The whole process repeats continuously until an endpoint is reached.This is not much different to the real autonomy stack of the car. A higher fidelity Occupancy is found using Perception. Immediate trajectories need to factor in the complexity of the road with Navigation. And we mist execute on this trajectory with Control.To formalize, Perception would receive sensor messages and do processing. Navigation would receive the processed sensor data and figure out a set of actions to take. Control would receive the actions and perform them, finally sending commands to the motor. In fact, WATonomous’s software stack follows a very similar architecture, but with more node subdivision within each group to perform more complex computations.","goal#Goal":"The Foxglove simulation consists of a robot within an environment. The robot has three wheels, two motors, a camera, and a 1-dimensional laser scanner.You can freely control the robot’s movement within the environment using the Foxglove Teleop panel.As you may have noticed, the Teleop control of the robot is pretty terrible. It oversteers, stops late, and has a ton of inertia. What if we could simply instruct the robot to move to a specific point on the map instead?The goal of your hands-on training is to do just that! By specifying a goal on the foxglove map, your robot should naively navigate to that point on the map.\nNote: It is important to know that the node you are about to create does not allow the robot to avoid obstacles. It will simply follow the straight-line trajectory to the point you specified.\nAs naive as this sounds, this is actually the basis of robot control. You will see later how this simple control algorithm you’ll make is used with the rest of the navigation stack to achieve obstacle avoidance!","ros#ROS":"Robot Operating System (ROS) is a popular framework for robotics codebases. At its core, it is a message broker – a system which manages and passes data between programs. This allows for modularity, management of complex data types, and interoperability. What makes ROS so powerful is the ability to easily interface libraries and tools that were built on top of ROS to bring advanced capabilities and interactive visualizations to your software stack. Some examples that we will be using are:\nFoxglove, a web-based ROS dashboard for visualizing data and interacting with the robot\nGeometry messages, a ROS package which includes a lot of standard geometry data types like Pose, Point, Twist, Quaternion, etc.\nTransforms 2 (Tf2), a ROS package which allows easy transformations between coordinate frames\nThe basic building block of the ROS framework are nodes. Nodes are responsible for performing a specific set of tasks, and interact with other nodes by receiving (subscribing) or sending out (publishing) messages. You can think of ROS like a network, with many nodes interacting with each other through ROS to achieve the overall function of the software stack.On the technical side, ROS nodes are just c++ or python programs with the ROS library. The ROS framework then manages the building, running, and message passing of each node behind the scenes. The ROS library provides the syntax for creating a node, subscribing to messages, sending out messages, and native objects for ROS messages.\nNote: This document will give a quick overview of ROS2 code, however we highly recommend going through the ROS2 Getting Started documentation which is much more detailed and explains ROS2 from start to finish. There may be a steep learning curve, so if you are confused throughout these training modules, feel free to ask the team leads for an explanation on discord!\nHere is an example ROS node in C++:\n#include \"transformer_node.hpp\"\nExampleNode::ExampleNode() : Node(\"example_node\"){\n  // Subscribe to a topic\n  subscription_ = this->create_subscription<geometry_msgs::msg::Pose>(\n    \"/example_topic0\", 20, \nstd::bind(&ExampleNode::subscription_callback, this, \nstd::placeholders::_1));\n  // Publish to a topic\n  publisher_ =\n this->create_publisher<geometry_msgs::msg::Pose>(\"/example_topic1\", 20);\n}\n// Subscription callback: gets called when a message from the subscription comes in\nvoid ExampleNode::subscription_callback(const geometry_msgs::msg::Pose::SharedPtr msg)\n{\n\t// Print to the console\nRCLCPP_INFO(this->get_logger(), \"Received message…\");\n\t…\n\t// Publish a message\n   \tpublisher_->publish(msg);\n}\n// Boilerplate code for starting up the node (gets called by ROS   framework)\nint main(int argc, char ** argv)\n{\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared<ExampleNode>());\n  rclcpp::shutdown();\n  return 0;\n}\nLet’s break it down.First, we have the constructor of our node, which inherits the Node class from the ROS library (rclcpp). There we pass the name of our node, “example_node”.\nExampleNode::ExampleNode() : Node(\"example_node\"){\n …\n}","publisher#Publisher:":"Inside the constructor, we initialize our publishers and subscribers. Here is the syntax to create a publisher with the rclcpp library:\n// In header file:\nrclcpp::Publisher<MESSAGE_TYPE>::SharedPtr publisher_;\n// In node constructor:\npublisher_ = this->create_publisher<MESSAGE_TYPE>(\"/TOPIC\", 20);\nWhere MESSAGE_TYPE is the message you are publishing (i.e. geometry_msgs::msg::Pose for a Pose message) and “/TOPIC” is the topic you are publishing to. The 20 at the end is the buffer size, 20 is a good number for that.\nYou can then publish a message to the publisher using:\npublisher_->publish(msg);\nWhere msg is an object of the type you specified (i.e. geometry_msgs::msg::Pose()).","subscriber#Subscriber":"Next, here is the syntax to create a subscriber:\n// In header file:\nrclcpp::Subscription<MESSAGE_TYPE>::SharedPtr subscriber_;\n// In node constructor:\nsubscriber_= this->create_subscription<MESSAGE_TYPE>(\n    \"/TOPIC\", 20, \nstd::bind(&CALLBACK, this, \nstd::placeholders::_1));\nWhere MESSAGE_TYPE is the message in the topic you want to subscribe to (i.e. geometry_msgs::msg::Pose), “/TOPIC” is the topic to subscribe to, and CALLBACK is a reference to a member function that will get called when a new message comes in (i.e. ExampleNode::subscription_callback). Again, the 20 is the buffer size, and std::placeholders::_1 is boilerplate code for calling the callback with an argument.When a message is received by the subscriber, it calls the callback function with a pointer to the message. Here is an example callback function you would define in your node class:\n// In header file:\nvoid ExampleNode::subscription_callback(const geometry_msgs::msg::Pose::SharedPtr msg);\n//Implementation:\nvoid ExampleNode::subscription_callback(const geometry_msgs::msg::Pose::SharedPtr msg)\n{\n …\n}\nThe ::SharedPtr is a special type of smart pointer, but it is functionally the same as a normal pointer and can be dereferenced and accessed using ->. Values of the message can be accessed just like a normal C++ object, such as “msg->position.x, msg->position.y, msg->position.z” to access the coordinates of the Pose message.You can reference the property names of any message by searching up the message followed by “ros2” on google. For example, here are the docs for the Pose message: docs for message, and you can see all the variables you can access in it.","timers#Timers:":"Timers are tasks that execute in a repeated loop. Example use cases for timers are control loops which consistently update motor commands based on observations. The syntax to create a timer is the following:\n//In header file:\nrclcpp::TimerBase::SharedPtr timer_;\nvoid timer_callback();\n//In node constructor:\ntimer_ = this->create_wall_timer(std::chrono::milliseconds(delay_ms), std::bind(&YourNode::timer_callback, this));\n//Callback implementation:\nvoid YourNode::timer_callback(){\n}\nWhere delay_ms is the time between timer calls in milliseconds, and YourNode::timer_callback is the callback function that is called when the timer executes.","printing#Printing":"To print to the console, use RCLCPP_INFO followed by a string. You can pass additional parameters to the RCLCPP_INFO and they get added to the string using c++ sprintf format. For example, adding %f to the string will add in a floating point number to the print, which is passed in as a parameter to RCLCPP_INFO. The following code segment shows printing position.x to the console.\nRCLCPP_INFO(this->get_logger(), \"Received message… x=%f\", msg->position.x);\n// Will output “Received message… x=10.5” or whatever position.x is\nThese are all of the core functionalities to writing a node in ROS! As an exercise, try implementing a publisher and subscriber in C++ using one of the empty nodes available in the training repo, such as src/robot/control/src/control_node.cppDeliverable 5.1 Write a timer that executes every second that will call a callback function and print out a message of your choice to the console. Verify that it works by running ./watod up and viewing the console printouts.Deliverable 5.2 Create a publisher that publishes a std_msgs::msg::String message to the “/example_string” topic. Modify the timer callback function you wrote in Deliverable 5.1 to include publishing of a string message to your publisher with a message of your choice. Verify the messages are being sent by adding a “raw message” widget to foxglove and set it to your new topic name. Congratulations, you published your first message!Deliverable 5.3 Write a subscriber that receives nav_msgs::msg::Odometry messages from the “/model/robot/odometry” topic (should be automatically available via Gazebo when you run the training repo) and prints its x,y,z coordinates to the console. Reference the ros2 documentation for the Odometry message properties. Congratulations, you subscribed to your first message!You have now successfully written a node to publish and subscribe to messages, which is the core functionality of ROS! You can now write nodes to interact with robots via subscribing and publishing ROS messages.","control#Control":"Now it’s time to implement your first node to interact with the robot! This node will be responsible for receiving a goal position to drive towards and send motor commands to the robot to drive towards it.To set a position for the robot to drive to, we will be using Foxglove’s 3D panel publish feature, which allows you to click on the 3D map and publish a message. In Foxglove under panel settings for the 3D panel, you should see the following listed under the Publish tab:This is where you select the message type and topic to publish to when you click on the 3D map. If you use our configuration file, these will be automatically selected to geometry_msgs::msg::Point and “/goal_point” topic.In order to publish that message, click on the circle icon at the top right of the 3D panel (shown below) and click on a point on the map.You can verify that your point is published by adding a Raw Message widget and point it to the “/goal_pose” topic and watch as a message appears when you click on the circle icon and click on the map.The purpose of the control node is to subscribe to this topic and drive the robot towards the point you clicked on the map.\nYou will be writing code in the control node, so go ahead open up src/robot/control/src/control_node.cpp in the wato_asd_training repo. Now, to receive these messages, create a new subscriber in the Control node (a blank node called Control has been setup already in the repo, you may edit that node) and make it subscribe to geometry_msgs::msg::PointStamped messages in the “/goal_point” message. Then, create a callback and print out the x,y,z values of that PointStamped message (reference link) using RCLCPP_INFO.Deliverable 6.1 Create a subscriber that receives geometry_msgs::msg::PointStamped messages from the “/goal_point” topic. Verify you receive the messages by printing out the x,y,z coordinates to the console.Now that you can receive the goal pose, the next step is to control the robot. Firstly, we need a timer callback that will run the control loop on repeat. To do this, create a timer that runs every 100ms and calls a blank function for now.Deliverable 6.2 Create a timer that calls a blank callback function every 100ms for control. You can verify it works by printing a random message using RCLCPP_INFO.Inside this timer, we will be using a simple proportional controller. A proportional controller is the first term in PID control, which simply multiplies a constant scalar to the error, where error is the distance between your current point and desired point.Command = Kp * errorIn this case, we have two error terms: x and y. The x error will define forward movement, and the y error will define angular movement.However, there is one problem. Currently, the point message is defined in the world coordinate frame, while we want the error in the robot’s relative coordinate frame to do relative movements. In order to do this, we will take advantage of ROS2’s transform system: TF2","transforms#Transforms":"Transforms define coordinate frames for geometry objects. One example could be a coordinate relative to the world versus relative to the robot, or a coordinate relative to an arm of the robot versus the base of the robot. Here is an illustration of a complex robot with multiple transforms (shown by red,blue,green axis markers):\nLuckily, the ROS2 library has an easy way for dealing with transforms. The gazebo environment we provided automatically publishes the transform of the robot, which allows us to easily converta point in world space to robot space. Once the point is in robot space, we can get the relative x and y offset from the robot’s position to the point.To access ROS2’s transform system, you must create a tf_buffer and tf_listener using the following syntax:\n// In header file   \nstd::unique_ptr<tf2_ros::Buffer> tf_buffer_;\nstd::shared_ptr<tf2_ros::TransformListener> tf_listener_;\n  \n// In node constructor\ntf_buffer_ = std::make_unique<tf2_ros::Buffer>(this->get_clock());\ntf_listener_ = std::make_shared<tf2_ros::TransformListener>(*tf_buffer_);\nThen, to access the transform between two frames, use the following syntax:\ngeometry_msgs::msg::TransformStamped transform;\ntry {\n    transform = tf_buffer_->lookupTransform(\"TO_FRAME\", \"FROM_FRAME\", tf2::TimePointZero);\n} catch (const tf2::TransformException & ex) {\n    RCLCPP_INFO(this->get_logger(), \"Could not transform %s\", ex.what());\n}\nWhere “TO_FRAME” is the frame you want to transform the point into, and “FROM_FRAME” is the frame the point originated in.Then to transform a point using that frame:\nauto transformed_point = geometry_msgs::msg::PointStamped();\n tf2::doTransform(original_point, transformed_point, transform);\nWhere original_point is the point in the FROM_FRAME and transformed_point is that same point in the TO_FRAME.We will be leveraging this code to transform our global point published to /goal_point into the robot’s frame. The goal_point will be in the frame “sim_world” (which is our global frame in this case) and the robot’s frame is “robot”.Deliverable 6.2 Create a tf_buffer and tf_listener in your control node. Then, in the control callback, get the transform from “sim_world” to “robot” and transform the goal_point to the robot frame. You may need to store goal_point in an instance variable in the Node class to access it in the timer callback after it has been received from the subscriber.Now that the point is transformed, the x coordinate relates to the robot’s forward motion while the y coordinate relates to the robot’s side to side motion (angular). Therefore, an easy proportional control loop may be derived where the linear command is proportional to the X component and the angular command is proportional to the Y component.Deliverable 6.3 Create two variables called Kp_linear and Kp_angular which will be the scalars for the proportional loop, initialize them to 0.5 as a good starting guess (these can be tuned later). Multiply Kp_linear by the X component of the goal_point transformed in the robot frame and Kp_angular by the Y component of the goal_point transformed in the robot frame. These will be your control signals to send to the robot.Now that you have the control signals, you must publish them to the robot.  By default, the robot takes in control signals in the form of a geometry_msgs::msg::Twist message, which has a linear and angular velocity associated with it. The simulated robot is setup to receive Twist messages in the “/cmd_vel” topic, so you can directly send commands to the robot by creating a publisher in the control node that publishes geometry_msgs::msg::Twist messages to the “/cmd_vel” topic. Once the subscriber is made, you can then package up your linear and angular velocity commands inside of a Twist message (linear velocity will use the x component of the linear Vector3 of Twist and angular velocity will use the z component of the angular Vector3 of Twist. The rest will remain 0) and publish to the robot to control it!Deliverable 6.4 Create a publisher that publishes geometry_msgs::msg::Twist messages to the “/cmd_vel” topic. In your control timer, create a new Twist message, set the linear x component to the result of the linear proportional loop and the angular z component to the result of the angular proportional loop. Then, your robot should drive towards the goal_point message when you publish in Foxglove!","conclusion#Conclusion":"Congratulations, you have now successfully controlled a robot using ROS2! The fundamentals of publishers, subscribers, and a few tricks you learned like Foxglove and transforms will be extremely valuable in your development in the WATO ASD software stack.Please contact the Director of ASD (Eddy Zhou) on discord, showing proof of completion and 2 subteams you are interested in joining. Link to subteams here. Welcome to WATonomous :)"}}}