{"/about":{"title":"About WATonomous","data":{"autonomy-noun#autonomy (noun)":"/…ôÀàt√§n…ômƒì/\nThe ability of a robot or system to perform tasks and make decisions without human intervention.\nFreedom from external control or influence; independence.\nThe right or condition of self-government.\nExample:\nThat car just drove me to San Francisco fully autonomously!WATonomous is the autonomy design team at the University of Waterloo. Our team centers around robotics software and designing systems capable of making autonomous decisions.We are an agile group of developers, engineers, businessmen, designers, and marketers looking to lead the next generation of robotic and their applications to society.","our-goal-and-values#Our Goal and Values":"WATonomous follows a singular goal:\nTo show the world a bunch of students can build autonomous robots! and in doing so, drive ourselves to become better people :)\nAt WATonomous, we value the following:\nHumility: Never act smarter than anyone else. Everyone starts somewhere, so never knock someone down for at least trying.\nInclusivity: All that matters is that you want to become a robotics engineer, nothing else. We will provide you the resources to become one no matter your background.\nIndependence: Our team is competition-independant. We function off of a diverse number of robotics projects driven by curiousity, ingenuity, and wow-factor.\nDreamy Pragmatism: The future is bright. In our lifetime, robots will do our chores, drive our cars, and send us to mars. However, the people who are gonna get us there aren't the ones with their head in the clouds.\nGrit: You're gonna need it.","want-to-know-more#Want to know more?":""}},"/about/constitution":{"title":"üìú WATonomous Constitution","data":{"article-i--name#Article I ‚Äì Name":"The name of this organization shall be WATonomous. Capital W A T.","article-ii--purpose#Article II ‚Äì Purpose":"The purpose of this organization is to show the world a bunch of students can build autonomous robots.\nIn pursuit of such purpose...\nWe pursue multiple robotics projects centered around a common toolset\nWe encourage communication and knowledge-sharing between different projects to nurture a community and collectively expand our knowledge of robotics\nWe drive ourselves to become better people","article-iii--team-structure#Article III ‚Äì Team Structure":"The organization's team structure is shown below:","section-1-federated-team-structure#Section 1: Federated Team Structure":"WATonomous operates using a federated team model that supports multiple autonomous robotics projects under one unified organization. This structure is designed to enable scalability, encourage ownership, and promote parallel development across different domains of autonomy.Key Characteristics\nProject-Based Divisions: Each project (e.g., Eve, Rover, Humanoid, F1Tenth) functions as a semi-autonomous unit.\nLocal Leadership: Projects are led by their own set of Co-Directors, who oversee execution and direction within their scope.\nSquad-Oriented Subteams: Each project is further divided into Squads, led by Squad Leads and composed of Core Members.\nCentral Coordination: The Co-Captains and Admin Core (e.g., BizOps, WATcloud) provide shared infrastructure, strategic alignment, and operational support.\nThis model balances decentralized execution with centralized coordination, allowing each project to innovate independently while remaining aligned with the team‚Äôs broader goals.","section-2-adaptive-leadership#Section 2: Adaptive Leadership":"Leadership within WATonomous is intentionally designed to be adaptive to meet the evolving needs of each project.Leadership Roles\nSquad Leads are assigned based on task ownership rather than fixed roles.\nSquad responsibilities are fluid (even between software and hardware), enabling members to take charge of specific initiatives or deliverables as needed.\nCo-Directors of each project are responsible for defining how leadership is distributed within their teams.\nGuiding Principles\nTask-Based Authority: Leadership is delegated according to expertise, initiative, and the nature of the work.\nCollaborative Structure: Squad Leads collaborate closely with Co-Directors to ensure technical and logistical alignment.\nLeadership Development: The system encourages all members to grow into leadership roles based on merit and engagement.\nThis approach fosters an agile, bottom-up culture that empowers members to lead, take initiative, and directly shape their project‚Äôs success.","section-3-squad-management#Section 3: Squad Management":"Squads are the fundamental working units within each WATonomous project or administrative group. They are designed to promote agility, collaboration, and focused execution.Purpose of Squads:\nOrganize members around specific goals, modules, or deliverables.\nEnable parallel workstreams within a larger project.\nEncourage ownership, communication, and iteration.\nSquad Composition:\nLed by a Squad Lead, who coordinates execution and ensures alignment with the project‚Äôs direction.\nComprised of Core Members, who contribute directly to the squad‚Äôs objectives.\nMay also include Incoming Members, who support tasks and ramp up under mentorship.\nA squad should consist of no more than 5 people. Otherwise, the scope of the squad should be reconsidered, and be divided up into smaller scopes for multiple squads to form.\nSquad Lead Responsibilities:\nSet meeting cadence and manage internal communication.\nTrack progress on squad goals and report to Co-Directors.\nSupport technical or logistical problem-solving within the group.\nAdjust squad composition or task focus as project needs evolve.\nFlexibility & Rotation:\nSquad Leads may rotate based on task cycles, availability, or project phase.\nSquads can be reorganized or dissolved at the discretion of the project‚Äôs Co-Directors to meet changing priorities.\nA squad cannot fully dissolve until comprehensive documentation of their contributions have been made. This is at the discretion of the Project Directors, other Squad Leads, and optionally Co-Captains.\nThis squad system provides a scalable, modular structure that adapts to project complexity and fosters leadership development across the team.","section-4-project-management#Section 4: Project Management":"A Project refers to a semi-autonomous team within WATonomous focused on a specific application of autonomous robotics. The number and scope of active projects may evolve over time. This section outlines the formal procedures for managing the lifecycle of a project.4.1 Establishing a Project\nAny member may propose a new project by submitting a short proposal to leadership (Directors or Co-Captains through public channels or direct messaging).\nThe proposal must include:\nProject name and scope\nTechnical objectives\nRelevance to the broader goals of WATonomous\nApproval requires a 80% vote from the leadership team.\nOnce approved, the project is added to the active roster and is granted access to shared resources.\nDirectorship and Squad Leadership of a project is determined during the project‚Äôs first official meeting. Members interested in leadership roles are expected to self-organize and reach a consensus. The final selection of Project Directors and Squad Leads must be communicated to WATonomous leadership within one week of the meeting.\n4.2 Pausing a Project\nA project may be paused if:\nIt is temporarily inactive due to academic term transitions\nIts members are reassigned to higher-priority efforts\nThe project directors must notify the rest of leadership with a proposed timeline for reevaluation.\nA paused project is not expected to meet standard term-end report.\n4.3 Merging Projects\nTwo or more projects may be merged if they share significant technical or logistical overlap.\nThe proposal must be initiated by one or more project leads and approved by a 80% vote of leadership.\nA new lead structure must be proposed for the merged project.\n4.4 Terminating a Project\nA project may be terminated if:\nIt has been inactive for over one term without a valid justification\nIt no longer aligns with WATonomous' long-term goals\nIt repeatedly fails to meet deliverables or maintain team structure\nTermination requires a 80% vote of leadership and must be communicated clearly to all members.\n4.5 Evaluating Projects\nProjects are reviewed at the end of each academic term by the leadership team.\nEvaluation criteria may include:\nProgress on technical milestones (see Goal Tracking)\nQuality and consistency of member engagement\nContribution to the overall mission of WATonomous\nDocumentation, demos, or integration into broader team efforts\nProjects failing to meet expectations may be recommended for support, restructuring, or sun-setting.","article-iv--role-descriptions#Article IV ‚Äì Role Descriptions":"Faculty Advisors and Alumni Board\nServe in an advisory capacity to provide long-term guidance, mentorship, and institutional memory.\nDo not participate in day-to-day operations but may review major strategic initiatives.\nServe as a direct connection to the University of Waterloo and Industry\nCo-Captains\nRepresent the overall WATonomous team and are responsible for strategic vision, inter-project coordination, and organizational cohesion.\nFacilitate communication across directors, oversee high-level planning, and ensure alignment with long-term goals.\nAppoint or confirm project and administrative leadership roles when necessary.\nDirectors and Co-Directors\nLead either an administrative vertical (e.g., BizOps, WATcloud) or a technical project (e.g., Rover, Eve).\nDefine goals, timelines, and processes for their domain.\nAre accountable to the Co-Captains for progress and member well-being within their teams.\nCo-Directors of a project operate with equal authority to enable collaborative leadership and load sharing.\nSquad Leads\nCoordinate and guide a focused group (or ‚Äúsquad‚Äù) within a project or administrative team.\nLead execution of key tasks or deliverables and facilitate collaboration among Core Members.\nAre appointed by the Co-Directors of their project based on need, expertise, or initiative.\nMay change over time depending on project needs (adaptive leadership).\nCore Members\nActively contribute to a project or administrative initiative on a regular basis.\nTake ownership of technical tasks, documentation, testing, event planning, or other responsibilities as needed.\nAre encouraged to pursue Squad Lead roles based on interest and engagement.","article-v--goal-tracking#Article V ‚Äì Goal Tracking":"WATonomous uses a Quest System to clearly define, track, and evaluate term goals. This system provides transparency, motivation, and alignment across all projects and administrative efforts. Each term, every robotics project and administrative unit maintains its own Quest Book, which serves as the definitive record of that group's objectives, their scoring criteria, and their progress.","section-1-quest-books#Section 1: Quest Books":"A Quest Book is the official document outlining the objectives (\"Quests\") for a given term within a specific robotics project or administrative unit.\nIt contains:\nThe Great Objective representing the robotics project's overarching goal, established at the time of its inception.\nA series of Term Objectives, each with defined scoring criteria and deadlines.\nThe Quest Book is treated as the source of truth for what that group aims to accomplish during the term.\nEach Quest is assigned a scoring weight of /5, /10, or /20, depending on its scope and importance.","section-2-how-quest-books-are-written#Section 2: How Quest Books Are Written":"Quests are proposed by Co-Captains and Directors prior to the start of each academic term.\nA separate Quest Book is maintained for each project (e.g., Eve, Rover, Humanoid, F1Tenth) and administrative group (e.g., BizOps, WATcloud).\nAll proposed Quests must follow the standardized Quest Template, including:\nClear descriptions\nSpecific due dates\nTransparent scoring breakdowns with minimum requirements for partial credit\nEach Quest Book must be reviewed by:\nThe respective project or group‚Äôs Directors\nThe Co-Captains\nOther relevant stakeholders (e.g. Advisors, Alumni)\nOptionally, Squad Leads should their squad persist between terms\nQuest Books are approved by Consensus among reviewers, ensuring fairness, clarity, and alignment with organizational priorities.\nOnce approved, Quest Books are published to the team and archived in the team wiki.","section-3-scoring#Section 3: Scoring":"Each Quest is scored on its assigned due date, during a live scoring meeting.\nPresent at the meeting must be the Director(s), Squad Lead(s), and key members responsible for that Quest.\nScoring follows the criteria defined in the Quest Book; subjective adjustments are not permitted.\nCompleted Quests are announced in team-wide communications (e.g., Discord, or All-Hands).\nResidual Quests may be created for partially completed work. These are added to the Quest Book with new due dates and scoring.\nOnce awarded, scores are final and cannot be changed.\nAt the end of the term, each Quest Book‚Äôs final scores are:\nPublished in team-wide announcements\nPermanently archived in the documentation or wiki\nA Quest cannot by considered done until comprehensive documentation has been made. This is at the discretion of the Project Directors, Squad Leads, and optionally Co-Captains.","section-4-quest-ownership-and-accountability#Section 4: Quest Ownership and Accountability":"Each Quest listed in a Quest Book must have clearly defined ownership to ensure accountability and clarity throughout the term.Roles and Responsibilities:\nDirectors are responsible for:\nDefining the high-level objectives within their project or group.\nEnsuring that Squad Leads and Core Members are aware of expectations.\nParticipating in the scoring process for their assigned Quests.\nBreaking down Quests into actionable tasks that will form squads.\nSquad Leads are responsible for:\nFurther break down of tasks into action items for the squad.\nTracking progress and removing blockers.\nLeading discussions related to their squad‚Äôs work.\nCore Members are responsible for:\nExecuting tasks tied to Quests.\nDocumenting progress and providing relevant evidence for scoring.\nProactively reporting issues to Squad Leads.\nOwnership Guidelines:\nEvery Quest must list at least one responsible Director or Squad Lead.\nQuests should not remain ownerless; if leadership transitions mid-term, responsibility must be reassigned.\nFor cross-functional Quests involving multiple squads, a primary lead must be designated for scoring purposes.\nPerformance Transparency:\nQuest ownership contributes to squad and team visibility during the term.\nFinal scores may be used to inform leadership decisions such as promotions, acknowledgments, and end-of-term evaluations.","article-vi--leadership-selection#Article VI ‚Äì Leadership Selection":"","section-1-term-limits#Section 1: Term Limits":"Term limits are imposed to facilitate a healthy transition of power and strengthen our team's ability to transfer knowledge across generations. Each lead position has its own term limit:\nCo-Captains and Project Directors: 6 terms. However, no two Co-Captains or Project Co-Directors can both be in 4th year. This is to reduce the chances of a succession crisis.\nSquad Leads: No term limit. However, continuation in the role is at the discretion of the respective Project Director.","section-2-transitioningpicking-leadership#Section 2: Transitioning/Picking Leadership":"Leads may resign from their position for two reasons:\nTheir term limit has been reached.\nBoth leads are 4th years. In that case, one can voluntarily resign.\nThey no longer wish to continue in a leadership role.\nWhen a lead resigns, a new lead must be appointed. The process depends on the role of the resigning lead:","co-captain#Co-Captain":"New Co-Captains are elected by a vote from the current Directors of WATonomous. A candidate must receive at least 80% of the vote to be selected. If no candidate meets this threshold, a re-vote is held. Candidates are encouraged to resolve differences and consolidate interests between re-votes.Qualifications:\nMust have held a lead position in WATonomous for at least one term.\nMust not have previously served as a Co-Captain.\nTransition Process:\nA Co-Captain announces their resignation, either due to term completion or personal choice.\nA team-wide announcement is made, and the search for candidates begins.\nThe first vote is held one week after the resignation announcement.\nIf the first vote fails, re-votes may occur, no sooner than 30 minutes after the previous vote.\nOnce a new Co-Captain is selected, a 1-month transition period begins, during which responsibilities and knowledge are transferred.\nNote: The resigning Co-Captain remains responsible for day-to-day tasks until the transition is complete.","director#Director":"New Directors are identified and selected by current Co-Captains and Directors.Qualifications:\n(Optional) Previously served as a Lead.\n(Optional) Expressed interest in leadership to current Directors or Co-Captains (via DMs or public messages).\nDemonstrated leadership ability.\nTransition Process:\nA Director announces their resignation, either due to term completion or personal choice.\nA project-wide announcement is made, and the search for a replacement begins.\nThe resigning Director is responsible for identifying a suitable replacement.\nThe candidate must receive 80% approval from all other Directors and Co-Captains.\nIf the first vote fails, other Directors or Co-Captains may propose additional candidates. Re-votes may occur no sooner than 30 minutes after the previous vote.\nOnce a new Director is selected, a 2-week transition period begins, during which responsibilities and knowledge are transferred.\nNote: The resigning Director remains responsible for day-to-day tasks until the transition is complete.","squad-lead#Squad Lead":"New Squad Leads are selected by the Project/Division Directors and are appointed based on the tasks at hand.Qualifications:\n(Optional) Expressed interest in leadership to Directors or Co-Captains (via DMs or public messages).\nDemonstrated leadership ability.\nTransition Process:\nNew tasks emerge that require the formation of a squad (e.g., perception, electrical, etc.). The Director defines the squad's scope.\nThe Project Director appoints a Squad Lead.\nThe Squad Lead remains in position until the Director reassigns the role or the squad is dissolved.\nIf the Squad Lead resigns while the squad is still active, they are responsible for recommending a replacement.\nIf the recommended replacement is not approved by the Directors, the Directors will select an alternative candidate.\nOnce a replacement is selected, a 1-week transition period begins for knowledge transfer.\nNote: Squad Leads and squads are fluid and form based on the tasks at hand. Resigning Squad Leads are expected to retain responsibilities until a replacement is confirmed.","alumni-board-member#Alumni Board Member":"The role of an Alumni Board Member is reserved for individuals who have made a lasting impact on WATonomous. These members serve as stewards of institutional memory and provide valuable connections to graduate programs and industry.Eligibility Criteria\nAn individual may be considered for Alumni Board Membership if they meet at least one of the following:\nFormer Co-Captain of WATonomous\nEligibility begins immediately after their term concludes.\nSignificant Contributor to the team, regardless of title or role\nMust be nominated by a current Director, Co-Captain, Faculty Advisor, or existing Alumni Board Member.\nNomination and Selection Process\nNominations trigger a formal vote by current team leadership (Co-Captains, Directors, and optionally Squad Leads).\nPrior to the vote, either the nominee or nominator may present their case to leadership.\nAlumni Board Membership is granted upon receiving at least 80% approval from current leadership active during the formal vote.\nNominations may only proceed when WATonomous is not in a state of emergency.\nThe purpose of this membership is to recognize individuals who have sacrificed their lives (whether it be health, employment, etc.) in the pursuit of helping others. Although the system in place is honour-based, this mechanism should not be abused. Doing so affects the integrity of the team, and brings into questions whether WATonomous should still exist as an entity.","article-vii--membership#Article VII ‚Äì Membership":"","section-1-eligibility#Section 1: Eligibility":"Membership is open to:\nStudents and faculty of the University of Waterloo\nAlumni of the University of Waterloo\nNote: Special arrangements may be made for individuals outside the University of Waterloo if approved by an 80% majority vote from the WATonomous leadership.\nTo join WATonomous, the application process is determined by the current leadership and may vary over time. The most current application process will always be properly explained and documented on the WATonomous website and wiki.","section-2-expectations#Section 2: Expectations":"Members are expected to:\nParticipate in meetings or team activities\nContribute regularly to projects or club events","section-3-revoking-membership#Section 3: Revoking Membership":"Membership may be revoked for the following reasons:\nProlonged lack of active participation on the team\nActs of violence, harassment, racism, or bigotry witnessed and documented internally\nNote: WATonomous reserves the right to escalate sensitive cases to an appropriate governing body at the University of Waterloo.","article-viii--amendments#Article VIII ‚Äì Amendments":"Any member may propose amendments to this constitution.\nAmendments must be approved by a 80% majority of Directors, Co-Captains, and active Alumni Board Members. A notification of the meeting must be sent out to all parties involved at least 1 week in advance.\nFinal approval (over GitHub PR), must be made by one of the Co-Captains.\nCommit history will be used as a direct reference to the change history of this constitution.","how-to-structure-an-amendment#How to structure an Amendment":"Make a change to the constitution in a GitHub branch.\nCreate a Pull Request to the main branch of the wiki, outlining the changes made to the constitution.\nMerging is not permitted until the proper amendment process is followed.","article-ix--emergency-clause#Article IX ‚Äì Emergency Clause":"In times of emergency, term limits may be suspended, and leadership transitions will be handled by the current Leadership Team, Faculty Advisors, and active Alumni Board Members.","section-1-what-constitutes-an-emergency#Section 1: What Constitutes an Emergency?":"An emergency may be declared if any of the following conditions are met:\nWATonomous membership drops below 20 people.\nA serious accident causes hospitalization of team members.\nNatural disasters or major political events affecting the region (e.g., war, martial law, rebellion, or murder).","section-2-declaration-and-termination-of-emergency#Section 2: Declaration and Termination of Emergency":"Declaring an Emergency:\nA State of Emergency may be declared if:\nOne or more qualifying emergency conditions are present, and\nCo-Captains agree to initiate emergency status:\nA formal vote or written record must be created confirming the declaration.Emergency Leadership Meeting:\nOnce declared, a meeting must be announced team-wide at least one week in advance of when it will be held. This meeting must address:\nTeam Strategy ‚Äì How the team will respond to the crisis, including concrete milestones and task ownership.\nLeadership ‚Äì Who will guide the organization through this period.\nDecision-Making: Full consensus is preferred. If consensus cannot be reached, decisions may proceed with a 80% vote among attendees representing at least two of the three stakeholder groups (Leadership, Faculty, Alumni Board Members).\nEnding an Emergency:\nA State of Emergency may be ended if:\nThe original condition(s) no longer apply,\nA review meeting is held with representation from the Leadership Team, Faculty Advisors, and Alumni Board,\nAnd a 80% vote from these groups confirms readiness to return to normal operations.\nA minimum of two weeks must pass between the declaration and termination of an emergency unless otherwise agreed by all three groups.","section-3-failsafe-procedures#Section 3: Failsafe Procedures":"In the event that quorum cannot be met due to incapacity, absence, or disengagement, the following backup measures apply:\nQuorum Flexibility: A minimum of four participants from at least two different groups (e.g., Faculty + Alumni, or Leadership + Faculty) may convene an emergency meeting.\nSuccession Protocol: If both Co-Captains are incapacitated, any current Director or Squad Lead may temporarily assume emergency coordination duties until quorum is restored.\nRemote Participation: Emergency meetings may be held virtually or asynchronously (e.g., via email or team-wide poll) if timely response is required.\nCooldown Period: A new emergency may not be declared within three months of a previous one unless unanimously approved by all three stakeholder groups."}},"/about/projects":{"title":"Projects","data":{}},"/administrative":{"title":"Administrative","data":{}},"/about/social_channels":{"title":"Social Channels","data":{}},"/administrative/important_dates":{"title":"Important Dates","data":{}},"/administrative/purchasing_and_reimbursements":{"title":"Purchasing and Reimbursements","data":{}},"/admission_assignments":{"title":"WATonomous Admission Assignments","data":{"":"This is a directory of Assignments that are used to evaluate and train potential members of the team. Rule is, if you finish the assignment, then you are part of the team. Note: individuals can skip the assignment should they posses skills that WATonomous desperately needs.Note: WATonomous hosts waves of group-admissions, where people interested in joining the team can work together to finish the assignment. This is called the Expedited Admission, and anyone who completes the assignment partially (cutoff determined on date expedited period ends) is allowed to join the team.","ready-to-learn-about-robotics#Ready to learn about robotics?":""}},"/admission_assignments/apd_admission_assignment":{"title":"APD Admission Assignment","data":{"":"We currently do not have a generalized assignment meant for those interested in the mechatronics aspects of robotics."}},"/admission_assignments/asd_admission_assignment":{"title":"ASD Admission Assignment","data":{"":"The ASD Admission Assignment seeks to teach you all the relevant software tools and robotics knowledge you'll need for your future career in state-of-the-art robotics.\nThis assignment can be a bit daunting, but we promise that with grit, you can complete this assignment regardless of your current capabilities. Feel free to ask any questions on Discord.","rules#Rules":"If you finished the assignment (or the deadline has been reached), submit a link to your code on github plus video proof to the assignment-completions discord channel.\nYou're allowed to work in groups of 3. Be sure to credit your group members in the submission. You can look for group members in the assignment-groups discord channel.\nYou are allowed to use AI tools such as LLMs.\nYou are allowed to look at the answer. If you know how to find it. We'll know if you used the answer though ;)\nWhy so many loopholes? It's because we aren't testing you. It's to give you a chance to really hold yourself accountable, and explore the interesting world of robotics. Sure you can copy your way through, but what's the point? Are you really becoming a better you by doing that? Do you care more about your own personal growth or more about superficially showing yourself off?","goal-of-the-assignment#Goal of the Assignment":"In this assignment, you are tasked with giving a simulated robot the intelligence to navigate from point-to-point while avoiding static objects. The robot is differential drive and has a camera and laser scanner.\nThe code you will write, and the libraries you will use, are industry-standard. That means you will be writing code that could be transferable to companies you will be working for in the future! Here's a non-exhaustive list:\nNvidia\nGoogle (Intrinsic)\nAmazon (Amazon Robotics)\nTesla\n... and MANY robotics startups\nOf course, we cannot cover everything. So feel free to use this assignment as a good starting point to get into the world of robotics. :)","getting-started#Getting Started":"","prerequisites#Prerequisites":"We recommend you refresh your knowledge of the following before beginning the assignment:\nGitHub and git\nnavigating the terminal\nC++ and Python (You will be using C++ for this assignment because it is easier to learn rclpy coming from rclcpp)\nConcurrent Programming and Interprocess Communication","setup#Setup":"The ASD Admission Assignment is compatible with Linux, Windows (WSL), and MacOS. We utilize docker for ease of reproducibility and deployability, so you'll barely need to install anything on your own computer to make this assignment work!In the terminal:\nDownload Docker Engine (or Docker Desktop if you have no choice)\nClone the WATonomous ASD Admissions Assignment\ngit clone git@github.com:WATonomous/wato_asd_training.git\nYou're ready to begin!","warm-up-how-to-use-the-repository#Warm Up: How to use the Repository":"The ASD Assignment utilizes our custom-built Monorepo Infrastructure. It is a Docker Compose wrapper that orchestrates various concurrent programs together. It also lets us build code, setup VScode Intellisense, and deploy to robots.In this warm up, you will learn how to use the infrastructure. More specifically, how to open Foxglove to visualize data, make changes to the code, build the code, and witness your changes on Foxglove. We highly recommend you reference the Monorepo Infrastructure Docs while you go through this warm up and the rest of the assignment.","first-encounter-with-foxglove#First Encounter with Foxglove":"1. Set your watod-config.sh to run robot, gazebo, and vis_tools as the active modules.\nACTIVE_MODULES=\"robot gazebo vis_tools\"\n2. Build these module images\n./watod build\n3. Up the module containers\n./watod up\n4. Figure out your Foxglove URL\nIf you are developing locally, this can be found in the logs after you run ./watod up, it should look like something along the lines of https://localhost:#####\nIf you are developing on watcloud, it should act the same as deving locally. If not, check that the port is being forwarded by looking at the PORTS tab in VScode\n5. Open up Foxglove (web app or desktop app). Click Open Connection and enter the URL you found previously. The Foxglove dashboard should open.6. Import the pre-made Foxglove layout located in config/wato_asd_training_foxglove_config.json into the Foxglove Dashboard.\nYou should see the following:\nYou can move the robot around using the Teleop Panel.","coding-the-robot-and-propagating-the-change-into-foxglove#Coding the Robot and Propagating the change into Foxglove":"This section will get you to integrate a simple publisher node using one of the empty packages located in the src directory. Later on, we will give you a rundown on what you are actually writing.0. Get Intellisense working. This will give you code completion on VScode.\n./watod --setup-dev-env robot\n1. In src/robot/costmap/include/costmap_node.hpp place the following code\n#ifndef COSTMAP_NODE_HPP_\n#define COSTMAP_NODE_HPP_\n#include \"rclcpp/rclcpp.hpp\"\n#include \"std_msgs/msg/string.hpp\"\n#include \"costmap_core.hpp\"\nclass CostmapNode : public rclcpp::Node {\n  public:\n    CostmapNode();\n    \n    // Place callback function here\n    void publishMessage();\n  private:\n    robot::CostmapCore costmap_;\n    // Place these constructs here\n    rclcpp::Publisher<std_msgs::msg::String>::SharedPtr string_pub_;\n    rclcpp::TimerBase::SharedPtr timer_;\n};\n#endif \n2. In src/robot/costmap/src/costmap_node.cpp place the following code\n#include <chrono>\n#include <memory>\n#include \"costmap_node.hpp\"\nCostmapNode::CostmapNode() : Node(\"costmap\"), costmap_(robot::CostmapCore(this->get_logger())) {\n  // Initialize the constructs and their parameters\n  string_pub_ = this->create_publisher<std_msgs::msg::String>(\"/test_topic\", 10);\n  timer_ = this->create_wall_timer(std::chrono::milliseconds(500), std::bind(&CostmapNode::publishMessage, this));\n}\n// Define the timer to publish a message every 500ms\nvoid CostmapNode::publishMessage() {\n  auto message = std_msgs::msg::String();\n  message.data = \"Hello, ROS 2!\";\n  RCLCPP_INFO(this->get_logger(), \"Publishing: '%s'\", message.data.c_str());\n  string_pub_->publish(message);\n}\nint main(int argc, char ** argv)\n{\n  rclcpp::init(argc, argv);\n  rclcpp::spin(std::make_shared<CostmapNode>());\n  rclcpp::shutdown();\n  return 0;\n}\n3. Because you added a new ROS2 libary std_msgs, you need to instruct ROS2's package manager to install and compile with std_msgs\nIn src/robot/costmap/package.xml add in the following:\n<?xml version=\"1.0\"?>\n<package format=\"3\">\n  <name>costmap</name>\n  <version>0.0.0</version>\n  <description>A sample ROS package for pubsub communication</description>\n  <maintainer email=\"oleather@watonomous.ca\">Owen Leather</maintainer>\n  <license>Apache2.0</license>\n  <!--https://www.ros.org/reps/rep-0149.html#dependency-tags-->\n  <buildtool_depend>ament_cmake</buildtool_depend>\n  <depend>rclcpp</depend>\n  <!--YOU ARE ADDING THIS MAINLY-->\n  <depend>std_msgs</depend>\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n  <test_depend>ament_cmake_gtest</test_depend>\n  <!--https://www.ros.org/reps/rep-0149.html#export-->\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\nThis is telling the ROS2 pacakge manager to install the std_msgs library.\nIn src/robot/costmap/CMakeLists.txt add in the following:\ncmake_minimum_required(VERSION 3.10)\nproject(costmap)\n# Set compiler to use C++ 17 standard\nif(NOT CMAKE_CXX_STANDARD)\n  set(CMAKE_CXX_STANDARD 17)\nendif()\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n# Search for dependencies required for building this package\nfind_package(ament_cmake REQUIRED) # ROS2 build tool\nfind_package(rclcpp REQUIRED)      # ROS2 C++ package\nfind_package(std_msgs REQUIRED)    # YOU ARE ONLY ADDING THIS TO THE FILE\n# Compiles source files into a library\n# A library is not executed, instead other executables can link\n# against it to access defined methods and classes.\n# We build a library so that the methods defined can be used by\n# both the unit test and ROS2 node executables.\nadd_library(costmap_lib\n  src/costmap_core.cpp)\n# Indicate to compiler where to search for header files\ntarget_include_directories(costmap_lib\n  PUBLIC include)\n# Add ROS2 dependencies required by package\nament_target_dependencies(costmap_lib \n  rclcpp\n  std_msgs # YOU ARE ONLY ADDING THIS TO THE FILE\n)\n# Create ROS2 node executable from source files\nadd_executable(costmap_node src/costmap_node.cpp)\n# Link to the previously built library to access costmap classes and methods\ntarget_link_libraries(costmap_node costmap_lib)\n# Copy executable to installation location\ninstall(TARGETS\n  costmap_node\n  DESTINATION lib/${PROJECT_NAME})\n# Copy launch and config files to installation location\ninstall(DIRECTORY\n  config\n  DESTINATION share/${PROJECT_NAME})\nament_package()\nThis is telling ROS2 compiler that you need std_msgs to compile the code.4. Rebuild and Rerun the robot\n# In a separate terminal while the other modules are running\n./watod down robot # shuts down the robot service\n./watod build robot # rebuilds the robot service\n./watod up robot # starts the robot service up again\nYou can also just down, build, and up the whole thing. But this will eat up precious time.5. View your changes on Foxglove\nCreate a new panel that views raw messages, enter /test_topic as the topic you want to view\nYou should see something like the following:\nCongratulations! You now know how to make a code change and have it propagate into Foxglove!","what-did-i-just-write#What did I just Write?":"From the last example, you were probably thinking, \"What the hell did I just write?\" Well, you just wrote C++ code using the ROS2 Humble library! ROS2 is a powerful toolset of C++/Python libraries, build utilities, orchestration managers, logging tools, and visualization software used by the open-source robotics community. ROS originally spawned in the Bay Area as a way to mitigate \"reinventing the wheel\" of robotics infrastructure. Nowadays, it is used by a plethora of startups, and its repository has be forked many times for companies such as Cruise and Zoox. Currently, the maintainers of ROS were aquired by Google as a political play. You can learn more about the history of ROS here.So how do you use ROS? Learning ROS (and robotics programming in general) takes time. As you make more mistakes, you will get a better understanding of it. For now, we will only go over some basic aspects of ROS that are expecially relevant to this assignment.\nWe use ROS and ROS2 interchangeably. But keep in mind that they are two different libraries that you download separately. That being said, there is a lot that is similar between them.","ros-system-diagram#ROS System Diagram":"Here's a simple diagram of three ROSnodes talking to each other. This is the generic way most robotics developers look at robotics systems.","ros-node#ROS Node":"A process that is running instances of ROS objects is called a ROSnode. What this means is, as long as your code has ROS objects that listen/publish data to the network, then it is considered a ROSnode. You can think of ROSnodes as the building blocks of a robotics system.","ros-messages-and-topics#ROS Messages and Topics":"ROS at its core is a framework for interprocess communication. Given two processes (ROSnodes) running, how do you make them communicate with each other? Especially when one process requires the output of another.The way ROS does this is through passing messages over a transport protocol such as UDP or TCP. In simpler terms, it is packaging data into a format that can be passed throughout your computer's internal network, at which point another program can then read that pacakged data, unpack it, and use it for processing.ROS communication involves two things: topics, and messages. A message is an arbitary data structure (an integer, or struct of multiple objects) that is used to represent the data that two or more processes will read and write. In ROS, there are a ton of prebuilt messages that can be used for your robotics project. We often don't need to create our own custom messages, but when we do, we have to build messages using ROS's build tools. You can think of messages as nothing more than the data that is to be sent between processes.As a result, another question arises: How do we manage where to read and write messages? For example, if two processes are publishing the same message type, then how can we differentiate between a message coming from one process vs coming from the other process? Well that is where topics become important. ROS topics represent the medium in which messages should be passed through. You can think of them as URLs to a website. It you specify one URL in you browser, then you will get all the information available in that URL (the website itself) Topics functions very similarly. You can specify the topic a process can read/write to, and ROS fill figure out all of the networking jargon in the background.You read more about it here","ros-publishers-and-subscribers#ROS Publishers and Subscribers":"ROS utilizes these topics and messages with simple callable objects in C++ and Python. If you want to send messages out to a topic, use a publisher. Likewise, if you want to listen for messages in a topic, use a subscriber. The syntax for writing a publisher and subscriber can be found here\nIn this assignment, you will have to split your code into a C++ header file and a regular C++ file. You can ask LLNs how to do this :).\nIn the warmup, we made you write a simple publisher. Please use the warmup code as a reference to how your ROS code should fit into the assignment's code format (as in what should be in the .hpp file and what should be in the .cpp file).","the-skys-the-limit#The Sky's the Limit":"A robotics system can be composed of a bunch of different ROSnodes communcating in a bunch of different ways. In fact, a ROSnode can consist of multiple publishers and subscribers. You just need to be careful of the concurrency between them.Unfortunately, the only way to truly understand ROS is to read their docs and use it. If you have any questions when programming ROS, please ask them in our Discord.","how-to-code-intelligence#How to Code Intelligence":"Now that you've gotten into the swing of things, let's take a step back and take a look at the field of robotics as a whole.A reoccuring question roboticists often have is:\n\"How do we make robots think?\"\nThis question scales with the complexity of the robot. For a line following robot, this could be extremely easy: the robot simply needs to turn slightly left or right depending on where the line is. However, for something as complex as a humanoid robot, the question becomes alot more daunting.\nLine Follower Robot\tHumanoid Robot\t\t\t\nAs you progress through your career in robotics, you will witness a multitude of robotics systems. Some that can be built by 5 people, others that have to be built by 100+ people (but actually only really written by like 5 people).However, all robotic systems seek to do one thing: mimic aspects of the human brain to achieve a desired goal. Of course, sometimes it's to mimic human cognition entirely, but for most applications of robotics, there is a economical usecase that needs to be fulfilled.In fact, every roboticist falls somewhere along a spectrum. At one end are the dreamers‚Äîa visionary group almost cult-like in their pursuit of playing god, driven by the singular ambition to create artificial general intelligence. At the other end are the pessimists‚Äîpragmatic, sometimes cynical individuals who see robots as little more than tools, focused solely on their limitations and building them only for profit.You decide where you want to be. But nevertheless, let's not start you off pessimistic.","a-simple-model-of-the-human-brain#A Simple Model of the Human Brain":"Human intelligence is a fascinating thing. Somehow this glob of flesh in your head is capable of controlling your every move, imagining the impossible, and deciphering the complexities of the universe. However, your mind has limitations, and all brains are not perfectly unique. In fact, every human brain shares a common set of modules. For example, it is the reason why we can translate languages. Noam Chomsky, a founding figure of linguistics, hypothesized that we as humans share an innate language faculty. That all of our brains have an innate structure responsible for communication. These limitations are what we use to understand the brain, and with that we can spawn hypotheses on how we think.","heres-a-good-guess-its-very-limited-but-its-good-enough-for-most-roboticists#Here's a good guess (it's very limited, but it's good enough for most roboticists):":"Generally, we as humans do the following:\nPerception: Identify, organize, and interpret information from our five senses.\nWorld Modeling: Utilize and contextualize what we perceive to build an inner model of the world. We also build higher-order attributes on these percepts for the sake of reasoning and predicting possible future states of the world.\nMemory: Store our models of the world, our actions, and their resulting consequences, for the sake of optimizing future actions.\nConfiguration: Decides what we should pay attention to. This part of our brain takes in input from all the other parts and configures them to perform the task at hand.\nAction: Plans and executes our desired actions based on the task at hand. This ranges from long-term planning down to distinct movements in your muscles.\nLet's describe your brain at this very moment... Right now your brain is perceiving and identifying your surroundings. Your eyes recognize the computer screen in front of you, a desk, a wall, perhaps a keyboard and mouse. At the same time, you also understand that the desk or computer screen in front of you is the same desk and computer screen you saw 2 seconds ago. That is your perception.After that, your brain stitches what it senses into a inner model of the world. You know that the computer screen in front of you is on top of the desk you are working on. It is the same reason why you still roughly know the structure of the room around you when you close your eyes. This inner world model is also used to draw connection and implications. For example, you know that a computer screen normally doesn't move on its own. These sorts of implications allow our brains to predict possible future states of the world.Based on all the information your brain has grabbed so far, you can then go on to decide what you want to pay attention to. For example, your mind is probably focused on finishing this assignment. Therefore, you configured your mind to be in a state that can finish coding assignments.Finally, now that you know as much as you can about the world, and what you want to do, you now plan you course of action. In order to finish your coding assignment, you know that you need to stay seated at your desk, and your hands ready to type. You brain converts these planned intentions into primitive motor movements such as moving the muscles in your fingures and your eyes.Once you know the results of your actions, your brain stores those results in your memory. You can then use past results to further improve your future actions.","how-does-the-brain-tie-into-robotics#How does the Brain Tie into Robotics?":"Every robotics system can be thought of in the context of the modules explained above. No matter how simple or complex the robot is, its inner software system generally follows a flow of information similar to our simplified model of the brain.\nThis is a random AV architecture we pulled from online. Usually software architectures are grouped differently by pretty much everyone. Try connecting its various modules to our simple model of the human brain.\nTake an autonomous vehicle for example. It's perception modules are its sensors (camera, radar, lidar, sonar, etc.). Its world model is a highly detailed map of the road (including lanes, traffic signs, lights, etc. This is known as an HD Map). The car also uses its world model to predict the future paths of people and cars around it. Its configurator is its behaviour tree, which is a more complex state machine that defines what the robot should do based on the current state its in. Finally, its action is the various levels of planning: Global Planning, Local Planning, and Control sequences to steer the vehicle and control its throttle.Something more simple can be a line follower robot. This robot's perception is a single sensor that tracks where the line is. Its world model is very limited, as it only needs to know where it is with respect to the line. Likewise, its configuration is sparse because its sole purpose is to follow a line. Finally, its action module just moves the left of right wheel faster based on where the line is with respect to the robot. Overall, you should be noticing a pattern here. We can view any robot through the lens of a simplified model of the human brain.","time-to-finish-your-assignment#Time to Finish your Assignment":"In this assignment, you are given a simulation of a robot with a laserscanner and a camera. Because the laser scanner gives us a very good understanding of how far hard objects are away from us, we will only the laser scanner for this assignment. The goal of the robot is to move to a point specified by the user.\nNote, to make the assignment easier, we have already given you the exact position of the robot with respect to the map. This is represented by an Odometry message present in the /odom/filtered topic. In real life, finding the exact position of a robot is impossible, and engineers use complex probabilistic models and sensor fusion (GPS, Lidar, Camera, IMU, ...) to figure out the best possible estimate of a robot's position.\nThink back to the previous section: if you were the robot in that simulation, and all you knew was how to control yourself, where your goal position was, and how far dangerous things are from you, then how would you plan to get to that point?\nFrom before, you know that this robot will need the following high-level modules: perception, world modeling, memory, configuration, and action. With time, you will learn to associate different algorithms to different modules, but for now, we will just tell you a good sequence of modules that can solve the assignment. FEEL FREE TO TRY DIFFERENT APPROACHES.","a-simple-navigation-architecture#A Simple Navigation Architecture":"Below is a diagram of a simple navigation architecture that can be used to help the robot navigate from point A to point B.Here is a description of each node:\nCostmap: A ROSnode that takes in LaserScans from the /lidar topic, and converts them into a costmap. This is a discretized grid of squares that represent the chances as object exists in that grid with an arbitrary score.\nMap Memory: A ROSnode that take in the costmaps overtime and stitches them together to form a global map. This utilizes the robot's own position in the map found as an Odometry message in the '/odom/filtered' topic.\nPlanner: A ROSnode that takes in the global map and odometry to use the A* algorithm to plan how to get from point A to point B.\nControl: A ROSnode that takes in the path from the planner and odometry, and does Pure Pursuit control to physically move the robot along that path. It does this by publishing Twist messages to the '/cmd_vel' topic.\nWith these four nodes, you can make the robot navigate to anywhere in the map!","implementation-details#Implementation Details":"This section will be details on how to program each node. You can use this section as a reference for your own code, and as a foundation for where to get started.","costmap-node#Costmap Node":"Should contain the following ROS constructs:\n1 Subscriber that subscribes to the '/lidar' topic for sensor_msgs::msg::LaserScan messages\n1 Publisher that publishes nav_msgs::msg::OccupancyGrid messages to a '/costmap' topic\nEquivalent to basic pereption, the Costmap Node is responsible for processing data from laserscans to create a discretized map of where the robot can and cannot go. A simple costmap can have binary values 0 and 1 and is called an occupancy grid. However, often sensor readings are noisey, so when a laser scan tells you something is 10m away, it might actually be 9.5-10.5 meters away. That is why a costmap is useful. It allows us to represent the map as a range of values representing the 'cost' that would incur should the robot cross into that part of the map. This means that a high cost equates to places where objects are definitely present and the robot should avoid. Areas of the map with low cost represent places where we are uncertain of whether an object is near, but we should trek safely nontheless.","steps-to-create-a-basic-costmap#Steps to Create a Basic Costmap":"Subscribe to the /lidar Topic\nReceive sensor_msgs::msg::LaserScan messages from the /lidar topic.\nThe LaserScan message contains:\nangle_min, angle_max: The start and end angles of the scan.\nangle_increment: The angular resolution of the scan.\nranges: Array of distance measurements.\nInitialize the Costmap\nCreate a 2D array to represent the OccupancyGrid. Each cell corresponds to a grid space in the real world.\nDefine the resolution of the costmap (e.g., 0.1 meters per cell) and its size.\nInitialize all cells to a default value (e.g., 0 for free space).\nConvert LaserScan to Grid Coordinates\nFor each valid range value in ranges:\nCompute the Cartesian coordinates of the detected point:\nx = range \\cdot \\cos(angle)\ny = range \\cdot \\sin(angle)\nTransform these coordinates into grid indices using the resolution and origin of the costmap.\nMark Obstacles\nSet the cells corresponding to detected obstacle positions to a high cost (e.g., 100 for \"occupied\").\nInflate Obstacles\nDefine an inflation radius (e.g., 1 meter) and a maximum cost for inflated cells.\nFor each obstacle cell:\nCalculate the Euclidean distance to surrounding cells.\nAssign a cost to the surrounding cells based on the distance:\ncost = max\\_cost \\cdot \\left(1 - \\frac{distance}{inflation\\_radius}\\right)\nOnly assign a cost if the calculated cost is higher than the cell's current value.\nDo not assign a cost to cells beyond the inflation radius.\nPublish the Costmap\nConvert the 2D costmap array into a nav_msgs::msg::OccupancyGrid message.\nPopulate the OccupancyGrid fields:\nheader: Include the frame ID and timestamp.\ninfo: Define the resolution, origin, and size of the grid.\ndata: Flatten the 2D costmap array into a 1D array.\nPublish the message to the /costmap topic.","code-example-pseudo-ros-2-implementation#Code Example (Pseudo-ROS 2 Implementation)":"void laserCallback(const sensor_msgs::msg::LaserScan::SharedPtr scan) {\n    // Step 1: Initialize costmap\n    initializeCostmap();\n    // Step 2: Convert LaserScan to grid and mark obstacles\n    for (size_t i = 0; i < scan->ranges.size(); ++i) {\n        double angle = scan->angle_min + i * scan->angle_increment;\n        double range = scan->ranges[i];\n        if (range < scan->range_max && range > scan->range_min) {\n            // Calculate grid coordinates\n            int x_grid, y_grid;\n            convertToGrid(range, angle, x_grid, y_grid);\n            markObstacle(x_grid, y_grid);\n        }\n    }\n    // Step 3: Inflate obstacles\n    inflateObstacles();\n    // Step 4: Publish costmap\n    publishCostmap();\n}","key-points#Key Points":"The LaserScan data provides obstacle locations in polar coordinates, which are converted to grid coordinates.\nThe costmap marks obstacles and inflates costs around them using a linear scale within a defined radius.","map-memory-node#Map Memory Node":"Should contain the following ROS constructs:\n1 Subscriber that subscribes to the '/costmap' topic for nav_msgs::msg::OccupancyGrid messages\n1 Subscriber that subscribes to the '/odom/filtered' topic for nav_msgs::msg::Odometry messages\n1 Publisher that publishes nav_msgs::msg::OccupancyGrid messages to a '/map' topic\n1 Timer that is used to limit the number of times the map is updated (this is for optimization, we don't need to update the map everytime we get a costmap)\nThis node is a combination of world modeling and memory. Given that we know where we can and cannot go at a given time, we can infer the inehrent structure of the entire map as we move the robot around. This is useful because we can stop the robot from retracing its steps and planning a new route that goes through obstacles that we previously detected.To reduce the frequency of map updates, you can design the mapping node to only fuse maps when the robot has moved a set distance. For the explanations below, we use 1.5m.","steps-to-build-and-update-the-map#Steps to Build and Update the Map":"Subscribe to /costmap Topic\nReceive nav_msgs::msg::OccupancyGrid messages from the /costmap topic.\nEach costmap represents the local environment as detected by the robot at a specific moment.\nSubscribe to /odom/filtered Topic\nReceive nav_msgs::msg::Odometry messages from the /odom/filtered topic.\nUse this data to track the robot‚Äôs movement and determine when it has moved 5 meters since the last map update.\nTrack Robot Movement\nStore the robot‚Äôs initial position and periodically compute its distance from the last update position:\ndistance = \\sqrt{(x_{current} - x_{last})^2 + (y_{current} - y_{last})^2}\nAggregate Costmaps into the Global Map\nMaintain a global OccupancyGrid that represents the map of the environment.\nWhen the robot moves 1.5 meters, integrate the most recent costmap into the global map:\nTransform the costmap into the global frame using the robot‚Äôs current position and orientation.\nUpdate the global map by merging the transformed costmap into it, prioritizing new data over old data.\nOptimize with a Timer\nUse a timer to limit the frequency of map updates to a reasonable rate (e.g., every 1 second).\nOnly perform a map update if the robot has moved 1.5 meters since the last update position.\nPublish the Map\nPublish the aggregated global map as a nav_msgs::msg::OccupancyGrid message to the /map topic.","logic-for-map-update#Logic for Map Update":"Check if the robot has moved 5 meters since the last map update.\nIf yes, aggregate the most recent costmap into the global map.\nPublish the updated map.","linear-fusion-of-costmaps#Linear Fusion of Costmaps":"When integrating costmaps:\nIf a cell in the new costmap has a known value (occupied or free), overwrite the corresponding cell in the global map.\nIf a cell in the new costmap is unknown, retain the previous value in the global map.","code-example-pseudo-ros-2-implementation-1#Code Example (Pseudo-ROS 2 Implementation)":"class MappingNode : public rclcpp::Node {\npublic:\n    MappingNode() : Node(\"mapping_node\"), last_x(0.0), last_y(0.0), distance_threshold(5.0) {\n        // Initialize subscribers\n        costmap_sub_ = this->create_subscription<nav_msgs::msg::OccupancyGrid>(\n            \"/costmap\", 10, std::bind(&MappingNode::costmapCallback, this, std::placeholders::_1));\n        odom_sub_ = this->create_subscription<nav_msgs::msg::Odometry>(\n            \"/odom/filtered\", 10, std::bind(&MappingNode::odomCallback, this, std::placeholders::_1));\n        // Initialize publisher\n        map_pub_ = this->create_publisher<nav_msgs::msg::OccupancyGrid>(\"/map\", 10);\n        // Initialize timer\n        timer_ = this->create_wall_timer(\n            std::chrono::seconds(1), std::bind(&MappingNode::updateMap, this));\n    }\nprivate:\n    // Subscribers and Publisher\n    rclcpp::Subscription<nav_msgs::msg::OccupancyGrid>::SharedPtr costmap_sub_;\n    rclcpp::Subscription<nav_msgs::msg::Odometry>::SharedPtr odom_sub_;\n    rclcpp::Publisher<nav_msgs::msg::OccupancyGrid>::SharedPtr map_pub_;\n    rclcpp::TimerBase::SharedPtr timer_;\n    // Global map and robot position\n    nav_msgs::msg::OccupancyGrid global_map_;\n    double last_x, last_y;\n    const double distance_threshold;\n    bool costmap_updated_ = false;\n    // Callback for costmap updates\n    void costmapCallback(const nav_msgs::msg::OccupancyGrid::SharedPtr msg) {\n        // Store the latest costmap\n        latest_costmap_ = *msg;\n        costmap_updated_ = true;\n    }\n    // Callback for odometry updates\n    void odomCallback(const nav_msgs::msg::Odometry::SharedPtr msg) {\n        double x = msg->pose.pose.position.x;\n        double y = msg->pose.pose.position.y;\n        // Compute distance traveled\n        double distance = std::sqrt(std::pow(x - last_x, 2) + std::pow(y - last_y, 2));\n        if (distance >= distance_threshold) {\n            last_x = x;\n            last_y = y;\n            should_update_map_ = true;\n        }\n    }\n    // Timer-based map update\n    void updateMap() {\n        if (should_update_map_ && costmap_updated_) {\n            integrateCostmap();\n            map_pub_->publish(global_map_);\n            should_update_map_ = false;\n        }\n    }\n    // Integrate the latest costmap into the global map\n    void integrateCostmap() {\n        // Transform and merge the latest costmap into the global map\n        // (Implementation would handle grid alignment and merging logic)\n    }\n    // Flags\n    nav_msgs::msg::OccupancyGrid latest_costmap_;\n    bool should_update_map_ = false;\n};","key-points-1#Key Points":"The global map is updated only when the robot moves at least 5 meters and at a controlled frequency.\nThe node uses odometry to track movement and costmaps to update the environment representation.\nA timer ensures that updates do not occur too frequently, optimizing computational resources.","planner-node#Planner Node":"Should contain the following ROS constructs:\n1 Subscriber that subscribes to the '/map' topic for nav_msgs::msg::OccupancyGrid messages\n1 Subscriber that subscribes to the '/goal_point' topic for geometry_msgs::msg::PointStamped messages\n1 Subscriber that subscribes to the '/odom/filtered' topic for nav_msgs::msg::Odometry messages\n1 Publisher that publishes nav_msgs::msg::Path messages to a '/path' topic\n1 Timer that tracks when we have reached the goal, or timedout, and we need to replan\nThe planner node corrosponds to a combination of configuration and action. Because the robots sole purpose is to move to a specified point, its configuration is very simple. Within the planner, you can model its configurator as a simple state machine that switches between two states: waiting for a goal, waiting for robot to reach the goal. These two states are sufficient enough for the task at hand. Furthermore, the planning node also produces a global plan from the robot's starting position to the final goal. It does this by using the discretized grid of the map and iterating through the grid to reach an optimal path. This plan is updated everytime the map updates.To plan from one point in an incomplete map to another, there are a multiple of algorithms you can use. The simplest is Breadth First Search, which will inflate outwards from the robot's starting position, and end when it reaches the goal point. For this assignment, we suggest you make your life more interesting and learn how to use A*. Here's a nice video about how A* worksTo give you an easier time with building A*, here's some structs that could help you with building and traversing the graph:\n// ------------------- Supporting Structures -------------------\n// 2D grid index\nstruct CellIndex\n{\n  int x;\n  int y;\n  CellIndex(int xx, int yy) : x(xx), y(yy) {}\n  CellIndex() : x(0), y(0) {}\n  bool operator==(const CellIndex &other) const\n  {\n    return (x == other.x && y == other.y);\n  }\n  bool operator!=(const CellIndex &other) const\n  {\n    return (x != other.x || y != other.y);\n  }\n};\n// Hash function for CellIndex so it can be used in std::unordered_map\nstruct CellIndexHash\n{\n  std::size_t operator()(const CellIndex &idx) const\n  {\n    // A simple hash combining x and y\n    return std::hash<int>()(idx.x) ^ (std::hash<int>()(idx.y) << 1);\n  }\n};\n// Structure representing a node in the A* open set\nstruct AStarNode\n{\n  CellIndex index;\n  double f_score;  // f = g + h\n  AStarNode(CellIndex idx, double f) : index(idx), f_score(f) {}\n};\n// Comparator for the priority queue (min-heap by f_score)\nstruct CompareF\n{\n  bool operator()(const AStarNode &a, const AStarNode &b)\n  {\n    // We want the node with the smallest f_score on top\n    return a.f_score > b.f_score;\n  }\n};","node-behavior-and-design#Node Behavior and Design":"The planner operates as a state machine with two states:\nWaiting for Goal: The node waits for a new goal point from the /goal_point topic.\nWaiting for Robot to Reach Goal: Once a goal is received, the node plans a path using A* and monitors the robot's progress toward the goal.","key-functionalities#Key Functionalities":"Global Plan:\nUses the A* algorithm to find the optimal path from the robot's current position to the goal on the occupancy grid (/map).\nPathfinding accounts for obstacle constraints and generates a path that minimizes the cost.\nState Transition:\nTransition from \"waiting for goal\" to \"waiting for robot to reach the goal\" occurs when a valid goal is received.\nTransition back occurs when:\nThe goal is reached.\nA timeout is reached.\nThe map updates, requiring a replan.\nTimer for Replanning:\nChecks for timeout or completion conditions periodically.\nReplans if the robot fails to progress or if a new map update invalidates the previous plan.","steps-to-implement#Steps to Implement":"Subscribers:\n/map: Receives nav_msgs::msg::OccupancyGrid messages.\nUsed as the grid for A* pathfinding.\n/goal_point: Receives geometry_msgs::msg::PointStamped messages.\nSpecifies the goal point.\n/odom/filtered: Receives nav_msgs::msg::Odometry messages.\nTracks the robot's current position and progress toward the goal.\nPublisher:\n/path: Publishes nav_msgs::msg::Path messages.\nContains the planned path from the robot's position to the goal.\nTimer:\nPeriodically checks if the goal is reached or if replanning is required.\nEnsures efficient operation by limiting unnecessary updates.\nA* Implementation:\nUse the occupancy grid as the search space.\nInitialize start and goal points based on the robot‚Äôs position and the goal point.\nImplement A*:\nMaintain an open list of nodes to be evaluated and a closed list of evaluated nodes.\nUse a cost function:\n[\nf(n) = g(n) + h(n)\n]\n( g(n) ): Cost to reach the node.\n( h(n) ): Heuristic estimate of cost to reach the goal (e.g., Euclidean or Manhattan distance).\nExpand nodes until the goal is reached or no valid path exists.\nState Machine:\nDefine and handle transitions between \"waiting for goal\" and \"waiting for robot to reach goal.\"","code-example-pseudo-ros-2-implementation-2#Code Example (Pseudo-ROS 2 Implementation)":"class PlannerNode : public rclcpp::Node {\npublic:\n    PlannerNode() : Node(\"planner_node\"), state_(State::WAITING_FOR_GOAL) {\n        // Subscribers\n        map_sub_ = this->create_subscription<nav_msgs::msg::OccupancyGrid>(\n            \"/map\", 10, std::bind(&PlannerNode::mapCallback, this, std::placeholders::_1));\n        goal_sub_ = this->create_subscription<geometry_msgs::msg::PointStamped>(\n            \"/goal_point\", 10, std::bind(&PlannerNode::goalCallback, this, std::placeholders::_1));\n        odom_sub_ = this->create_subscription<nav_msgs::msg::Odometry>(\n            \"/odom/filtered\", 10, std::bind(&PlannerNode::odomCallback, this, std::placeholders::_1));\n        // Publisher\n        path_pub_ = this->create_publisher<nav_msgs::msg::Path>(\"/path\", 10);\n        // Timer\n        timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(500), std::bind(&PlannerNode::timerCallback, this));\n    }\nprivate:\n    enum class State { WAITING_FOR_GOAL, WAITING_FOR_ROBOT_TO_REACH_GOAL };\n    State state_;\n    // Subscribers and Publisher\n    rclcpp::Subscription<nav_msgs::msg::OccupancyGrid>::SharedPtr map_sub_;\n    rclcpp::Subscription<geometry_msgs::msg::PointStamped>::SharedPtr goal_sub_;\n    rclcpp::Subscription<nav_msgs::msg::Odometry>::SharedPtr odom_sub_;\n    rclcpp::Publisher<nav_msgs::msg::Path>::SharedPtr path_pub_;\n    rclcpp::TimerBase::SharedPtr timer_;\n    // Data Storage\n    nav_msgs::msg::OccupancyGrid current_map_;\n    geometry_msgs::msg::PointStamped goal_;\n    geometry_msgs::msg::Pose robot_pose_;\n    bool goal_received_ = false;\n    void mapCallback(const nav_msgs::msg::OccupancyGrid::SharedPtr msg) {\n        current_map_ = *msg;\n        if (state_ == State::WAITING_FOR_ROBOT_TO_REACH_GOAL) {\n            planPath();\n        }\n    }\n    void goalCallback(const geometry_msgs::msg::PointStamped::SharedPtr msg) {\n        goal_ = *msg;\n        goal_received_ = true;\n        state_ = State::WAITING_FOR_ROBOT_TO_REACH_GOAL;\n        planPath();\n    }\n    void odomCallback(const nav_msgs::msg::Odometry::SharedPtr msg) {\n        robot_pose_ = msg->pose.pose;\n    }\n    void timerCallback() {\n        if (state_ == State::WAITING_FOR_ROBOT_TO_REACH_GOAL) {\n            if (goalReached()) {\n                RCLCPP_INFO(this->get_logger(), \"Goal reached!\");\n                state_ = State::WAITING_FOR_GOAL;\n            } else {\n                RCLCPP_INFO(this->get_logger(), \"Replanning due to timeout or progress...\");\n                planPath();\n            }\n        }\n    }\n    bool goalReached() {\n        double dx = goal_.point.x - robot_pose_.position.x;\n        double dy = goal_.point.y - robot_pose_.position.y;\n        return std::sqrt(dx * dx + dy * dy) < 0.5; // Threshold for reaching the goal\n    }\n    void planPath() {\n        if (!goal_received_ || current_map_.data.empty()) {\n            RCLCPP_WARN(this->get_logger(), \"Cannot plan path: Missing map or goal!\");\n            return;\n        }\n        // A* Implementation (pseudo-code)\n        nav_msgs::msg::Path path;\n        path.header.stamp = this->get_clock()->now();\n        path.header.frame_id = \"map\";\n        // Compute path using A* on current_map_\n        // Fill path.poses with the resulting waypoints.\n        path_pub_->publish(path);\n    }\n};","key-points-2#Key Points":"State Machine: Clear separation of \"waiting for goal\" and \"waiting for robot to reach goal.\"\nA* Planning: Finds an optimal path on the discretized occupancy grid using heuristic cost estimation.\nReplanning: Automatically triggered by timer-based checks for progress or map updates.","control-node#Control Node":"Should contain the following ROS constructs:\n1 Subscriber that subscribes to the '/path' topic for nav_msgs::msg::Path messages\n1 Subscriber that subscribes to the '/odom/filtered' topic for nav_msgs::msg::Odometry messages\n1 Publisher that publishes geometry_msgs::msg::Twist messages to a '/cmd_vel' topic\n1 Timer to follow a stored path\nThe control node is the final piece of the puzzle. It is the final action layer that is between your robotics software, and moving the robot (well, there's also the hardware abstraction layer that actually takes what this node outputs and converts it into actual electrical signals to move the motors of the robot).The field of controls is vast, and there are multiple different types of control schemes. As you go through university, you will probably learn about PID control. This form of control is used EVERYWHERE.\nAs of writing this assignment I was telling my dad about how I learned PID control, and how we can optimize signals in the Laplace domain, find parameters of a system with frequency analysis, blah, blah, blah. And he responded by telling me how he learned the same thing back in China, and uses it all the time to control temperatures and electrical signals in oil refineries.\nTurns out you can use PID controls for following the path you just made. You simply calculate how far your robot deviates from the path (and its discrete derivative and integral), and use that to adjust your robot back onto the path. HOWEVER, PID controls is quite finnicky, so lets teach you a different control scheme.Pure Pursuit Control is a geometric algorithm used to guide a robot along a predefined path. The robot selects a \"lookahead point\" on the path at a fixed distance ahead, and the algorithm calculates the steering angle needed to reach this point. This angle is based on the curvature of a circular arc connecting the robot‚Äôs current position to the target point.The lookahead distance controls how far ahead the target point is chosen. A larger distance makes the motion smoother but less precise, while a smaller distance improves accuracy but can lead to sharper, less smooth movements. As the robot moves, the target point and steering angle are updated continuously, allowing the robot to follow the path dynamically.You can read more about Pure Pursuit here.","node-behavior-and-design-1#Node Behavior and Design":"The Pure Pursuit Controller node is responsible for taking a path and odometry data as inputs and generating velocity commands to guide the robot along the path. The node operates in a closed-loop fashion: it continuously checks the robot's current position, calculates the desired heading based on the \"lookahead point,\" and generates appropriate linear and angular velocity commands.The node subscribes to:\n/path: Receives the global path as a series of waypoints.\n/odom/filtered: Tracks the robot's current position and orientation.\nThe node publishes:\n/cmd_vel: Outputs velocity commands in the form of linear and angular velocities.\nAdditionally, the node employs a timer to ensure regular updates of the velocity commands.","key-functionalities-1#Key Functionalities":"Path Tracking:\nExtract the current lookahead point from the path based on the robot‚Äôs position and a fixed lookahead distance.\nCompute the steering angle required to reach the lookahead point.\nVelocity Commands:\nCalculate linear and angular velocities using the steering angle and the robot‚Äôs dynamics.\nError Handling:\nHandle edge cases like when the path is empty or the robot is close to the final goal.\nTimer for Updates:\nThe node updates the velocity commands at a fixed rate (e.g., 10 Hz) for smooth and consistent control.","pseudo-ros-code-example#Pseudo ROS Code Example":"#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/path.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <cmath>\n#include <optional>\nclass PurePursuitController : public rclcpp::Node {\npublic:\n    PurePursuitController() : Node(\"pure_pursuit_controller\") {\n        // Initialize parameters\n        lookahead_distance_ = 1.0;  // Lookahead distance\n        goal_tolerance_ = 0.1;     // Distance to consider the goal reached\n        linear_speed_ = 0.5;       // Constant forward speed\n        // Subscribers and Publishers\n        path_sub_ = this->create_subscription<nav_msgs::msg::Path>(\n            \"/path\", 10, [this](const nav_msgs::msg::Path::SharedPtr msg) { current_path_ = msg; });\n        odom_sub_ = this->create_subscription<nav_msgs::msg::Odometry>(\n            \"/odom/filtered\", 10, [this](const nav_msgs::msg::Odometry::SharedPtr msg) { robot_odom_ = msg; });\n        cmd_vel_pub_ = this->create_publisher<geometry_msgs::msg::Twist>(\"/cmd_vel\", 10);\n        // Timer\n        control_timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(100), [this]() { controlLoop(); });\n    }\nprivate:\n    void controlLoop() {\n        // Skip control if no path or odometry data is available\n        if (!current_path_ || !robot_odom_) {\n            return;\n        }\n        // Find the lookahead point\n        auto lookahead_point = findLookaheadPoint();\n        if (!lookahead_point) {\n            return;  // No valid lookahead point found\n        }\n        // Compute velocity command\n        auto cmd_vel = computeVelocity(*lookahead_point);\n        // Publish the velocity command\n        cmd_vel_pub_->publish(cmd_vel);\n    }\n    std::optional<geometry_msgs::msg::PoseStamped> findLookaheadPoint() {\n        // TODO: Implement logic to find the lookahead point on the path\n        return std::nullopt;  // Replace with a valid point when implemented\n    }\n    geometry_msgs::msg::Twist computeVelocity(const geometry_msgs::msg::PoseStamped &target) {\n        // TODO: Implement logic to compute velocity commands\n        geometry_msgs::msg::Twist cmd_vel;\n        return cmd_vel;\n    }\n    double computeDistance(const geometry_msgs::msg::Point &a, const geometry_msgs::msg::Point &b) {\n        // TODO: Implement distance calculation between two points\n        return 0.0;\n    }\n    double extractYaw(const geometry_msgs::msg::Quaternion &quat) {\n        // TODO: Implement quaternion to yaw conversion\n        return 0.0;\n    }\n    // Subscribers and Publishers\n    rclcpp::Subscription<nav_msgs::msg::Path>::SharedPtr path_sub_;\n    rclcpp::Subscription<nav_msgs::msg::Odometry>::SharedPtr odom_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_pub_;\n    // Timer\n    rclcpp::TimerBase::SharedPtr control_timer_;\n    // Data\n    nav_msgs::msg::Path::SharedPtr current_path_;\n    nav_msgs::msg::Odometry::SharedPtr robot_odom_;\n    // Parameters\n    double lookahead_distance_;\n    double goal_tolerance_;\n    double linear_speed_;\n};\n// Main function\nint main(int argc, char **argv) {\n    rclcpp::init(argc, argv);\n    auto node = std::make_shared<PurePursuitController>();\n    rclcpp::spin(node);\n    rclcpp::shutdown();\n    return 0;\n}","key-details#Key Details":"Lookahead Distance: The controller selects a point on the path at a fixed distance ahead of the robot. This distance must be tuned for the robot's dynamics and desired path-following performance.\nLinear Speed: The robot‚Äôs speed is typically kept constant, while the angular velocity adjusts based on the curvature.\nEdge Cases: Ensure the node handles scenarios like an empty path or reaching the final goal. You can stop the robot when the goal is within the goal_tolerance.\nYaw Calculation: Converting the robot‚Äôs quaternion orientation to yaw is essential for computing the steering angle.\nThis implementation provides a simple yet effective way to integrate Pure Pursuit Control in ROS for smooth path following.","helpful-tips#Helpful Tips":"You never need to run colcon build because our monorepo infrastructure runs it automatically whenever you run ./watod build. A good amount of ROS packaging and Launchfiles has also been abstracted away from you. This was deliberate in order to make your experience with the assignment as \"code-heavy\" as possible.\nIf you want more into about how our infrastructure works, refer to the docs, or just read the watod scripting. It's not as complex as you think it is.\nWhat are launchfiles you may ask? Well basically ROS compiles your code into executable binaries inside a ./install directory. Launchfiles, and its python api, are used to navigate the binaries within the ./install directory. Note, you do not see any ./install directories in this assignment because we build your code inside docker containers.\nThe resolution of the costmap should be more than the map. This is to allow the map memory algorithm to match points from the costmap to the map without leaving holes.\nThroughout this assignment, you should really learn how to use Foxglove. It is a powerful tool to see what messages are being sent throughout the robot. Use 3D panel to see the messages in an intuitive way. Use Raw Message panels to see what the message actually looks like. Good for debugging.\nMake sure your map memory node publishes a map on initialization. Otherwise, your planner won't be able to plan unless if you move the robot manually for a bit.\nYour planner node might plan through objects it can't see. That's okay, as the robot moves, the map will update, and so will the planned path.","final-remarks#Final Remarks":"Congratulations! You have just achieved basic navigation in a static environment. You can now submit your assignment to the Discord, and use your newfound expertise to code REAL robots!!Now that you've built a simple navigation system, it's time to open your mind:\nHow do I connect my code to a real robot? There exists hardware (sensors, motors, actuators, etc.) that are built to convert ROS messages into serial messages and finally into hardware voltages. These are called drivers, and many hardware manufacturers create ROS drivers to sell their product easier. However, there exists stupid people that only give you a simple C++ API, or better yet a serial interface, which you then need to create your own ROS driver for. If you are in mechatronics, you'll get a knack for this stuff.\nHow can we change this software system to account for other sensors such as cameras, radars, and sonars?\nIf we weren't directly given the robot's position with respect to the map, then how would we localize the robot? In other words, how do we make the robot know where it is?\nWhat if we wanted the robot to do more complex decision making? What if its goal was more than just going from one point to another?\nHow can we change this algorithm to account for moving obstacles?\nWhat if the robot wasn't differential drive? What if it was two separate carts connected to each other at a pivot point?\nAll of these are probing questions that should get you excited about the limitless potential of robotics. You have just learned how to code a robot. Now it's time for you to learn to how code even more intelligent robots. Your future is bright :).Whether you used this assignment to get in WATonomous, or you used it for your personal growth, we wish you luck in your future career. :D"}},"/autonomous_platform_general":{"title":"Autonomous Platform General","data":{"":"Contains general autonomous platform documentation related to all projects."}},"/autonomous_platform_general/about_ap":{"title":"About Ap","data":{}},"/autonomous_software_general":{"title":"Autonomous Software General","data":{"":"Contains general autonomous software documentation related to all projects."}},"/autonomous_software_general/about_asd":{"title":"About Asd","data":{}},"/autonomous_software_general/monorepo_infrastructure":{"title":"Monorepo Infrastructure","data":{"":"As our team continues to encompass more fields of robotics around the school, the one thing holding us all together is our software infrastructure. The WATonomous Monorepo Infrastructure is a heavily optimized build and development system utilizing industry-standard software tools such as ROS2, Docker, Docker Compose, and Foxglove.We aim to further optimize our Monorepo Infrastructure as time progresses.\nMaintenance of the Monorepo Infrastructure is a joint responsibility by all Autonomy Directors of WATonomous","history#History":"","early-autodrive#Early Autodrive":"The WATonomous monorepo originated during the AutoDrive Challenge, where the team faced the critical decision of selecting a software framework to support the complex demands of autonomous vehicle development. After evaluating options, the team chose the Robotics Operating System (ROS) as its primary message-passing infrastructure due to its robust ecosystem, modularity, and tools for communication and debugging. ROS‚Äôs publish-subscribe model allowed different teams to independently develop components like perception, localization, and control while maintaining seamless integration.This decision established a scalable and flexible foundation for the team‚Äôs software architecture, enabling rapid iteration and collaboration. As the project evolved, the monorepo grew to include custom solutions tailored to the competition‚Äôs unique challenges, solidifying WATonomous's ability to tackle sophisticated autonomous driving tasks efficiently.","docker-and-the-introduction-of-watcloud#Docker and the Introduction of WATcloud":"As the project grew, team members faced challenges with dependency management and reproducibility. Frequent updates and compatibility issues disrupted workflows, and the introduction of a shared compute cluster (WATcloud) made managing environments even more complex. To address these problems, Docker and Docker Compose were introduced into the repository.Docker allowed the team to encapsulate the software environment, including dependencies and configurations, into portable containers. This ensured consistent execution across local machines and the shared cluster, eliminating environment mismatches and streamlining workflows. Docker's adoption improved collaboration, reproducibility, and scalability, becoming a key enabler for the monorepo‚Äôs continued development.","optimizing-development-qol#Optimizing Development QoL":"While the introduction of Docker was a game-changer for reproducibility and consistency, it also introduced new challenges. One major hurdle was the steep learning curve for new members, who not only had to grasp ROS but also learn containerization basics. This created a significant onboarding barrier and slowed development for newcomers.To address this, the team shifted focus toward abstracting Docker from the everyday developer‚Äôs workflow. This includes automating common tasks under shell scripts (watod) to reduce the need for deep Docker expertise. Other key improvements to the monorepo infrastructure included:\nFoxglove Integration: Foxglove replaced VNC for remote data visualization, providing a more efficient and user-friendly interface for analyzing real-time data streams.\nReverse Mounting: This feature exposed coding libraries outside the Docker container, enabling developers to use local tools for code completion and navigation without being confined to the container environment.\nWatod Bash Completion: This feature enables terminal command completion for watod commands.\nThese enhancements streamlined the development process, improved accessibility for new members, and reinforced the monorepo's adaptability and efficiency.","how-it-works#How It Works":"","a-visual-understanding#A Visual Understanding":"Below is a possible way to visualize how the Monorepo Infrastructure works:The entire system communicates amongst itself using ROS2 messages. All the cool algorithms for perception, planning, control, etc. are wrapped in their own ROS2 nodes to function concurrently. Various nodes share docker containers if they require the same libraries and tools to function (eg. two nodes may require the same version of PyTorch, so they share the same docker container).","on-startup#On Startup":"The visualization of the tech stack above represents our code in a running state. On startup, watod (docker-compose under the hood) is used to orchestrate the startup of each and every docker container, ROS2 node, and core algorithms. It carefully and automatically builds out the entire software architecture piece by piece until you end up with the visualization above.","communication#Communication":"You don't really need to know this, but if you want to know more you can see examples of ROS2 and Docker working together here.\nROS 2 nodes communicate within and across containers using the DDS middleware. Within a container, nodes leverage shared memory for intra-process communication, avoiding the overhead of serialization. For inter-process communication within the same container, DDS operates over the loopback network. Across containers, nodes use a shared Docker bridge network, enabling them to communicate using container names as hostnames. DDS handles node discovery via multicast, while message transfer occurs over unicast UDP or TCP. All containers are configured with the same ROS_DOMAIN_ID to ensure they participate in the same communication domain.For optimization, shared memory is utilized for large messages on the same host, and ROS 2 QoS profiles are configured to balance reliability and performance. Critical data, such as control commands, use reliable QoS, while less critical data, like video streams, use best-effort QoS. In scenarios requiring minimal latency, containers may use host networking to bypass Docker‚Äôs network stack. This architecture enables seamless, efficient communication across nodes, whether they are within the same container or spread across multiple containers.","how-to-use#How to Use":"","edit-your-watod-configsh#Edit your watod-config.sh":"This config file includes various commented out fields that you can set. With time, you will understand what all the field do, but all that matters is you set:\nACTIVE_MODULES=\"<the_modules_you_want_to_enable>\"\nA list of possible modules to enable is given in the config. You can copy the config file to a local config named watod-config.local.sh which will not be pushed onto git and overrides the default watod-config.sh.","build-the-code#Build the Code":"To build all the code in the modules you want to run, use:\n./watod build","run-the-code#Run the Code":"To run the modules, simply do:\n./watod up\nThis workflow should be exactly how any robot is started. Any bootstrapping code should be strictly prohibited.","how-to-actually-use#How to Actually Use":"Congratulations! You made the code run. However, this is just the tip of the iceberg. The Monorepo Infrastructure has a plethora of tools to help you develop in an efficient manner.Please go through the following to get an in-depth overview of all the tools available to you.","what-are-services#What are Services?":"Services are containers a module runs. A docker compose module can consist of 1 or more services. The number of services in a module is totally arbitrary and up to the system designer of your project.We generally use modules to differentiate core parts of the software stack.\nExample. We have a module for visualization tools, and a module for the robot code itself. This lets us run the robot code without necessarily running the visualization tools and vice versa.\nTo specifically build and up services of your choosing, you can run:\n./watod build <service_you_want_to_build>\n./watod up <service_you_want_to_up>\nIn fact, any docker compose command works with watod, so you can do some pretty cool things like:\n./watod run <service_you_want_to_run_a_command_in> ros2 topic list","developing-in-the-monorepo-infrastructure#Developing in the Monorepo Infrastructure":"There are two ways to develop:","developing-outside-a-container-Ô∏è#Developing outside a container üèïÔ∏è":"The Monorepo Infrastructure has the ability to automatically setup your VScode environment to enable code-completion and navigation. To do so, you just need to run the following:\n./watod --setup-dev-environment <service_you_want_to_dev_on>\nFollow the steps prompted in the terminal and you are good to go. Use this method most of the time.To build your code and see changes, do:\n./watod build\n./watod up\nIf you want to make changes to only one service while keeping the other services running:\n./watod up\n# in a separate terminal\n./watod build <service_you_want_to_build>\n./watod down <service_you_want_to_build>\n./watod up <service_you_want_to_build>\nTo run ROS2 commands outside the container, use:\n./watod run <service> <ros2_command>\nSome examples of ROS2 commands include:\n# To build and run code using roslaunch (this is if you don't want to use the roslaunch file specified in the docker-compose module file)\n./watod build <service>\n./watod run <service> ros2 launch <package_name> <launch_file>\n# To run a ros2bag\n./watod run <service_containing_bag_player> ros2 bag play <bag_file_path.mcap>\n# To debug topics\n./watod run <service> ros2 topic list|info|hz","developing-inside-a-container-#Developing inside a container üè°":"The Monorepo Infrastructure has the ability to automatically setup a Docker Devcontainer for you to develop in. To do so, do the following:Set your watod-config.sh to run in develop mode:\nMODE_OF_OPERATION=\"develop\"\nUp the service you want to dev in:\n./watod up <service>\nEnter the container with VScode and go to /home/bolty/ament_ws to begin developing.\nYou can run ros2 commands natively when inside the container. Likewise, you can also run colcon build commands and roslaunch.\nMake sure to source /opt/watonomous/setup.bash before running any ros2 commands. This points your terminal to the ROS binaries so that you can use them.\nSome examples of ROS2 commands include:\n# To build all the code in the src\ncolcon build\n# To build only specific packages \ncolcon build --packages-select <ros_packages_you_want_to_build>\n# ROS2 debug messages","docker-registries#Docker Registries":"To speed up the process of building images, the Monorepo Infrastructure utilizes docker registries to store prebuilt images for you to use."}},"/autonomous_software_general/watcloud_dev":{"title":"Developing with WATcloud","data":{"":"You must complete your Cluster Access Form before proceeding with this guide.\nHere, we discuss setting up WATcloud to be used for software development in the Autonomous Software Division.","why-watcloud-what-is-watcloud#Why WATcloud? What is WATcloud?":"Due to the high computational requirements of many aspects of the ASD stack, WATO has a large server infrastructure for remote development WATcloud. In this section, you will learn to connect to WATcloud on VS code. Connecting to a server to do remote development is not only a crucial aspect of software development at WATonomous, but is also a very common practice in the industry.\nFun Fact: WATcloud closely mimics server infrastructures used by OpenAI, NASA, Nvidia, and more!","a-look-from-afar#A Look from Afar":"","how-does-watcloud-share-compute-resources-fairly#How does WATcloud share compute resources fairly?":"WATcloud relies heavily on a resource management tool known as SLURM. SLURM ensures that all resources in WATcloud are shared in a fair and well-managed manner.For the everyday developer, you can imagine SLURM as a \"build your own computer\" tool. You specify to SLURM what compute resources you want (CPU, RAM, GPU, memory, time limit, etc.) and SLURM will build a compute node with the resources it has on hand.","so-how-does-remote-development-actually-work#So how does remote development actually work?":"Remote development for a WATonomous member typically consists of a local machine, host machine, SLURM node, and a docker container. They are defined below:\nLocal Machine Your personal computer.\nHost Machine The computer you connect to. In the case of WATcloud, this is the SLURM login node.\nSLURM Node Used to manage compute resources. It creates SLURM Jobs according to your needs.\nSLURM Job An \"imaginary computer\" that is created by WATcloud. You specify to WATcloud what compute you need by running commands in the SLURM login node.\nDocker Container An isolated coding environment.\nTo do remote development in the Autonomous Software Division, the process can be summed up by the image below:\nAs shown in the image, there are two ways to use a SLURM node.","job-scheduling-vs-interactive-development#Job Scheduling vs Interactive Development":"Use job scheduling when you want to run a command for a very long time (>1 day long). Use interactive development when you are actively making changes to your code and testing it.For most WATonomous members, you would use job scheduling for tasks like training neural networks, large data processing, numerical optimization, etc. On the other hand, you would use interactive development when you are coding/testing ROS2 nodes, interacting with / visualizing live data, making code changes in general, etc.","setting-up-watcloud-for-asd#Setting up WATcloud for ASD":"This section is experimental. Please let us know of any issues on our Discord\nDealing with SSH can be quite foreign to alot of new developers. Thankfully, we provide a series of helper scripts that will make setup for WATcloud easier on you.","general-setup#General Setup":"This section is required so that you have proper access to our server cluster.","local-machine-clone-the-wato_asd_tooling-repository#[Local Machine] Clone the wato_asd_tooling repository":"git clone git@github.com:WATonomous/wato_asd_tooling.git","local-machine-generate-an-ssh-config#[Local Machine] Generate an SSH config":"If you have never created an ~/.ssh/config file before, do that now. Note, we assume that all your SSH files are stored under ~/.ssh\ntouch ~/.ssh/config\nGenerate a WATcloud SSH config. Follow the prompts whenever you get them.\ncd wato_asd_tooling\nbash ssh_helpers/generate_ssh_config.sh\nYou should now be able to connect our cluster login nodes using these commands:\nssh wato-login1\nssh wato-login2","local-machine-setup-vscode-for-ssh#[Local Machine] Setup VScode for SSH":"To do this, download the Remote - SSH VScode Extension. After that, you should be able to attach VScode to any of the machines.","local-machine-setup-agent-forwarding#[Local Machine] Setup Agent Forwarding":"Agent forwarding lets us carry our identity onto other machines that we connect to. What this means is, you can use git commands on other machines without having to create an SSH key on each machine you connect to.Linux/Mac: Setup agent forwarding with our helper script. Follow the prompts whenever you get them.\ncd wato_asd_tooling\nbash ssh_helpers/configure_agent_forwarding.sh\nMac Users Beware: Whenever you restart/shutdown your mac, your ssh keys are removed from your agent, so you‚Äôll have to re-add them on startup.\nssh-add --apple-use-keychain $PATH_TO_PRIVATE_KEY\nWindows: Open Powershell (as adminstrator) and run the following command,\nSet-Service -Name ssh-agent -StartupType Automatic; Start-Service ssh-agent","host-machine-confirm-agent-forwarding-works#[Host Machine] Confirm Agent Forwarding Works":"You should now be able to use git on all the WATcloud machines you connect to. Confirm by running the following inside a WATcloud machine you connected to.\nssh -T git@github.com\nDeliverable Get SSH and SSH Agent Forwarding working.","setup-for-interactive-development#Setup for Interactive Development":"Unlike job scheduling, SLURM was not built to handle interactive development. Luckily we have a team of very talented individuals, and we managed to make interactive development work nonetheless :).Creating an interactive development environment entails starting an SSH server inside the SLURM node, some wacky SSH key sharing, a netcast proxycommand, as well as pointing docker to a persistent filesystem. You don't have to do that though. You just need to do the following.","one-time-setup#One-time Setup":"Follow these steps if you are setting up the SLURM dev sessions for the first time, or you were using past solutions for SLURM that WATO provided.","host-machine-add-your-public-key-into-the-slurm-nodes-authorized-keys#[Host Machine] Add your public key into the SLURM node's authorized keys":"Copy your public key on your local computer and paste it into the authorized_keys file on the SLURM node.\nyour_local_machine$ cat ~/.ssh/your_key.pub (copy the output of this)\nyour_local_machine$ ssh [wato-login1 or wato-login2]\nwatcloud_slurm_node$ touch ~/.ssh/authorized_keys\nwatcloud_slurm_node$ nano ~/.ssh/authorized_keys (paste you rpublic key into here)","host-machine-add-the-following-to-your-bashrc-in-your-slurm-node#[Host Machine] Add the following to your bashrc in your SLURM node":"watcloud_slurm_node$ nano ~/.bashrc\nAdd the following to the end of your ~/.bashrc file.\nif [[ \"$(hostname)\" == *\"slurm\"* ]]; then \n    export XDG_RUNTIME_DIR=${XDG_RUNTIME_DIR:-/tmp/run}\n    export XDG_CONFIG_HOME=${XDG_CONFIG_HOME:-/tmp/config}\n    export DOCKER_HOST=\"unix://${XDG_RUNTIME_DIR}/docker.sock\"\nfi","local-machine-build-the-computer-you-desire#[Local Machine] Build the Computer you desire!":"Use your favorite text editor to edit wato_asd_tooling/session_config.sh.\ncd wato_asd_tooling\nnano session_config.sh\nThe file itself contains descriptions of all of the parameters and how to set them.","local-machine-start-a-slurm-dev-session#[Local Machine] Start a SLURM Dev Session":"Run the helper script to startup a SLURM Dev Node. Follow all the prompts carefully.\ncd wato_asd_tooling\nbash start_interactive_session.sh\nDO NOT START MORE THAN ONE DEV NODE. You have a chance of corrupting your docker filesystem. Starting more than one dev node is like building multiple computers. It is NOT the same as creating multiple terminals.","local-machine-setup-ssh-for-slurm#[Local Machine] Setup SSH for SLURM":"Run this last helper script LOCALLY. Follow the prompts carefully.\ncd wato_asd_tooling\nbash ssh_helpers/setup_slurm_ssh.sh","local-machine-stop-the-slurm-dev-session#[Local Machine] Stop the SLURM Dev Session":"Stop the interactive session by hitting Ctrl+C.","starting-a-slurm-dev-session-regularly#Starting a SLURM Dev Session Regularly":"Once you have done the one-time setup, connecting to a SLURM Dev Session is easy.","local-machineoptional-build-the-computer-you-desire#[Local Machine][Optional] Build the Computer you desire!":"Use your favorite text editor to edit wato_asd_tooling/session_config.sh.\ncd wato_asd_tooling\nnano session_config.sh","local-machine-start-a-slurm-dev-session-1#[Local Machine] Start a SLURM Dev Session":"Run the helper script to startup a SLURM Dev Node. Follow all the prompts carefully.\ncd wato_asd_tooling\nbash start_interactive_session.sh","local-machine-connect-to-the-slurm-dev-session-with-vscode#[Local Machine] Connect to the SLURM Dev Session with VScode":"You can connect to the SLURM Dev Session using the VScode ssh extension. The remote host is called asd-dev-session by default.\nAnd you're good to go! Whenever you want to startup a SLURM Dev Node, start one up by running start_interactive_session.sh, and then SSH into the SLURM node through VScode.","setup-for-job-scheduling#Setup for Job Scheduling":"There is no setup. Creating an SLURM job is really easy. It was what SLURM was designed for. You can view docs on SLURM in the WATcloud documentation.\nDeliverable Run a SLURM batch job with 2 CPUs that counts to 60."}},"/finance":{"title":"Finance System","data":{"":"Instructions on how to use the WATonomous Finance System, which is used to track funding, our account balances, and manage and reimburse purhcases.Creating and Submitting a Personal Purchase Request Contains details for members on how to purchase items using their own money and get reimbursed for it. (go-to)Creating and Submitting a Purchase Request Contains details for members on how to purchase items using WATonomous cash or directly from one of our funds. (go-to)"}},"/finance/creating_personal_purchases":{"title":"Creating Personal Purchases","data":{"":"Go to https://finance-frontend.watonomous.ca/\nCreate New Ticket > Ticket Type: Personal Purchase\nEnter all purchase details. For funding item link, you would either be using FI-1 (WATO Cash) or one of the Funding Items that match the category. For example, if you're purchasing an RTX 4090 it would probably fit under GPU funding.\nAwait team approval. Please reach out to the finance team and the faculty advisor.\nOnce this has been completed, the status will transition to READY_FOR_BUY. You should purchase the item at this step.\nOnce the item has been purchased, please upload the University of Waterloo Finance Expense Claim Form. Instructions found below.\nClick Confirm Item(s) Purchased and Submit Reimbursement","creating-expense-claim-forms#Creating Expense Claim Forms":"Navigate here\nClick the link in step 1 titled \"reimbursement form (excel)\"\nEnter the following fields\nPayee is Student\nClaimant Name, Student Number, Department, Phone Number, E-mail Address\nMailing Address, City, Province/State, Postal Code. For these, use the address your items were shipped to, or your personal address.\nDestination/Reason for Request: Please put the corresponding reference number. For WEEF, this can be found here. Reach out to @smileycow on Discord if you do not know what this is\nIgnore the Travel Advance Request section, and fill in your item details on the next section. For the description, please put the name of your item, and if available, the reason it was purchased.\nHas an advance been issued: No\nEnter your signature of Claimant\nFollow the rest of the instructions on the original page (steps 2 to 4)\nUpload the receipt to the corresponding personal purchase at https://finance-frontend.watonomous.ca/"}},"/finance/creating_purchase_requests":{"title":"Creating Purchase Requests","data":{"":"Made with  using  and  by WATonomous and WATcloud.\nGo to https://finance-frontend.watonomous.ca/\nCreate New Ticket > Ticket Type: UW Finance Purchase\nEnter all purchase details. For funding item link, you would either be using FI-1 (WATO Cash) or one of the Funding Items that match the category. For example, if you're purchasing an RTX 4090 it would probably fit under GPU funding.\nAwait team approval. Please reach out to the finance team and the faculty advisor.\nOnce this has been completed, the item will be sent to the finance coordinator. They will purchase the item, in which case the status will be ORDERED\nOnce the item has arrived, the item will be transitioned to READY_FOR_PICKUP, and the pick up instructions will give you next steps\nOnce completed, the item should be in the PICKED_UP state, completing the flow."}},"/":{"title":"Welcome to the Wiki","data":{"":"Welcome to the WATonomous Wiki! This is a public wiki for all things WATonomous.","new-here#New Here?":"","how-to-edit#How to Edit":"Editing the WATonomous wiki is easy. Simply click Edit this page on the far right of each page. Note, you must be part of the WATonomous Organization to be able to edit.More how-to-edit here. You can use regular react components, or choose from a plethora of pre-built components and tools here."}},"/quest_books":{"title":"Quest Books","data":{}},"/quest_books/archive/s25_eve_quests":{"title":"Eve Quest Book - Spring 2025 (S25)","data":{"the-great-objective-level-5-robo-taxi-around-campus#The Great Objective: Level 5 Robo-taxi Around Campus":"WATonomous aims to achieve a fully autonomous Level 5 robo-taxi capable of navigating the University of Waterloo campus by the end of Spring 2025. This involves integrating hardware, software, and cognition systems to enable decision-making directly within the car. Achieving this milestone establishes the groundwork for advanced autonomous vehicle research and real-world applications.","term-objectives-summary#Term Objectives Summary":"The objectives for Spring 2025 focus on integrating hardware, developing software modules, and preparing the platform for autonomous operation.\nHardware Integration\nOSCC boards\nSensor drivers\nMounts\nElectrical Safety & Stability\nSoftware Modules\nPerception Stack\nLocal Plan and Controller\nHD Map and Global Plan\nBehavior Tree\nLocalization","term-objectives-and-scoring#Term Objectives and Scoring":"","hardware-integration#Hardware Integration":"OSCC Boards\nScore\tCriteria\t10/10\tJoystick command passes through ROS2 stack to OSCC ‚Üí verified motion with hard & soft e-stop systems functional.\t7/10\tOne or more OSCC control drivers implemented and verified with test motion (but not all subsystems functional).\t4/10\tBoards are wired and installed; no verified movement or joystick input.\t0/10\tNo integration, wiring incomplete or testing blocked.\t\nMinimum Requirements: Boards are wired and installed; no verified movement or joystick input 4/10.\nSensor Drivers\nScore\tCriteria\t10/10\tDrivers for all LiDARs, radar, GPS/IMU, and camera implemented, functional, and recorded in rosbags.\t7/10\tMajority of sensors interfaced and tested; partial or intermittent rosbag data.\t4/10\t3 cameras and 1 Lidar interfaced with static rosbag recording.\t0/10\tNo progress.\t\nMinimum Requirements: 3 cameras and 1 Lidar interfaced with static rosbag recording 4/10.\nMount Fabrication & Installation\nScore\tCriteria\t5/5\tAll mounts (LiDAR mirrors + radar) installed on vehicle and road-tested with weatherproofing in mind.\t3/5\tPartial fabrication/installation completed, or totally completed but not suitable for natural outdoor conditions.\t1/5\tBasic fabrication but no installation.\t0/5\tNo progress.\t\nMinimum Requirements: At least one mount fabricated for a score above 1/5.\nElectrical Safety & Stability\nScore\tCriteria\t5/5\tE-Stops, OSCC, compute rack, top rack and relay wiring fully implemented and enclosed and documented where necessary.\t3/5\tE-Stops wired and partial enclosure/organization complete.\t1/5\tWiring is completed but enclosures incomplete or unsafe.\t0/5\tNo e-stop implementation, car presents electrical hazards to a regular user.\t\nMinimum Requirements: Wiring is completed but enclosures incomplete or unsafe 4/10.","software-modules-all-must-be-on-the-main-branch#Software Modules (All must be on the main branch)":"Perception Stack\nScore\tCriteria\t20/20\tFully functional pipeline including anomaly detection running at around 30fps with all sensors, rigorously tested on the car under diverse scenarios.\t15/20\tPipeline with tracked 3D bounding boxes, lights and sign detection, using all sensors, tested on the car with minor issues.\t10/20\tPipeline with tracked 3D bounding boxes with almost all sensors, tested on the car with minor issues.\t5/20\tPartial pipeline with only 2D-3D bounding boxes from the 3 cameras and Lidar tested on the car.\t0/20\tNo progress.\t\nMinimum Requirements: Basic pipeline functionality (tracked 3D bounding boxes with signs and lights detection) running at least 20fps with at least 3 cameras and 1 Lidar tested on the car for a score above 5/20.\nLocal Plan and Controller\nScore\tCriteria\t20/20\tMPC (or MPPI) Fully functional, rigorously tested on the car, capable of turning.\t15/20\tIntial testing of MPC on car completed\t10/20\tFull working integration of MPC (or MPPI) and Local Planner with world modeling in simulation\t5/20\tIntegration with World Modeling started but not done.\t0/20\tNo progress.\t\nMinimum Requirements: Full working integration of MPC (or MPPI) and Local Planner with world modeling in simulation for a score of 10/20.\nHD Map\nScore\tCriteria\t10/10\tSuccessfully load custom maps (non-simulation) and initial testing with car\t7/10\tInsertion and handling of any Perception detection\t4/10\tInsertion of regulatory elements detected by Perception stack\t0/10\tNo progress.\t\nMinimum Requirements: Insertion of regulatory elements detected by Perception stack for a score of 4/10.\nBehavior Tree\nScore\tCriteria\t5/5\tExpanded behaviour tree with full integration with local planner\t3/5\tBasic integration with local planner\t1/5\tInitial tree design fully implemented and tested with HD Map\t0/5\tNo progress.\t\nMinimum Requirements: Implemented tree tested with HD Map for a score above 1/5.\nLocalization\nScore\tCriteria\t10/10\tFully tested on the car and functional with GPS/IMU and car odometry.\t7/10\tImplementation of localization module integrating GPS/IMU with car odometry, tested in simulation\t4/10\tTesting setup for car odometry in simulation\t0/10\tNo progress.\t\nMinimum Requirements: Localization must be functional in simulation for a score above 7/10.\nBlog Posts\nScore\tCriteria\t5/5\tBlog post every month\t3/5\tThree blog posts\t1/5\tSingle blog post\t0/5\tNo progress.\t\nMinimum Requirements: At least one blog post 1/5.","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tNotes\tOSCC Boards\t\t2025-07-6\t5/10\tVerifiable communication between pc -> board -> car -> board -> pc\tSensor Drivers\t\t2025-06-15\t7/10\tDrivers should be in monorepo, but wasn't set as a requirement. Camera frame drop could possibly be because of a bandwidth issue. Networking stuff: confirm that sesnors connect without internet, and if not, dnsmasq setup and documentation needed.\tMounts\t\t2025-06-15\t3/5\t\tElectrical Safety & Stability\t\t2025-08-10\t4/5\t\tPerception Stack\t\t2025-07-6\t7/20\ttasks should be split more for better understanding of how we did\tHD Map\t\t2025-08-10\t7/10\t\tBehavior Tree\t0/5\t2025-08-10\t\t\tLocalization\t7/10\t2025-07-6\t\t\tBlog posts\t\t2025-08-17\t0/5\tWe REALLY need to encourage this. Separates REAL contributors from the noise."}},"/quest_books/archive/w25_bizops_quests":{"title":"WATonomous Quest Book - Winter 2025 (W25)","data":{"term-objectives-summary#Term Objectives Summary":"The objectives for Winter 2025 focus on securing funding, enhancing marketing efforts, and progressing with hardware and software integration for the autonomous car. The following key areas are critical for WATonomous' continued success:\nFunding\nSecure funding from WEEF, MEF, Deans, Engsoc, and external sources.\nMarketing\nCreate social media content to showcase progress, and ensure the robot moves at an information stand.\nWebsite\nSponsorship Page\nBlog Page","term-objectives-and-scoring#Term Objectives and Scoring":"","funding#Funding":"WEEF Funding\nScore\tCriteria\t10/10\tFull funding secured with all necessary documents submitted and approval received.\t7/10\tPartial funding secured or submission in progress.\t4/10\tPreliminary discussions held, no official submission.\t0/10\tNo progress.\t\nMinimum Requirements: At least a submission for funding for a score above 4/10.\nMEF Funding\nScore\tCriteria\t10/10\tFull funding secured with all necessary documents submitted and approval received.\t7/10\tPartial funding secured or submission in progress.\t4/10\tPreliminary discussions held, no official submission.\t0/10\tNo progress.\t\nMinimum Requirements: At least a submission for funding for a score above 4/10.\nDeans‚Äô Funding\nScore\tCriteria\t10/10\tFull funding secured with all necessary documents submitted and approval received.\t7/10\tPartial funding secured or submission in progress.\t4/10\tPreliminary discussions held, no official submission.\t0/10\tNo progress.\t\nMinimum Requirements: At least a submission for funding for a score above 4/10.\nEngsoc Funding\nScore\tCriteria\t10/10\tFull funding secured with all necessary documents submitted and approval received.\t7/10\tPartial funding secured or submission in progress.\t4/10\tPreliminary discussions held, no official submission.\t0/10\tNo progress.\t\nMinimum Requirements: At least a submission for funding for a score above 4/10.\nExternal Funding\nScore\tCriteria\t10/10\tFull external funding secured, and discussions completed with partners.\t7/10\tPartial external funding secured or submission in progress.\t4/10\tInitial outreach made to potential external funders.\t0/10\tNo progress.\t\nMinimum Requirements: At least one external partner reached out for a score above 4/10.","marketing#Marketing":"Social Media Posts\nScore\tCriteria\t10/10\tMultiple posts made regularly, showcasing key milestones and progress with high engagement.\t7/10\tPosts made but with limited engagement or frequency.\t4/10\tFew posts made with little to no engagement.\t0/10\tNo posts made.\t\nMinimum Requirements: At least one post with reasonable engagement for a score above 4/10.\nInformation Stand with Moving Robot\nScore\tCriteria\t5/5\tFully operational stand with robot moving autonomously in front of a live audience.\t3/5\tPartial setup with the robot moving in a limited capacity.\t1/5\tStand set up but robot not moving or operational.\t0/5\tNo progress.\t\nMinimum Requirements: At least a partial setup for a score above 1/5.","website#Website":"Sponsorship Page\nScore\tCriteria\t5/5\tFully operational with sponsors and little edits needed\t3/5\tPartially operational with sponsors\t1/5\tOperational but no sponsors\t0/5\tNo progress.\t\nMinimum Requirements: At least a partial setup for a score above 1/5.\nBlog Page\nScore\tCriteria\t5/5\tFully operational with blog posts and little edits\t3/5\tFully operational with either blog posts or little edits\t1/5\tOperational but neither blog posts or many edits needed\t0/5\tNo progress.\t\nMinimum Requirements: At least a partial setup for a score above 1/5.","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tWEEF Funding\tFull funding secured.\t2025-04-15\t7/10\tMEF Funding\tFull funding secured.\t2025-04-15\t7/10\tDeans‚Äô Funding\tFull funding secured.\t2025-04-15\t0/10\tEngsoc Funding\tFull funding secured.\t2025-04-15\t0/10\tExternal Funding\tFull external funding secured.\t2025-04-15\t7/10\tSocial Media Posts\tRegular posts made with high engagement.\t2025-04-15\t4/10\tInformation Stand\tRobot moving at the stand.\t2025-04-15\t0/5\tSponsorship Page\tOperational Sponsorship Page\t2025-04-15\t5/5\tBlog Page\tOperational Blog Page\t2025-04-15\t1/5"}},"/quest_books/archive/w25_eve_quests":{"title":"WATonomous Quest Book - Winter 2025 (W25)","data":{"the-great-objective-level-5-robo-taxi-around-campus#The Great Objective: Level 5 Robo-taxi Around Campus":"WATonomous aims to achieve a fully autonomous Level 5 robo-taxi capable of navigating the University of Waterloo campus by the end of Winter 2025. This involves integrating hardware, software, and cognition systems to enable decision-making directly within the car. Achieving this milestone establishes the groundwork for advanced autonomous vehicle research and real-world applications.","term-objectives-summary#Term Objectives Summary":"The objectives for Winter 2025 focus on integrating hardware, developing software modules, and preparing the platform for autonomous operation. These include:\nHardware Integration\nOSCC boards: Fully integrated and functional with control drivers.\nSensor drivers: Implemented for all sensors (camera, GPS/IMU, Lidar, radar) and tested with rosbag recordings.\nMounts: Fabrication and installation of Lidar mirror mounts and radar mounts.\nSoftware Modules\nPerception Stack: Sensor integration and testing on the car.\nLocal Plan and Controller: Turning and obstacle avoidance capabilities tested on the car.\nHD Map and Global Plan: Load and test an HD map of the Waterloo regional test track.\nBehavior Tree: Implementation without requiring car testing.\nLocalization: Functional with GPS/IMU and car odometry, tested on the car.","term-objectives-and-scoring#Term Objectives and Scoring":"","hardware-integration#Hardware Integration":"OSCC Boards\nScore\tCriteria\t10/10\tFully integrated, tested, and functional on the car with all required control drivers in the monorepo.\t7/10\tPartially integrated with basic functionality; at least one control driver implemented and tested.\t4/10\tOSCC boards physically installed but no significant software functionality.\t0/10\tNo progress.\t\nMinimum Requirements: At least one control driver implemented and tested for a score above 4/10.\nSensor Drivers\nScore\tCriteria\t10/10\tDrivers for all sensors (camera, GPS/IMU, Lidar, radar) implemented in the monorepo, with full functionality verified and rosbag recordings completed.\t7/10\tDrivers for most sensors implemented and partially tested; incomplete recordings.\t4/10\tBasic drivers implemented for one or two sensors with minimal testing.\t0/10\tNo progress.\t\nMinimum Requirements: At least one sensor driver implemented and tested for a score above 4/10.\nMounts\nScore\tCriteria\t5/5\tAll mounts (Lidar mirror mounts and radar mounts) fabricated, installed, and tested.\t3/5\tSome mounts fabricated and installed, with partial testing.\t1/5\tBasic fabrication started but not installed.\t0/5\tNo progress.\t\nMinimum Requirements: At least one mount fabricated for a score above 1/5.","software-modules-all-must-be-on-the-main-branch#Software Modules (All must be on the main branch)":"Perception Stack\nScore\tCriteria\t20/20\tFully functional with all sensors, rigorously tested on the car under diverse scenarios.\t15/20\tFunctional with most sensors, tested on the car with minor issues.\t10/20\tBasic functionality with limited sensor integration, initial car tests completed.\t5/20\tImplementation started but no testing conducted.\t0/20\tNo progress.\t\nMinimum Requirements: Basic functionality with at least one sensor tested on the car for a score above 5/20.\nLocal Plan and Controller\nScore\tCriteria\t20/20\t(MPPI or MPC) Fully functional, rigorously tested on the car, capable of turning.\t15/20\t(PID or Pure Pursuit) Functional with some limitations, tested on the car with minor issues.\t10/20\tBasic functionality implemented with initial car tests conducted.\t5/20\tImplementation started but no testing conducted.\t0/20\tNo progress.\t\nMinimum Requirements: Basic functionality with initial testing for a score above 5/20.\nHD Map and Global Plan\nScore\tCriteria\t10/10\tSuccessfully loads HD map of Waterloo regional test track and is fully tested on the car.\t7/10\tLoads HD map but with minor issues; partially tested on the car.\t4/10\tBasic implementation with no significant car testing.\t0/10\tNo progress.\t\nMinimum Requirements: Must load an HD map and perform basic functionality for a score above 4/10.\nBehavior Tree\nScore\tCriteria\t5/5\tQueries written, reviewed, and merged into the main branch.\t3/5\tDraft written and partially reviewed.\t1/5\tInitial development started but incomplete.\t0/5\tNo progress.\t\nMinimum Requirements: At least a draft written for a score above 1/5.\nLocalization\nScore\tCriteria\t10/10\tFully tested on the car and functional with GPS/IMU and car odometry.\t7/10\tFunctional with either GPS/IMU or car odometry; partially tested.\t4/10\tBasic implementation started but no significant testing.\t0/10\tNo progress.\t\nMinimum Requirements: Must be functional with either GPS/IMU or car odometry for a score above 4/10.","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tOSCC Boards\tFully integrated and functional on the car.\t2025-02-15\t2/10\tSensor Drivers\tImplemented for all sensors with rosbag recordings.\t2025-03-8\t6/10\tMounts\tLidar mirror mounts and radar mounts completed.\t2025-03-8\t3/5\tPerception Stack\tSensor integration and car testing completed.\t2025-03-31\t9/20\tLocal Plan and Controller\tTurning and obstacle avoidance tested on the car.\t2025-04-12\t8/20\tHD Map and Global Plan\tLoads HD map of Waterloo test track, tested on car.\t2025-02-15\t4/10\tBehavior Tree\tFully implemented and merged into main branch.\t2025-03-15\t2/5\tLocalization\tFunctional with GPS/IMU and car odometry.\t2025-04-12\t4/10","comments#Comments":"This term was heavily influenced by our lack of access to AVRIL."}},"/quest_books/archive/w25_humanoid_quests":{"title":"W25 Humanoid Quests","data":{}},"/quest_books/archive/w25_rover_quests":{"title":"Rover Quest Book - Winter 2025 (W25)","data":{"the-great-objective-autonomous-navigation-for-the-uwrt-rover#The Great Objective: Autonomous Navigation for the UWRT Rover":"The WATO Rover Team is helping build the autonomous navigation system for the UWRT's URC 2025 rover.","term-objectives-summary#Term Objectives Summary":"The objectives for Winter 2025 focus on integrating UWRT's existing modules with the WATO Monorepo/watod infrastructure, as well as building out the autonomy software using nav2.\nPoint Cloud Generator:\nCreate a point cloud using the IntelRealSense D435 cameras and the RealSense SDK, and publish the point cloud message to Nav2 for Costmap Generation, and localization.\nObject Detection:\nUsing camera feeds to generate field relative poses for mission-critical objects.\nArUco Marker Detection:\nUsing camera feeds to generate field relative poses for ArUco markers.\nBehaviour Tree:\nUsing the Behaviour Tree to plan the navigation of the rover.\nNav2 Integration:\nUse sensor data from rover to plan and control the rover.\nUse the rover's odometry to plan and control the rover (costmap, localize, plan, and control).\nSimulation:\nGenerate rover URDF and TF tree.\nGazebo simulation for both the rover and desert environment.","term-objectives-and-scoring#Term Objectives and Scoring":"Point Cloud Generator\nScore\tCriteria\t10/10\tRover is able to reliably generate and publish a point cloud to Nav2.\t5/10\tRover is able to inreliably generate and publish a point cloud to Nav2.\t0/10\tWe were not able to meet the above goals\t\nMinimum Requirements:  Rover is able to inreliably generate and publish a point cloud to Nav2 for a score above 5/10.Score: 10/10\nObject Detection\nScore\tCriteria\t10/10\tRover is able to accurately detect and publish goal pose for Object.\t5/10\tRover is able to inaccurately detect and publish goal pose for Object.\t0/10\tNo progress.\t\nMinimum Requirements: Rover is able to inaccurately detect and publish goal pose for Object for a score above 5/10.Score: 5/10\nArUco Marker Detection\nScore\tCriteria\t10/10\tRover is able to accurately detect and publish goal pose for ArUco marker.\t5/10\tRover is able to inaccurately detect and publish goal pose for ArUco marker.\t0/10\tNo progress.\t\nMinimum Requirements: Rover is able to inaccurately/unreliably detect ArUco marker but not publish goal pose for a score above 5/10.Score: 10/10\nBehaviour Tree\nScore\tCriteria\t10/10\tRover is successfully able to navigate to any goal autonomously. (Object, ArUco, or GNSS Waypoint).\t7/10\tRover is able to navigate to one of the goals, but is not able to switch between them.\t4/10\tRover has a basic behaviour tree set up but is not able to navigate to any goal.\t0/10\tNo progress.\t\nMinimum Requirements:: Rover has a basic behaviour tree set up but is not able to navigate to any goal for a score above 4/10.Score: 4/10\nNav2 Integration\nScore\tCriteria\t10/10\tRover is achieve all mission goals. Costmap, localization, planning, and control all work together and fully implemented.\t7/10\tRover is able to navigate to all mission goals, but slowly/slightly illegally. Costmap, localization, planning, and control work together but not fully implemented.\t4/10\tRover is unable to navigate to all mission goals. One more more aspects of Nav2 not implemented.\t0/10\tNo progress.\t\nMinimum Requirements: Rover is unable to navigate to all mission goals. One more more aspects of Nav2 not implemented for a score above 4/10.Score: 4/10\nSimulation\nScore\tCriteria\t10/10\tAccurate Gazebo simulation for both the rover and a desert-like environment.\t7/10\tAccurate simulation for the rover but not the environment.\t4/10\tBasic simulation for an example rover with our sensors, but not accurate to our actual rover.\t0/10\tNo progress.\t\nMinimum Requirements: An accurate simulation of the rover for a score above 7/10.Score: 4/10","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tPoint Cloud Generator\tCreate a point cloud using the IntelRealSense D435 cameras, and publish that to Nav2 for Costmap Generation, and localization.\t2025-03-31\t10/10\tObject Detection\tUsing camera feeds to generate field relative poses for mission-critical objects.\t2025-03-31\t5/10\tArUco Marker Detection\tUsing camera feeds to generate field relative poses for ArUco markers.\t2025-03-31\t10/10\tBehaviour Tree\tUsing the Behaviour Tree to plan the navigation of the rover.\t2025-04-25\t4/10\tNav2 Integration\tUse sensor data from rover to plan and control the rover.\t2025-04-25\t4/10\tSimulation\tCreate URDF and TF tree for the rover, get Gazebo simulation running.\t2025-03-16\t4/10\t\nOverall Score: 5/10"}},"/quest_books/f25_eve_quests":{"title":"Eve Quest Book - Fall 2025 (F25)","data":{"the-great-objective-level-5-robo-taxi-around-campus#The Great Objective: Level 5 Robo-taxi Around Campus":"WATonomous aims to achieve a fully autonomous Level 5 robo-taxi capable of navigating the University of Waterloo campus by the end of Fall 2025. This involves integrating hardware, software, and cognition systems to enable decision-making directly within the car. Achieving this milestone establishes the groundwork for advanced autonomous vehicle research and real-world applications.","term-objectives-summary#Term Objectives Summary":"The objectives for Fall 2025 focus on integrating hardware, developing software modules, and preparing the platform for autonomous operation.\nHardware Integration\nThrottle Board\nSteering Board\nBrake Board\nGateway Board\nCamera Firmware\nLidar Firmware\nGPS/IMU Firmware\nJoystick\nROS2CCO\nSensor Networking\nLidar Mount\nCamera Mount\nGPS/IMU Mount\nOSCC Board Enclosures\nRelay Enclosures\nE-Stop\nSoftware Modules\nDeep ROS\nCamera Object Detection\n2D Tracking\n2D-3D Association\nLidar Costmap Migration\nHD Map\nBehavior Planner\nLocalization\nGlobal Planner\nLattice Planner (builds IDEAL splines from behaviour)\nLocal Planner (builds a collision-aware spline)\nController (MPPI, MPC, PID, Pure Pursuit)","term-objectives-and-scoring#Term Objectives and Scoring":"","hardware-integration#Hardware Integration":"Throttle Board\nScore\tCriteria\t10/10\tThrottle board fully integrated - can send desired commands and car executes them correctly.\t7/10\tThrottle board functional with minor issues or response delays.\t4/10\tThrottle board installed and wired but not fully operational.\t0/10\tNo throttle board integration.\t\nMinimum Requirements: Throttle board installed and wired for a score of 4/10.\nSteering Board\nScore\tCriteria\t10/10\tSteering board fully integrated - can send desired commands and car executes them correctly.\t7/10\tSteering board functional with minor issues.\t4/10\tSteering board installed and wired but not fully operational.\t0/10\tNo steering board integration.\t\nMinimum Requirements: Steering board installed and wired for a score of 4/10.\nBrake Board\nScore\tCriteria\t10/10\tBrake board fully integrated - can send desired commands and car executes them correctly.\t7/10\tBrake board functional with minor issues.\t4/10\tBrake board installed and wired but not fully operational.\t0/10\tNo brake board integration.\t\nMinimum Requirements: Brake board installed and wired for a score of 4/10.\nGateway Board\nScore\tCriteria\t10/10\tGateway board fully functional with all CAN communication working reliably.\t7/10\tGateway board operational with occasional communication issues.\t4/10\tGateway board installed and basic communication established.\t0/10\tNo gateway board integration.\t\nMinimum Requirements: Gateway board installed with basic communication for a score of 4/10.\nCamera Firmware\nScore\tCriteria\t10/10\tAll camera drivers in monorepo and functional - streaming synchronized data at expected framerates.\t7/10\tMost camera drivers in monorepo and functional with minor synchronization or quality issues.\t4/10\tAt least one camera driver in monorepo and streaming data reliably.\t0/10\tNo functional camera firmware.\t\nMinimum Requirements: At least one camera streaming data for a score of 4/10.\nLidar Firmware\nScore\tCriteria\t10/10\tAll lidar drivers in monorepo and functional - publishing synchronized point clouds at expected rates.\t7/10\tLidar drivers in monorepo and functional with minor configuration or timing issues.\t4/10\tAt least one lidar driver in monorepo and publishing point cloud data.\t0/10\tNo functional lidar firmware.\t\nMinimum Requirements: At least one lidar publishing data for a score of 4/10.\nGPS/IMU Firmware\nScore\tCriteria\t10/10\tGPS/IMU drivers in monorepo and fully functional with RTK corrections.\t7/10\tGPS/IMU drivers in monorepo and operational with standard GPS accuracy.\t4/10\tGPS/IMU drivers in monorepo with basic data being published.\t0/10\tNo functional GPS/IMU firmware.\t\nMinimum Requirements: Basic GPS/IMU data publishing for a score of 4/10.\nJoystick OSCC\nScore\tCriteria\t10/10\tJoystick interfaceable with OSCC boards directly without ROS.\t0/10\tNo joystick integration.\t\nMinimum Requirements: Optional, but a quick way to get Joystick working (theoretically).\nJoystick Node\nScore\tCriteria\t10/10\tJoystick node in monorepo and controlling basic vehicle functions reliably.\t4/10\tJoystick node in monorepo and publishing data to ROS2.\t0/10\tNo joystick integration.\t\nMinimum Requirements: Joystick publishing data to ROS2 for a score of 4/10.\nROS2CCO\nScore\tCriteria\t10/10\tROS2CCO fully functional with all vehicle interfaces and safety checks.\t7/10\tROS2CCO operational with most interfaces working.\t4/10\tBasic ROS2CCO communication established.\t0/10\tNo ROS2CCO implementation.\t\nMinimum Requirements: Basic ROS2CCO communication for a score of 4/10.\nSensor Networking\nScore\tCriteria\t10/10\tAll sensors networked with proper bandwidth management and minimal latency.\t7/10\tSensor network functional with occasional bandwidth issues.\t4/10\tBasic sensor networking established for critical sensors.\t0/10\tNo sensor networking implementation.\t\nMinimum Requirements: Basic networking for critical sensors for a score of 4/10.\nLidar Mount\nScore\tCriteria\t10/10\tBoth lidars mounted and connected.\t4/10\tOnly one side lidar mounted and connected.\t0/10\tNo lidar mounting solution.\t\nMinimum Requirements: Basic lidar mounting for a score of 4/10.\nCamera Mount\nScore\tCriteria\t10/10\tCamera mounts secure and functional.\t4/10\tBasic camera mounting solution in place.\t0/10\tNo camera mounting solution.\t\nMinimum Requirements: Basic camera mounting for a score of 4/10.\nGPS/IMU Mount\nScore\tCriteria\t10/10\tGPS/IMU mount secure and connected.\t0/10\tNot complete.\t\nMinimum Requirements: Basic GPS/IMU mounting for a score of 10/10.\nOSCC Board Enclosures\nScore\tCriteria\t10/10\tFunctional enclosures protecting all boards installed.\t4/10\tBasic enclosure solution for critical boards fabricated.\t0/10\tNo board protection.\t\nMinimum Requirements: Basic enclosures for critical boards for a score of 4/10.\nRelay Enclosures\nScore\tCriteria\t10/10\tFunctional relay enclosure installed.\t4/10\tBasic enclosure solution for relay fabricated.\t0/10\tNo relay protection.\t\nMinimum Requirements: Basic relay protection for a score of 4/10.\nE-Stop\nScore\tCriteria\t10/10\tPrimary e-stop system functional with proper testing.\t4/10\tBasic e-stop wired and partially functional.\t0/10\tNo e-stop implementation.\t\nMinimum Requirements: Basic e-stop functionality for a score of 4/10.","software-modules-all-must-be-on-the-main-branch#Software Modules (All must be on the main branch)":"Deep ROS\nScore\tCriteria\t10/10\tBasic Deep ROS setup with initial integration with camera object detection.\t0/10\tNo Deep ROS implementation.\t\nMinimum Requirements: Basic Deep ROS setup for a score of 10/10.\nCamera Object Detection\nScore\tCriteria\t10/10\tMulti-camera detection of all object types.\t7/10\tObject detection functional.\t0/10\tNo object detection.\t\nMinimum Requirements: Basic object detection on one camera for a score of 4/10.\n2D Tracking\nScore\tCriteria\t10/10\tRobust multi-object tracking with consistent IDs across all cameras.\t7/10\tTracking functional.\t4/10\tBasic tracking implementation.\t0/10\tNo tracking implementation.\t\nMinimum Requirements: Basic tracking implementation for a score of 4/10.\n2D-3D Association\nScore\tCriteria\t10/10\tAssociation functional with minimal accuracy issues.\t4/10\tBasic 2D-3D association working.\t0/10\tNo 2D-3D association.\t\nMinimum Requirements: Basic 2D-3D association for a score of 4/10.\nLidar Costmap Migration (Patchworks++)\nScore\tCriteria\t10/10\tFull costmap generation from lidar with dynamic obstacle handling.\t4/10\tBasic costmap generation from lidar data migrated to perception.\t0/10\tNo costmap implementation.\t\nMinimum Requirements: Basic costmap generation for a score of 4/10.\nHD Map\nScore\tCriteria\t10/10\tCustom HD maps loaded with full regulatory element support.\t7/10\tHD map functional with perception integration.\t4/10\tBasic HD map loading and visualization.\t0/10\tNo HD map implementation.\t\nMinimum Requirements: Basic HD map functionality for a score of 4/10.\nBehavior Planner\nScore\tCriteria\t10/10\tBehavior planner functional in monorepo for basic navigation.\t4/10\tInitial behavior planner implementation.\t0/10\tNo behavior planner.\t\nMinimum Requirements: Initial behavior planner for a score of 4/10.\nLocalization\nScore\tCriteria\t10/10\tMulti-sensor localization functional in monorepo.\t4/10\tBasic localization using GPS.\t0/10\tNo localization implementation.\t\nMinimum Requirements: Basic GPS localization for a score of 4/10.\nGlobal Planner\nScore\tCriteria\t10/10\tGlobal planner in monorepo and generating optimal routes with traffic consideration.\t4/10\tBasic global path planning.\t0/10\tNo global planner.\t\nMinimum Requirements: Basic global planning for a score of 4/10.\nLattice Planner\nScore\tCriteria\t10/10\tLattice Planner in monorepo handling multiple different behaviors.\t4/10\tBasic Lattice Planner implementation.\t0/10\tNo local planner.\t\nMinimum Requirements: Basic local planning for a score of 4/10.\nLocal Planner\nScore\tCriteria\t10/10\tLocal planner in monorepo handling dynamic obstacles smoothly.\t4/10\tBasic local planning implementation.\t0/10\tNo local planner.\t\nMinimum Requirements: Basic local planning for a score of 4/10.\nController\nScore\tCriteria\t10/10\tAdvanced controller in monorepo fully tuned and tested on vehicle.\t4/10\tBasic controller implementation.\t0/10\tNo controller implementation.\t\nMinimum Requirements: Basic controller for a score of 4/10.","scoring-template#Scoring Template":"","hardware-integration-1#Hardware Integration":"Quest Name\tDescription\tScore\tThrottle Board\tComplete throttle control integration\t\tSteering Board\tComplete steering control integration\t\tBrake Board\tComplete brake control integration\t\tGateway Board\tEstablish CAN communication gateway\t\tCamera Firmware\tDeploy and configure all camera drivers\t\tLidar Firmware\tDeploy and configure all lidar drivers\t\tGPS/IMU Firmware\tDeploy GPS/IMU with RTK capabilities\t\tJoystick OSCC\tJoystick commanding directly with OSCC\t\tJoystick Node\tJoystick command with ROS -> ROS2CCO\t\tROS2CCO\tComplete ROS2 vehicle interface\t\tSensor Networking\tEstablish robust sensor network infrastructure\t\tLidar Mount\tInstall professional lidar mounting system\t\tCamera Mount\tInstall weatherproof camera mounting system\t\tGPS/IMU Mount\tInstall GPS/IMU with proper ground plane\t\tOSCC Board Enclosures\tInstall weatherproof board enclosures\t\tRelay Enclosures\tInstall professional relay protection\t\tE-Stop\tImplement hard and soft emergency stop systems","software-modules#Software Modules":"Quest Name\tDescription\tScore\tDeep ROS\tIntegrate Deep ROS for perception optimization\t\tCamera Object Detection\tMulti-camera object detection at 30+ fps\t\t2D Tracking\tRobust multi-object tracking across cameras\t\t2D-3D Association\tAccurate camera-lidar data fusion\t\tLidar Costmap Migration\tDynamic costmap generation from lidar\t\tHD Map\tLoad custom HD maps with regulatory elements\t\tBehavior Planner\tComplex scenario handling and traffic rules\t\tLocalization\tMulti-sensor localization with cm accuracy\t\tGlobal Planner\tOptimal route planning with traffic consideration\t\tLattice Planner\tDynamic obstacle avoidance and smooth navigation\t\tLocal Planner\tDynamic obstacle avoidance and smooth navigation\t\tController\tMPC/MPPI controller for precise vehicle control"}},"/quest_books/questbook_template":{"title":"[Team] Quest Book - [Term] [Year] ([Term Code])","data":{"the-great-objective-main-goal#The Great Objective: [Main Goal]":"[Description of the main goal of this project. It should act a North Star to properly decide future targets and milestones.]","term-objectives-summary#Term Objectives Summary":"[Brief overview of what will be accomplished this term, organized into categories.]\n[Category 1]\n[Item 1]\n[Item 2]\n[Item 3]\n[Category 2]\n[Item 1]\n[Item 2]\n[Item 3]","term-objectives-and-scoring#Term Objectives and Scoring":"","category-1#[Category 1]":"[Quest Name 1]\nScore\tCriteria\t10/10\t[Full completion criteria - best possible outcome]\t7/10\t[Good completion with minor issues]\t4/10\t[Basic functionality achieved]\t0/10\t[No progress made]\t\nMinimum Requirements: [Description of minimum acceptable achievement for partial credit.]\n[Quest Name 2]\nScore\tCriteria\t10/10\t[Full completion criteria - best possible outcome]\t7/10\t[Good completion with minor issues]\t4/10\t[Basic functionality achieved]\t0/10\t[No progress made]\t\nMinimum Requirements: [Description of minimum acceptable achievement for partial credit.]","category-2#[Category 2]":"[Quest Name 1]\nScore\tCriteria\t10/10\t[Full completion criteria - best possible outcome]\t7/10\t[Good completion with minor issues]\t4/10\t[Basic functionality achieved]\t0/10\t[No progress made]\t\nMinimum Requirements: [Description of minimum acceptable achievement for partial credit.]","scoring-template#Scoring Template":"","category-1-1#[Category 1]":"Quest Name\tDescription\tScore\t[Quest Name 1]\t[Brief description of the quest objective]\t\t[Quest Name 2]\t[Brief description of the quest objective]\t\t[Quest Name 3]\t[Brief description of the quest objective]","category-2-1#[Category 2]":"Quest Name\tDescription\tScore\t[Quest Name 1]\t[Brief description of the quest objective]\t\t[Quest Name 2]\t[Brief description of the quest objective]\t\t[Quest Name 3]\t[Brief description of the quest objective]"}},"/quest_books/s25_humanoid_quests":{"title":"Humanoid Quest Book","data":{"":"These are the objectives for the Spring/Summer 2025 term. Blog posts / documention should be added for each component, giving a +2 to the section.","software#Software":"","simulation#Simulation":"Score\tDescription\t20/20\tFull system completed: Sensor Imitation, URDF Automation, Inverse Kinematics / PID Control, and initial Reinforcement Learning (RL) development.\t17/20\tAll components complete except initial RL development.\t15/20\tSensor Imitation, URDF Automation, and Inverse Kinematics / PID Control implemented; minor integration issues remain.\t10/20\tSensor Imitation and URDF Automation complete, but no control components working yet.\t5/20\tOne or two major components completed; others in development.\t3/20\tSystem partially started; most tasks still in development.\t0/20\tNo visible progress made.","behaviour#Behaviour":"Score /25\tCriteria\t+10\tMovement API for Physical system\t\t5/5 Movement API with teleop and VR control implemented\t\t4/5 Movement API with teleop control implemented\t\t1-3/5 Movement API being developed\t\t0/5 No progress\t+5\tSimulation API Integration\t\t5/5 Movement API is fully compatible with simulation\t\t3/5 Movement API currently being integrated with simulation\t\t0/5 No progress\t+5\tVoxal Grid\t\t5/5 Implemented, integrated with controller\t\t3/5 Implemented, controller ingratiation needed\t\t1-3/5 Development\t+5\tSystem localization / Initialization\t\t5/5 System implemented for getting initial state of the robot\t\t3/5 Manual system initialization\t\t0/5 No progress","perception#Perception":"Score\tCriteria\t+5\tObject detection - inital basic object detection with 3D object detections\t+5\tKeyboard specific objected detection and localization\t+5\tMaterial Classification - inital development and research\t+5\tPublishing filtered RGBD data","interfacing#Interfacing":"Score /20\tCriteria\t+4\tCAN Package management system developed\t+4\tDeployment System built to support system scaling\t+4\tSensor filtering / denoising implement\t+4\tSLAM inital research\t+4\tSensor Drivers - Sensor drivers integrated, exposing sensor data to system","embedded#Embedded":"Score\tCriteria\t+5\tSensor data collected and sent over CAN\t+5\tFinger position data consistently maintained when running\t+3\tEmbedded devices run tension control loop with current draw feed back\t+2\tFlashing STM32 firmware with ESP32 implemented\t+3\tMove fingers to set positions via PID controls\t+2\tEmbedded devices process CAN messages\t0/20\tNo progress","hand#Hand":"","traincar#Traincar:":"Criteria /20\tRequirements\t+2\tMechanical (Finger Actuation, Forearm integration/mounting)\t+3\tMotor Control (Current Sense, Motor Movement, Magnetic Encoder feedback)\t+2\tCommunication (I2C, SPI, WIFI, CAN-FD)\t13/20\tMechanical enclosure for PCB and motors designed & printed\t10/20\tCustom PCB created, ordered, shipped & delivered\t0/20\tNo Progress","mechanical#Mechanical:":"Criteria /20\tRequirements\t20/20\tHand is fully integrated into software: URDF Created & CAD lines up w/ real life model\t+4\tHand is fully integrated with Forearm:\t\t4/4: Mechanically mounted & wires routed (no pinching!)\t\t3/4: Mechanically mounted\t+4\tPalm meet all requirements:\t\t4/4: Fingers & Thumb are tested & integrated into palm\t\t2/4: Palm prototype is manufactured\t\t1/4: Palm prototype is designed\t\t0/4: No Progress\t+4\tFingers meet all requirements:\t\t4/4: Finger is integrated into palm\t\t3/4: Finger is tested\t\t2/4: Finger prototype is manufactured\t\t1/4: Finger prototype is designed\t\t0/4: No Progress\t+4\tThumb meet all requirements:\t\t4/4: Thumb is integrated into palm\t\t3/4: Thumb is tested\t\t2/4: Thumb prototype is manufactured\t\t1/4: Thumb prototype is designed\t\t0/4: No Progress","sensors#Sensors:":"Criteria /10\tRequirements\t+4\tSensors integrated into mechanical design\t+4\tSensors tested & integrated (able to communicate, test rig + procedure setup and used, integrated into traincar)\t2/10\tSensors purchased\t1/10\tSensor categories identified & researched\t0/10\tNo Progress","forearm#Forearm":"Criteria /40\tRequirements\t40/40\tForearm ready for Demo (finger & wrist actuation)\t35/40\tForearm integrated & assembled with Hand\t30/40\tForearm manufactured & assembled with Traincar\t+2\tWrist motors purchased\t+8\tForearm CAD + URDF Complete\t+8\tWrist CAD + URDF Complete\t2/50\tDesign requirements & constraints specified (rated load, material/manufacturing method, DoF)\t0/50\tNo Progress","arm#Arm":"Criteria /15\tRequirements\t+10\tMechanical meets requirements:\t\t10/10 Mechanical Design finished with parts ordered\t\t8/10 Mechanical Design finished with final integrations left\t\t0-5/10 Mechanical Design in progress\t+5\tMotors:\t\t5/5 Motors integrated with design\t\t3/5 Motors sourced and purchase\t\t2/5 Required motor specifications calculated\t0/15\tNo progress","body#Body":"Building out the mount for the arms and the cameras","power-systems#Power Systems":"Criteria /10\tRequirements\t10/10\tPower system designed and built with interfaces with system components\t7/10\tSystem design complete, parts purchased and installed\t5/10\tSystem design complete and part selection complete\t2/10\tSystem design complete\t0/10\tNo progress","mechanical-1#Mechanical":"Criteria /10\tRequirements\t10/10\tBody built with aesthetic design, integrated into system\t7/10\tBody built with mounts for system\t5/10\tBody design complete\t2/10\tBody design in progress\t0/10\tNo progress"}},"/quest_books/s25_micro_quests":{"title":"Micro Autonomy Quest Book - Spring 2025","data":{"the-great-objective-fully-autonomous-f1tenth-racing-car-with-lidar-and-camera#The Great Objective: Fully Autonomous F1tenth Racing Car with LiDAR and Camera":"Micro Autonomy aims to win the F1teneth Autonomous Racing Competition in the future","term-objectives-summary#Term Objectives Summary":"The objectives for this term focuses on integrating the hardware and software stacks. This includes:\nHardware Setup\nAssemble & test everything including Jetson, LiDAR and VESC to work properly and publish ros messages needed for autonomous navigation\nState Estimation\nEstimates current position of vehicle against known world map\nMapping: SLAM\nGenerate a map in image format and a yaml file for the map specifications based on IMU, encoder and LiDAR specifications on a real world setup\nPlanning: Lattice Planner\nStatic obstacle avoidance support\nControls: Pure Pursuit\nStatic obstacle avoidance support\nHardware Setup\nScore\tCriteria\t10/10\tSetup interfaces for sensors to publish readings and actuators to take ROS topic commands to support autonomous control\t7/10\tTest electronic & tune motor controllers\t5/10\tAssemble all components for F1Tenth Car to start manually controlled movement\t0/10\tNo Progress\t\nMinimum Requirements: Autonomous control ready (10/10)\nState Estimation\nScore\tCriteria\t5/5\tTuned state estimation position based on IMU, encoder and LiDAR in actual vehicle\t3/5\tUn-tuned state estimation position based on IMU, encoder and LiDAR in actual vehicle\t0/5\tNo Progress\t\nMinimum Requirements: Untuned vehicle estimation (3/5)\nMapping: SLAM\nScore\tCriteria\t10/10\tHigh quality map and yaml file generated through state estimation and LiDAR scans on actual vehicle for complex routes\t7/10\tMap and yaml file generated through state estimation and LiDAR scans on actual vehicle for simple routes\t5/10\tMap and yaml file generated through state estimation and LiDAR scanes in simulation\t0/10\tNo Progress\t\nMinimum Requirements: Map and yaml file generated through state estimation and LiDAR scans on actual vehicle for simple routes (7/10)\nRaceline Generation/Optimization\nScore\tCriteria\t10/10\tOptimizes centerline to generate a raceline that minimizes actual lap time based on complex vehicle dynamics\t0/10\tNo Progress\t\nMinimum Requirements: (TBD) No minimum requirements\nPlanning: Lattice Planner\nScore\tCriteria\t10/10\tDynamic obstacle avoidance\t8/10\tGenenerates/optimizes local trajectory based on cost map and vehicle dynamics to achieve static obstalce avoidance on actual vehicle\t5/10\tGenerates & optimize a local trajectory for obstacle avoidance static obstacles are on the ideal path\t3/10\tGenerates local trajectory by using a cost map to achieve obstacle avoidance when static obstacles are on the ideal path\t0/10\tNo Progress\t\nMinimum Requirements: Real vehicle static obstacle avoidance (8/10)\nControls: Pure Pursuit\nScore\tCriteria\t10/10\tTune pure pursuit controller for dynamic obstacle avoidance on actual vehicle\t8/10\tTune pure pursuit controller for static obstacle avoidance on actual vehicle\t5/10\tFollows local trajectories and ideal velocities smoothly on actual vehicle\t0/10\tNo Progress\t\nMinimum Requirements: Real vehicle static obstacle avoidance (8/10)\nIntegration\nScore\tCriteria\t10/10\tFull software integration & hardware interfaces for real life autonomous racing\t8/10\tFull software integration in simulation for autonomous driving, limited hardware interfaces\t0/10\tNo progress\t\nMinimum Requirements: Full integration (10/10)","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tHardware Setup\t\t2025-08-31\t\tState Estimation\t\t2025-08-31\t\tMapping: SLAM\t\t2025-08-31\t\tRaceline Generation/Optimization\t\t2025-08-31\t\tPlanning: Lattice Planner\t\t2025-04-31\t\tControls: Pure Pursuit\t\t2025-08-31\t\tIntegration\tFull Integration\t2025-08-31"}},"/quest_books/s25_rover_quests":{"title":"Rover Quest Book - Spring 2025 (S25)","data":{"the-great-objective-simulate-navigation-for-a-rover#The Great Objective: Simulate navigation for a rover.":"The WATO Rover Team is helping build the autonomous navigation subsystem for the UW Robotics Team's URC 2026 rover. We hope to pass the System Acceptance Review and do well in next year's competition.","term-objectives-summary#Term Objectives Summary":"The objectives for Spring 2025 focus on building upon Winter 2025 progress and completing essential autonomous navigation functionality for the URC.\nSimulate collecting depth camera data from three different cameras.\nBuild upon the camera simulation created last term that publishes simulated point cloud data, which will allow all members to test algorithms that involve the cameras, regardless of hardware access.\nCostmap\nConvert depth camera data into a costmap.\nRover localization\nComplete localization using GPS/IMU and odometry and test on simulated data and on the rover.\nCompute and publish goal pose of detected object.\nObject detection of mission-critical objects for the competition (e.g. rubber mallets, water bottles) was achieved last term and tested with webcam. We need to integrate with our RealSense cameras and transform detected objects to map/odom frame so we can publish a navigation goal.\nNavigate autonomously to chosen waypoint.\nComplete the planning and control modules so the rover can autonomously navigate to any waypoint. This is required for navigating to the position of a detected object, as well as navigating to a provided GPS waypoint (another required task for the competition).\nWrite three blogs on simulating the rover\nWe are a relatively new team and hope to improve our outreach and provide documentation for future members by writing blog posts throughout the term showcasing our progress.","term-objectives-and-scoring#Term Objectives and Scoring":"Simulate depth camera data\nScore\tCriteria\t10/10\tAll three depth cameras are generating and publishing data to Nav2.\t8/10\tAll three depth cameras are generating, but not publishing data to Nav2\t5/10\tOne or two depth cameras are generating and publishing data to Nav2\t3/10\tOne or two depth cameras are generating but not publishing data to Nav2\t0/10\tNo depth cameras are generating data to Nav2\t\t\nMinimum Requirements:  One or two depth cameras are generating but not publishing data to Nav2 for a score of 3/10.\nGenerate costmap from depth camera data\nScore\tCriteria\t10/10\tDepth cameras are able to simulate an accurate costmap, complete with obstacle and inflation layers.\t5/10\tDepth cameras are able to simulate an inaccurate costmap.\t0/10\tDepth cameras are unable to simulate a costmap and/or depth cameras are not completed\t\nUpstream Dependency: Depth Camera DataMinimum Requirements: Depth cameras are able to simulate an innacurate costmap for a score of 5/10.\nRover localization\nScore\tCriteria\t10/10\tAccurate localization based on GPS, IMU, and odometry is achieved and fully tested in both simulation and on rover.\t7/10\tAccurate localization is achieved and fully tested in simulation.\t5/10\tInaccurate localization and/or partial testing in simulation.\t0/10\tNo progress\t\nMinimum Requirements: Accurate localization is achieved and tested using simulated data for a score of 7/10.\nCompute and publish goal pose of detected object\nScore\tCriteria\t10/10\tRover is able to accurately compute the pose of detected object (e.g. mallet or water bottle) and publish goal to the correct topic.\t7/10\tRover is able to compute and publish an approximate pose.\t0/10\tRover is unable to do any pose computation for the detected objects.\t\nMinimum Requirements: Rover is able to compute and publish an approximate pose for a score above 7/10.\nAutonomously navigate to chosen point\nScore\tCriteria\t10/10\tRover is able to accurately and efficiently navigate to a point.\t7/10\tRover is able to inefficiently navigate to a point.\t4/10\tRover is unreliably able to navigate to a point. (Less than 50% success rate out of 20 tests)\t0/10\tRover is unable to navigate to a point\t\nMinimum Requirements: Rover is able to inefficiently navigate to a point for a score above 7/10.\nBlogs\nScore\tCriteria\t20/20\tMore than three blogs have been written on simulating software for the rover.\t15/20\tThree blogs have been written on simulating software for the rover.\t10/20\tTwo blogs have been written on simulating software for the rover.\t5/20\tOne blog has been written on simulating software for the rover.\t0/20\tNo blogs have been written on simulating software for the rover.\t\nMinimum Requirements:: Three blogs have been written on simulating software for the rover for a score above 15/20.","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tDepth Camera Simulation\tSimulate collecting depth camera data from three different cameras and publish to Nav2.\tMay 30\t\tCostmap Generation\tConvert depth camera data into a costmap for navigation using Nav2.\tJune 13\t\tLocalization\tComplete localization using GPS/IMU and odometry and testing on the rover.\tJune 13\t\tGoal Pose Generation\tCompute and publish goal poses of detected objects we need to navigate to.\tJune 20\t\tAutonomous Navigation (Planning and Control)\tNavigate the rover autonomously and efficiently to a chosen point using the generated costmap.\tJuly 11\t\tBlog Posts\tWrite at least three blogs on simulating the rover in autonomy software and lessons learned.\tAugust 11"}},"/quest_books/s25_watcloud_quests":{"title":"WATcloud Quest Book - Summer 2025","data":{"term-objectives-summary#Term Objectives Summary":"These are WATcloud's objectives for the Summer 2025 term.\nHardware additions: nebula machines and wato-drive3\nHandle outages, optimizing for zero downtime\nTransition to login node architecture\nWrite blog posts!\nGet more funding!","term-objectives-and-scoring#Term Objectives and Scoring":"Hardware additions: nebula machines and wato-drive3\nScore\tCriteria\t20/20\tAll 3 machines are in the cluster along with a new wato-drive\t10/20\tAll 3 machines is in the cluster\t0/10\tNo progress.\t\nHandle outages, optimizing for zero downtime\nScore\tCriteria\t20/20\tAll outages are handled with zero downtime for critical services (try UPS for critical machines)\t10/20\tAll outages are handled with < 12 hours downtime for critical services\t5/20\tAll outages are announced before hand\t\nTransition to login node architecture\nScore\tCriteria\t10/10\tCompletely moved away from VM architecture and all resources are Slurm\t5/10\tLogin nodes work but VMs are still available for users\t0/10\tNo progress.\t\nWrite blog posts!\nScore\tCriteria\t20/20\t2 blog posts are written and published\t10/20\t1 blog post is written and published\t0/20\tNo progress.\t\nGet more funding!\nScore\tCriteria\t10/10\t$5k +\t5/10\t$3k +\t0/10\tNo progress."}},"/quest_books/archive/w25_micro_quests":{"title":"Micro Autonomy Quest Book - Winter 2025","data":{"the-great-objective-fully-autonomous-f1tenth-racing-car-with-lidar-and-camera#The Great Objective: Fully Autonomous F1tenth Racing Car with LiDAR and Camera":"Micro Autonomy aims to win the F1teneth Autonomous Racing Competition in the future","term-objectives-summary#Term Objectives Summary":"The objectives for this term focuses on setting up hardware and developing the autonomy software stack. This includes:\nHardware Setup\nSetup everything including Jetson, LiDAR and VESC to work properly and publish ros messages needed for autonomous navigation\nState Estimation\nEstimates current position of vehicle relative to last position using Kalman Filter\nMapping: SLAM\nGenerate a map in image format and a yaml file for the map specifications based on IMU, encoder and LiDAR specifications\nRaceline Generation/Optimization\nGenerate a optimized raceline as global trajectory (ideal path) for the race car to follow to minimize lap time\nPlanning: Lattice Planner\nGenerate local trajectories for obstacle avoidance and following global trajectory during Racing\nControls: Pure Pursuit\nControls the vehicle's steering and trottle to following the trajectories genereated by the planning module\nHardware Setup\nScore\tCriteria\t10/10\tSetup interfaces for sensors to publish readings and actuators to take ROS topic commands\t7/10\tAssemble all components for F1Tenth Car to start autonomous movement\t5/10\tAssemble all (within current budget) components\t3/10\tPurchase & manufacture all (within current budget) components\t0/10\tNo Progress\t\nMinimum Requirements: Assemble all available components (7/10)\nState Estimation\nScore\tCriteria\t5/5\tEstimates position based on IMU, encoder and LiDAR in actual vehicle\t3/5\tEstimates position based on IMU, encoder and LiDAR in simulation\t0/5\tNo Progress\t\n**Minimum Requirements:*Estimates position in simulation (3/5)\nMapping: SLAM\nScore\tCriteria\t10/10\tMap and yaml file generated through state estimation and LiDAR scans on actual vehicle\t8/10\tMap and yaml file generated through state estimation and LiDAR scanes in simulation\t5/10\tMap and yaml file generated through true position (odometry) and LiDAR scans in simulation\t0/10\tNo Progress\t\n**Minimum Requirements:*Can generate a map and yaml file in simulation (5/10)\nRaceline Generation/Optimization\nScore\tCriteria\t10/10\tOptimizes centerline to generate a raceline that minimizes actual lap time based on complex vehicle dynamics\t7/10\tOptimizes centerline to generate a raceline that minimizes steering or total curvature and generates velocity profile\t3/10\tGenerates a centerline for testing purposes for the vehicle to follow\t0/10\tNo Progress\t\n**Minimum Requirements:**Optimizes centerline to generate a raceline that minimizes steering or total curvature (7/10)\nPlanning: Lattice Planner\nScore\tCriteria\t10/10\tGenenerates/optimizes local trajectory based on cost map and vehicle dynamics to achieve obstalce avoidance on actual vehicle\t9/10\tGenenerates/optimizes local trajectory based on cost map and vehicle dynamics to achieve obstalce avoidance\t8/10\tGenerates local trajectory by using a cost map to achieve obstacle avoidance when obstalces are on the ideal path\t5/10\tGenerates local trajectory to make vehicle follow the raceline (ideal path)\t0/10\tNo Progress\t\n**Minimum Requirements:**Generates local trajectory by using a cost map to achieve obstacle avoidance when obstalces are on the ideal path (7/10)\nControls: Pure Pursuit\nScore\tCriteria\t10/10\tFollows local trajectories and ideal velocities smoothly on actual vehicle\t8/10\tFollows local trajectory generated by planning through controlling steering angle and throttle based on velocity profile\t4/10\tFollows a global trajectory by controlling steerring angle with a constant velocity\t0/10\tNo Progress\t\n**Minimum Requirements:**Follows local trajectory generated by planning through controlling steering angle and throttle based on velocity profile (9/10)\nIntegration\nScore\tCriteria\t10/10\tFull software integration & hardware interfaces for real life autonomous racing\t8/10\tFull software integration in simulation for autonomous driving, limited hardware interfaces\t4/10\tLimited software integration between components\t0/10\tNo progress\t\nMinimum Requirements: Full software integration in simulation for autonomous driving (8/10)","scoring-template#Scoring Template":"Quest Name\tDescription\tDue Date\tScore\tHardware Setup\tSetup everything including Jetson, LiDAR and VESC to work properly and publish ros messages needed for autonomous navigation\t2025-04-31\t3\tState Estimation\tEstimates current position of vehicle relative to last position using Kalman Filter.\t2025-04-31\t3\tMapping: SLAM\tGenerate a map in image format and a yaml file for the map specifications based on IMU, encoder and LiDAR specifications.\t2025-04-31\t5\tRaceline Generation/Optimization\tGenerate a optimized raceline as global trajectory (ideal path) for the race car to follow to minimize lap time.\t2025-04-31\t7\tPlanning: Lattice Planner\tGenerate local trajectories for obstacle avoidance and following global trajectory during Racing.\t2025-04-31\t5\tControls: Pure Pursuit\tControls the vehicle's steering and trottle to following the trajectories genereated by the planning module.\t2025-04-31\t8\tIntegration\tFull software integration & hardware interfaces for real life autonomous racing\t2025-04-31\t4"}}}