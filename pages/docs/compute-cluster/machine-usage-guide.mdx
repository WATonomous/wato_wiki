# Machine Usage Guide

This document provides an overview of the machines in the WATcloud compute cluster, including their hardware, networking, operating system, services, and software.
It also includes guidelines for using the machines, troubleshoot instructions for common issues, and information about maintenance and outages.

## Types of Machines

There are two main types of machines in the cluster: [general-use machines](/machines#general-use-machines) and [SLURM compute nodes](/machines#slurm-compute-nodes).

General-use machines are meant for interactive use and are shared among all users in the cluster.
You can access them via [SSH](./ssh).

SLURM compute nodes are meant for running resource-intensive jobs.
They are managed by a popular HPC[^hpc] job scheduler called SLURM[^slurm].
Instructions for submitting jobs to the SLURM cluster can be found [here](./slurm).

[^hpc]: [High-performance computing](https://en.wikipedia.org/wiki/High-performance_computing) (HPC) is the use of supercomputers and parallel processing
  techniques to solve complex computational problems. Examples of HPC clusters include [Cedar](https://docs.computecanada.ca/wiki/Cedar) and
  [Graham](https://docs.computecanada.ca/wiki/Graham).

[^slurm]: Simple Linux Utility for Resource Management (SLURM) is an open-source job scheduler that allocates resources to jobs on a cluster of computers.
  It is widely used in HPC environments. Learn more about SLURM [here](https://slurm.schedmd.com/quickstart.html).

## Hardware

Most machines in the cluster come with standard workstation hardware that include CPU, RAM, GPU, and storage[^machine-specs]. In special
cases, you can request to have specialized hardware such as FPGAs installed in the machines.

[^machine-specs]: The specs of the machines can be found [here](/machines).

## Networking

All machines in the cluster are connected to both the university network (over 10Gbps or 1Gbps Ethernet)
and a cluster network (over 40Gbps or 10Gbps Ethernet). The IP address range for the university network is
`129.97.0.0/16`[^uwaterloo-ip-range] and the IP address range for the cluster network is `10.0.50.0/24`.

[^uwaterloo-ip-range]: The IP range for the university network can be found [here](https://uwaterloo.ca/information-systems-technology/about/organizational-structure/technology-integrated-services-tis/network-services-resources/ip-requests-and-registrations).

## Operating System

All general-use machines and SLURM compute nodes are virtual machines (VMs)[^hypervisor].
This setup allows us to easily manage the machines remotely and reduce the complexity of the bare-metal OSes.

[^hypervisor]: We use [Proxmox](https://www.proxmox.com/en/) as our hypervisor.

## Services

### `/home` Directory

We run an SSD-backed Ceph[^ceph] cluster to provide distributed storage for the machines in the cluster. All general-use machines
share a common `/home` directory that is backed by the Ceph cluster. This means that you can access your files (think
bashrc files, project files, miniconda environments, etc.) from any general-use machine in the cluster.

Due to the relatively expensive cost of SSDs, the Ceph cluster is only meant for storing small files. If you need to store large
files (e.g. datasets, videos, ML model checkpoints), you should use one of the alternatives listed below.

[^ceph]: [Ceph](https://ceph.io/) is a distributed storage system that provides high performance and reliability.

### `/mnt/wato-drive*` Directory

We have a few HDD-backed NFS[^nfs] servers that provide large storage for the machines in the cluster. These NASes are mounted
on all general-use machines at the `/mnt/wato-drive*` directories. You can use these mounts to store large files such as datasets.

[^nfs]: [NFS](https://en.wikipedia.org/wiki/Network_File_System) stands for "Network File System" and is used to share files over a network.

### `/mnt/scratch` Directory

Every general-use machine has an SSD-backed local storage pool that is mounted at the `/mnt/scratch` directory.
These storage pools are meant for temporary storage for long-running jobs that require fast and reliable filesystem access,
such as storing training data and model checkpoints for ML workloads.

The space on `/mnt/scratch` is limited. Please make sure to clean up your files after you are done with them.

An equivalent of `/mnt/scratch` is available on the SLURM compute nodes as well.
They can be requested by following the instructions [here](./slurm#grestmpdisk).

### Docker

Every general-use machine has Docker Rootless[^docker-rootless] installed. There is a per-user storage quota to ensure that everyone has
enough space to run their workloads. The storage quota is described on the [Quotas](./quotas) page.

[^docker-rootless]: [Docker](https://www.docker.com/) is a platform for neatly packaging software, both for development and deployment.
  [Docker Rootless](https://docs.docker.com/engine/security/rootless/) is a way to run Docker without root privileges.

### S3-compatible Object storage

We have an S3-compatible object storage that runs on the Ceph cluster. If you require this functionality, please contact a WATcloud
admin to get access.

### Kubernetes

We run a Kubernetes cluster using [microk8s](https://microk8s.io/). This is mostly for running internal WATcloud services. However,
if you require this functionality, please contact a WATcloud admin to get access.

### GitHub Actions Runners

We run a GitHub Runner farm on the Kubernetes cluster using [actions-runner-controller](https://github.com/actions/actions-runner-controller).
Currently, it's enabled for the WATonomous organization. If you require this functionality, please reach out to a WATcloud admin to get access.

## Software

We try to keep the machines lean. We generally refrain from installing software that make sense for rootless installation or running in
containerized environments.

Examples of software that we install:
- Docker (rootless)
- NVIDIA Container Toolkit
- zsh
- various CLI tools (e.g. `vifm`, `iperf`, `moreutils`, `jq`, `ncdu`)

Examples of software that we do not install:
- conda (use [miniconda](https://docs.conda.io/en/latest/miniconda.html) instead)
- ROS (use [Docker](https://hub.docker.com/_/ros) instead)
- CUDA (use [Docker](https://hub.docker.com/r/nvidia/cuda) instead. Or use [CVMFS](./slurm#cvmfs) on the SLURM compute nodes.)

If there is a piece of software that you think should be installed on the machines, please reach out to a WATcloud team member.

### `OMP_NUM_THREADS`

On general-use machines, we set the `OMP_NUM_THREADS` environment variable to `1` by default. This is to prevent system overload when running
programs that use OpenMP[^openmp]. This default is consistent with the
[default behaviour of PyTorch Distributed](https://github.com/pytorch/pytorch/blob/4b494d075093096d822b9d614e2719a0e821c6af/torch/distributed/run.py#L758-L763).
If you'd like to change this default, simply set the `OMP_NUM_THREADS` environment variable to the desired value, for example:

```bash copy
export OMP_NUM_THREADS=4
```

[^openmp]: [OpenMP](https://www.openmp.org/) is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran.

## Maintenance and Outages

We try to keep the machines in the cluster up and running at all times. However, we do need to perform regular maintenance to keep the machines
up-to-date and our services running smoothly. All scheduled maintenance will be announced in the
[infrastructure-support repo](https://github.com/WATonomous/infrastructure-support/discussions)[^maintenance-notify]. Emergency maintenance and maintenance
that has little effect on user experience will be announced in the `#server-use` channel on Discord.

[^maintenance-notify]: The GitHub team `@WATonomous/watcloud-compute-cluster-users` will be notified.  Please ensure that you
[enable notifications](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications)
to receive these notices.

Sometimes, the machines in the cluster may go down unexpectedly due to hardware failures or power outages. We have a comprehensive suite of
healthchecks and internal monitoring tools to detect these failures and notify us. However, due to the part-time nature of the student team, we may not
be able to respond to these failures immediately. If you notice that a machine is down, please restlessly ping the WATcloud team on Discord
(`@WATcloud` or `@WATcloud Leads`, in the `#server-use` channel).

To see if a machine is having issues, please visit [status.watonomous.ca](https://status.watonomous.ca). The WATcloud team uses this page as
a dashboard to monitor the health of the machines in the cluster.

## Usage Guidelines

- Be [nice](https://man7.org/linux/man-pages/man2/nice.2.html)
  - If you have a long-running non-interactive process, please [increase its niceness](https://www.tecmint.com/set-linux-process-priority-using-nice-and-renice-commands/) so that interactive programs don't lag.
  - Being nice is simply changing `./my_program arg1 arg2{:bash}` to `nice ./my_program arg1 arg2{:bash}`.
- Clean up after yourself
  - If you are using `/mnt/scratch`, please make sure to clean up your files after you are done with them.
  - Please only use `/home` for small files. Writing large files to `/home` will significantly slow down the filesystem for all users.
  - `/mnt/wato-drive*` are large storage pools, but they are not infinite and can fill up quickly with today's large datasets.
    Please remove unneeded files from these directories.
  - Please clean up your Docker images and containers regularly. `docker system prune --all{:bash}` is your friend.

## Troubleshooting

This section contains some common issues that users may encounter when using the machines in the cluster and their solutions. If you encounter an issue that is not listed here, please [reach out](./support-resources).

### Permission denied while trying to connect to the Docker daemon

You may encounter this error when trying to run Docker commands:

```
> docker ps
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/json": dial unix /var/run/docker.sock: connect: permission denied
```

This error occurs when your shell doesn't source `/etc/profile` (`zsh` is known to do this). Our Docker rootless setup
requires that the `DOCKER_HOST` environment variable be set properly, which is done in `/etc/profile`. To fix this,
please add the following line to your shell's rc file (e.g. `~/.zshrc`):

```bash copy
export DOCKER_HOST=unix://${XDG_RUNTIME_DIR}/docker.sock
```

Remember to restart your shell or source the rc file after making the change.

### Disk quota exceeded when running Docker commands

You may encounter the following error when running Docker commands:

```
> docker pull hello-world
...
open /var/lib/cluster/users/$UID/docker/tmp/GetImageBlob3112047691: disk quota exceeded
```

This means that you have exceeded your allocated storage quota[^quota-more-info].
Here are some commands[^docker-prune] you can use to free up disk space:

```bash
# remove dangling images (images without tags)
docker image prune
# remove all images without an existing container
docker image prune --all
# remove stopped containers
docker container prune
# remove all volumes not used by at least one container
docker volume prune
# remove all stopped containers, dangling images, unused networks, and unused build cache
docker system prune
# same as above, but also removes unused volumes
docker system prune --volumes
# same as above, but also removes unused images
docker system prune --volumes --all
```

[^docker-prune]: For more information about the Docker prune commands, please refer to the [Docker manual](https://docs.docker.com/config/pruning/)
[^quota-more-info]: For more information about storage quotas, including how to check your current quota usage, please see the [Quotas](./quotas) page.


### Cannot connect to the Docker daemon

You may encounter this error when trying to run Docker commands:

```
> docker ps
Cannot connect to the Docker daemon at unix:///run/user/$UID/docker.sock. Is the docker daemon running?
```

To troubleshoot this issue, please run the following command to confirm whether the Docker daemon is running:

```bash copy
systemctl --user status docker-rootless
```

A healthy Docker daemon should look like this:

```ansi
[0;1;32m●[0m docker-rootless.service - Docker Application Container Engine (Rootless)
     Loaded: loaded (/usr/lib/systemd/user/docker-rootless.service; enabled; vendor preset: enabled)
     Active: [0;1;32mactive (running)[0m since Tue 2024-01-16 05:47:04 UTC; 44s ago
       Docs: https://docs.docker.com/go/rootless/
   Main PID: 2090042 (rootlesskit)
      Tasks: 63
     Memory: 54.7M
        CPU: 587ms
     CGroup: /user.slice/user-1507.slice/user@1507.service/app.slice/docker-rootless.service
             ├─2090042 rootlesskit --net=slirp4netns --mtu=65520 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=builtin --copy-up=/etc --copy-up=/run --propagation=rslave /usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json
             ├─2090053 /proc/self/exe --net=slirp4netns --mtu=65520 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=builtin --copy-up=/etc --copy-up=/run --propagation=rslave /usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json
             ├─2090072 slirp4netns --mtu 65520 -r 3 --disable-host-loopback --enable-sandbox --enable-seccomp 2090053 tap0
             ├─2090080 dockerd --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json
             └─2090112 containerd --config /run/user/1507/docker/containerd/containerd.toml
```

If the daemon failed to start, the output may look like this:

```ansi
[0;31;1m×[0m docker-rootless.service - Docker Application Container Engine (Rootless)
     Loaded: loaded (/usr/lib/systemd/user/docker-rootless.service; enabled; vendor preset: enabled)
     Active: [0;31;1mfailed[0m (Result: exit-code) since Tue 2024-01-16 05:15:38 UTC; 9min ago
       Docs: https://docs.docker.com/go/rootless/
    Process: 2049816 ExecStart=/usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json [0;31;1m(code=exited, status=1/FAILURE)[0m
   Main PID: 2049816 (code=exited, status=1/FAILURE)
        CPU: 283ms

Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: docker-rootless.service: Scheduled restart job, restart counter is at 3.
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: Stopped Docker Application Container Engine (Rootless).
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: [0;1;38:5:185mdocker-rootless.service: Start request repeated too quickly.[0m
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: [0;1;38:5:185mdocker-rootless.service: Failed with result 'exit-code'.[0m
Jan 16 05:15:38 trpro-ubuntu2 systemd[73045]: [0;31;1mFailed to start Docker Application Container Engine (Rootless).
```

To get more information about why the daemon failed to start, you can view the logs by running:

```bash copy
journalctl --user --catalog --pager-end --unit docker-rootless.service
```

One of the most common reasons for the Docker daemon to fail to start is that the storage quota has been exceeded. If this is the case, you will see `disk quota exceeded` in the logs:

```ansi {3}
[0mJan 16 05:15:35 trpro-ubuntu2 dockerd-rootless.sh[2049885]: time="2024-01-16T05:15:35.066806684Z" level=info msg="containerd successfully booted in 0.020404s"
Jan 16 05:15:35 trpro-ubuntu2 dockerd-rootless.sh[2049852]: time="2024-01-16T05:15:35.077397993Z" level=info msg="stopping healthcheck following graceful shutdown" module=libcontainerd
Jan 16 05:15:36 trpro-ubuntu2 dockerd-rootless.sh[2049852]: failed to start daemon: Unable to get the TempDir under /var/lib/cluster/users/1507/docker: mkdir /var/lib/cluster/users/1507/docker/tmp: disk quota exceeded
Jan 16 05:15:36 trpro-ubuntu2 dockerd-rootless.sh[2049827]: [rootlesskit:child ] error: command [/usr/bin/dockerd-rootless.sh --data-root /var/lib/cluster/users/1507/docker --config-file /etc/docker/daemon.json] exited: exit status 1
Jan 16 05:15:36 trpro-ubuntu2 dockerd-rootless.sh[2049816]: [rootlesskit:parent] error: child exited: exit status 1
Jan 16 05:15:36 trpro-ubuntu2 systemd[73045]: [0;1mdocker-rootless.service: Main process exited, code=exited, status=1/FAILURE[0m
[0;38:5:245m░░ [0;32mSubject: Unit process exited[0m
[0;38:5:245m░░[0m [0;32mDefined-By: systemd[0m
[0;38:5:245m░░[0m [0;32mSupport: http://www.ubuntu.com/support[0m
[0;38:5:245m░░[0m
[0;38:5:245m░░[0m [0;32mAn ExecStart= process belonging to unit UNIT has exited.[0m
[0;38:5:245m░░[0m
[0;38:5:245m░░[0m [0;32mThe process' exit code is 'exited' and its exit status is 1.[0m
Jan 16 05:15:36 trpro-ubuntu2 systemd[73045]: [0;1;38:5:185mdocker-rootless.service: Failed with result 'exit-code'.
```

For more information about storage quotas, please see the [Quotas](./quotas) page.

To free up space for Docker, you can selectively delete files from the `/var/lib/cluster/users/$(id -u)/docker` directory until you are no longer over quota. However, it could be difficult to determine which files to delete. If you would like to delete all your Docker files (including images, containers, volumes, etc.), which effectively resets your Docker installation, you can delete the entire directory:

```bash copy
buildah unshare rm -r /var/lib/cluster/users/$(id -u)/docker
```

Here, `buildah unshare{:bash}` puts us in the USERNS[^buildah-unshare], which allows us to manage files created by `uidmap` (used by Docker rootless).

[^buildah-unshare]: https://github.com/containers/podman/issues/1386#issuecomment-417640661

If you would prefer to use Docker tools to clean up your Docker installation (e.g. `docker system prune --all{:bash}`), you can reach out to a WATcloud team member to temporarily increase your storage quota so that Docker daemon can be started.

Once you have freed up enough space, you can restart the Docker daemon by running:

```bash copy
systemctl --user restart docker-rootless
```


{
// Separate footnotes from the main content
}
import { Separator } from "@/components/ui/separator"

<Separator className="mt-6" />
